{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p><sub><sup>Logo design by Elizabeth Jarboe</sup></sub></p>"},{"location":"#welcome-to-the-stela-toolkit","title":"Welcome to the STELA Toolkit","text":"<p>The STELA Toolkit is a Python package for interpolating astrophysical light curves using Gaussian Processes in order to compute frequency-domain and standard time domain data products.</p> <p>STELA is designed for:</p> <ul> <li>Researchers handling irregular, noisy light curve data</li> <li>Users who want to generate Fourier-based data products (lag spectra, coherence spectra, power spectra, cross spectra)</li> <li>... or standard time-domain products, like lags with CCF and linear ICCF.</li> </ul> <p>Get started:</p> <ul> <li>Overview \u2014 What STELA can do and how it works</li> <li>Tutorial \u2014 Hands-on with GP modeling, interpolation, and lag analysis</li> <li>Gaussian Processes \u2014 Background, intuition, and implementation</li> <li>Module Reference \u2014 Technical docs and API</li> </ul> <p>If you're new to GP modeling, the overview and tutorial are the best places to begin.</p>"},{"location":"gaussian_process_intro/","title":"Introduction to Gaussian Processes","text":"<p>A Gaussian Process (GP) is one of the most powerful tools in Bayesian statistics for modeling functions. In STELA, we use GPs to interpolate AGN light curves \u2014 enabling advanced time and frequency analyses even with noisy, irregularly sampled data.</p> <p>This page serves as both a conceptual and practical introduction to GPs, with an emphasis on Bayesian foundations, statistical structure, and how this model supports the goals of time-domain astronomy.</p>"},{"location":"gaussian_process_intro/#1-from-parametric-models-to-function-distributions","title":"1. From Parametric Models to Function Distributions","text":"<p>In classical modeling, such as linear regression, we assume:</p> \\[ y = X \\beta + \\epsilon \\] <p>This assumes a parametric form \u2014 a line or polynomial \u2014 and estimates parameters like \\( \\beta \\). But what if we don't know the form of the function?</p> <p>A Gaussian Process is a distribution over functions.</p> <p>Instead of picking a specific formula, we say:</p> <p>\"Any set of values from this unknown function should follow a multivariate normal distribution.\"</p> <p>This means:</p> <ul> <li>Every time point \\( t \\) is a random variable \\( f(t) \\)</li> <li>Any collection \\( [f(t_1), ..., f(t_n)] \\) is jointly Gaussian</li> </ul>"},{"location":"gaussian_process_intro/#2-gp-specification-mean-and-covariance-functions","title":"2. GP Specification: Mean and Covariance Functions","text":"<p>A GP is fully defined by:</p> <ul> <li>A mean function \\( m(t) = \\mathbb{E}[f(t)] \\)</li> <li>A covariance function \\( k(t, t') = \\text{Cov}(f(t), f(t')) \\)</li> </ul> <p>We write this as:</p> \\[ f(t) \\sim \\mathcal{GP}(m(t), k(t, t')) \\]"},{"location":"gaussian_process_intro/#in-stela","title":"In STELA:","text":"<ul> <li>We assume \\( m(t) = 0 \\) after standardizing the data.</li> <li>The covariance function is called the kernel, which determines the shape, smoothness, and periodicity of the model.</li> </ul>"},{"location":"gaussian_process_intro/#3-kernel-functions-the-heart-of-the-gp","title":"3. Kernel Functions: The Heart of the GP","text":"<p>The kernel defines the relationship between any two inputs. Common kernels include:</p> <ul> <li>RBF (Squared Exponential)</li> </ul> <p>$$   k(t, t') = \\sigma^2 \\exp\\left(-\\frac{(t - t')^2}{2\\ell^2}\\right)   $$</p> <ul> <li>Matern (\u00bd, 3/2, 5/2)</li> <li>Rational Quadratic</li> <li>Spectral Mixture</li> </ul> <p>Each kernel has hyperparameters:</p> <ul> <li>\\( \\ell \\): length scale</li> <li>\\( \\sigma \\): output variance</li> <li>For spectral: mixture weights, frequencies</li> </ul>"},{"location":"gaussian_process_intro/#4-bayesian-inference-and-gp-predictions","title":"4. Bayesian Inference and GP Predictions","text":""},{"location":"gaussian_process_intro/#goal","title":"Goal:","text":"<p>Given noisy data \\( \\mathbf{y} \\) at times \\( \\mathbf{t} \\), predict \\( f_* \\) at new times \\( \\mathbf{t}_* \\).</p> <p>We start with the prior:</p> \\[ \\begin{bmatrix} \\mathbf{y} \\\\ f_* \\end{bmatrix} \\sim \\mathcal{N}\\left(0,  \\begin{bmatrix} K + \\sigma_n^2 I &amp; K_*^\\top \\\\ K_* &amp; K_{**} \\end{bmatrix} \\right) \\] <p>Where:</p> <ul> <li>\\( K \\): covariance of training points</li> <li>\\( K_* \\): covariance between training and test</li> <li>\\( K_{**} \\): covariance of test points</li> <li>\\( \\sigma_n^2 \\): noise variance</li> </ul>"},{"location":"gaussian_process_intro/#posterior-prediction","title":"Posterior prediction:","text":"\\[ \\mathbb{E}[f_*] = K_*^\\top (K + \\sigma_n^2 I)^{-1} \\mathbf{y} \\] \\[ \\text{Cov}[f_*] = K_{**} - K_*^\\top (K + \\sigma_n^2 I)^{-1} K_* \\]"},{"location":"gaussian_process_intro/#5-noise-handling-in-stela","title":"5. Noise Handling in STELA","text":"<p>STELA handles noise in two ways:</p> <ul> <li>Explicit error bars on light curve points</li> <li>White noise model:</li> </ul> <p>Adds a diagonal term \\( \\sigma_w^2 I \\) to the kernel for unaccounted stochastic noise</p>"},{"location":"gaussian_process_intro/#6-training-the-gp-marginal-likelihood","title":"6. Training the GP: Marginal Likelihood","text":"<p>We don\u2019t sample hyperparameters \u2014 we optimize them by maximizing the marginal likelihood:</p> \\[ p(\\mathbf{y} \\mid \\theta) = \\mathcal{N}(0, K_\\theta + \\sigma_n^2 I) \\] <p>Its log form:</p> \\[ \\log p(\\mathbf{y}) = -\\frac{1}{2} \\mathbf{y}^\\top K^{-1} \\mathbf{y} - \\frac{1}{2} \\log |K| - \\frac{n}{2} \\log(2\\pi) \\] <p>Each term has an interpretation:</p> <ul> <li>Data fit: how well the model explains the data</li> <li>Complexity penalty: penalizes overfitting</li> <li>Normalization: adjusts for scale</li> </ul> <p>STELA minimizes the Negative Log Marginal Likelihood (NLML) using the Adam optimizer.</p>"},{"location":"gaussian_process_intro/#7-sampling-from-the-posterior","title":"7. Sampling from the Posterior","text":"<p>STELA allows you to:</p> <ul> <li>Sample multiple posterior realizations</li> <li>Feed each into time or frequency-domain tools</li> <li>Propagate uncertainty to downstream metrics</li> </ul>"},{"location":"gaussian_process_intro/#8-summary","title":"8. Summary","text":"<p>GPs offer:</p> <ul> <li>Flexibility: no fixed model form</li> <li>Interpretability: kernel tells you about variability</li> <li>Uncertainty: quantified at every prediction</li> <li>Consistency: grounded in Bayesian statistics</li> </ul> <p>They form the mathematical foundation for STELA\u2019s light curve modeling and variability analysis.</p>"},{"location":"installation/","title":"Installation","text":"<p>Not ready yet.</p>"},{"location":"overview/","title":"STELA Toolkit Overview","text":"<p>STELA (Sampling Time for Even Lightcurve Analysis) is a Python package for interpolating astrophysical light curves using Gaussian Processes in order to compute frequency-domain and standard time domain data products.</p> <p>This package was designed for researchers who need to:</p> <ul> <li>Interpolate irregular, noisy light curves</li> <li>Quantify variability in the time and frequency domains</li> <li>Model lag phenomena using cross-correlations and GP-informed analysis</li> <li>Simulate synthetic light curves with physically motivated structure</li> </ul>"},{"location":"overview/#core-capabilities","title":"Core Capabilities","text":""},{"location":"overview/#1-gaussian-process-gp-modeling","title":"1. Gaussian Process (GP) Modeling","text":"<p>STELA uses Gaussian Processes to model AGN light curves in a Bayesian framework:</p> <ul> <li> <p>Standardizes input data and removes trends</p> </li> <li> <p>Applies optional:</p> <ul> <li>Box-Cox transformations to improve normality</li> <li>Standardization/unstandardization for numerical stability</li> </ul> </li> <li> <p>Fits kernel hyperparameters by minimizing the negative log marginal likelihood (NLML)</p> </li> <li> <p>Allows easy selection among multiple kernel types:</p> <ul> <li>Radial Basis Function (RBF)</li> <li>Rational Quadratic (RQ)</li> <li>Matern (\u00bd, 3/2, 5/2)</li> <li>Spectral Mixture (for advanced periodic behavior)</li> </ul> </li> <li> <p>Supports white noise fitting alongside observational error</p> </li> </ul> <p>After training, you can generate posterior samples or predictions at arbitrary times \u2014 enabling accurate interpolation and uncertainty propagation.</p>"},{"location":"overview/#2-frequency-domain-tools","title":"2. Frequency-Domain Tools","text":"<p>Using GP-modeled light curves or evenly-sampled data, STELA enables:</p> <ul> <li>Power Spectrum \u2014 Measures variability power at different frequencies</li> <li>Cross Spectrum \u2014 Frequency-domain relationship between two light curves</li> <li>Coherence \u2014 Quantifies signal correlation at each frequency</li> <li>Lag-Frequency Spectrum \u2014 Time delay as a function of frequency</li> <li>Lag-Energy Spectrum \u2014 Time lag across energy bands</li> </ul> <p>All tools propagate uncertainty using GP samples or Monte Carlo simulations.</p>"},{"location":"overview/#3-time-domain-lag-analysis","title":"3. Time-Domain Lag Analysis","text":"<p>STELA supports two methods for measuring time-domain lags:</p> <ul> <li> <p>Interpolated Cross-Correlation Function (ICCF)</p> <ul> <li>Interpolates one light curve onto the other's grid</li> <li>Estimates peak or centroid of the correlation curve</li> </ul> </li> <li> <p>GP-Based Cross-Correlation</p> <ul> <li>Uses GP realizations to compute posterior-distributed lags</li> <li>Reflects realistic uncertainty and sampling effects</li> </ul> </li> </ul>"},{"location":"overview/#4-data-simulation-and-preprocessing","title":"4. Data Simulation and Preprocessing","text":"<p>STELA includes tools to:</p> <ul> <li> <p>Simulate synthetic light curves with:</p> <ul> <li>Power-law power spectra</li> <li>Specified variability amplitude</li> <li>Injected time lags</li> </ul> </li> <li> <p>Load time series from <code>.dat</code>, <code>.csv</code>, or FITS files</p> </li> <li> <p>Automatically detect and apply preprocessing steps:</p> <ul> <li>Normality correction</li> <li>Standardization</li> <li>Resampling to regular time grid</li> </ul> </li> </ul>"},{"location":"overview/#unified-api-design","title":"Unified API Design","text":"<p>Each major object (e.g., <code>PowerSpectrum</code>, <code>LagFrequencySpectrum</code>, <code>GaussianProcess</code>) includes:</p> <ul> <li><code>.plot()</code> method with consistent styling</li> <li>Uncertainty-aware results</li> <li>Fully documented parameters and attributes</li> </ul> <p>STELA accepts either raw <code>LightCurve</code> objects or trained <code>GaussianProcess</code> models in most functions \u2014 allowing users to apply the pipeline flexibly.</p>"},{"location":"overview/#next-steps","title":"Next Steps","text":"<ul> <li>Install STELA</li> <li>Understand Gaussian Processes</li> <li>Run the tutorial notebook</li> <li>Explore the module reference</li> </ul> <p>This package was designed for researchers who need to: - Interpolate irregular, noisy light curves - Quantify variability in the time and frequency domains - Model lag phenomena using cross-correlations and GP-informed analysis - Simulate synthetic light curves with physically motivated structure</p>"},{"location":"tutorial/","title":"Tutorial","text":"In\u00a0[1]: Copied! <pre># Import STELA\nfrom stela_toolkit import *\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n%load_ext autoreload\n%autoreload 2\n</pre> # Import STELA from stela_toolkit import *  import numpy as np import matplotlib.pyplot as plt %load_ext autoreload %autoreload 2 In\u00a0[2]: Copied! <pre># Option 1: Load light curve directly from a text file\nlightcurve = LightCurve(file_path='../data/NGC5548_U_swift.dat')\n\n# Option 2: Or from arrays\ndata = np.genfromtxt(f'../data/NGC5548_X_swift.dat')\nlightcurve = LightCurve(times=data[:, 0], rates=data[:, 1], errors=data[:, 2])\n</pre> # Option 1: Load light curve directly from a text file lightcurve = LightCurve(file_path='../data/NGC5548_U_swift.dat')  # Option 2: Or from arrays data = np.genfromtxt(f'../data/NGC5548_X_swift.dat') lightcurve = LightCurve(times=data[:, 0], rates=data[:, 1], errors=data[:, 2]) In\u00a0[3]: Copied! <pre># Basic plot\nlightcurve.plot()\n\n# Customized plot\nlightcurve.plot(\n    figsize=(8, 4),\n    xlabel='Modified Heliocentric Julian Date (Days)',\n    ylabel='Flux',\n    xlim=(lightcurve.times[0], lightcurve.times[-1]),\n    title='NGC 5548 U-band Light Curve',\n    plot_kwargs={'color': 'purple', 'fmt': 'o', 'lw': 1, 'ms': 3},\n    major_tick_kwargs={'direction': 'in', 'top': True, 'right': True, 'length': 6, 'width': 1},\n    minor_tick_kwargs={'direction': 'in', 'top': True, 'right': True, 'length': 3, 'width': 0.5}\n    # save='my_plot.png', savefig_kwargs={'dpi': 300}\n)\n</pre> # Basic plot lightcurve.plot()  # Customized plot lightcurve.plot(     figsize=(8, 4),     xlabel='Modified Heliocentric Julian Date (Days)',     ylabel='Flux',     xlim=(lightcurve.times[0], lightcurve.times[-1]),     title='NGC 5548 U-band Light Curve',     plot_kwargs={'color': 'purple', 'fmt': 'o', 'lw': 1, 'ms': 3},     major_tick_kwargs={'direction': 'in', 'top': True, 'right': True, 'length': 6, 'width': 1},     minor_tick_kwargs={'direction': 'in', 'top': True, 'right': True, 'length': 3, 'width': 0.5}     # save='my_plot.png', savefig_kwargs={'dpi': 300} ) In\u00a0[4]: Copied! <pre># Remove any NaNs in the time, flux, or error arrays\nPreprocessing.remove_nans(lightcurve)\n\n# Explore outlier removal using a rolling IQR window, or a global IQR (set rolling_window = None)\nPreprocessing.remove_outliers(lightcurve, threshold=1.5, rolling_window=10, plot=True, verbose=True, save=False)\nPreprocessing.remove_outliers(lightcurve, threshold=1.5, rolling_window=50, plot=True, verbose=True, save=False)\n\n# Trim the light curve to a specific observing window\nPreprocessing.trim_time_segment(lightcurve, end_time=56815, plot=True, save=False)\n\n# Detrend the light curve using a polynomial fit (here, degree 3)\nPreprocessing.polynomial_detrend(lightcurve, degree=3, plot=True, save=False)\n\n# Standardize the light curve (zero mean, unit variance)\nPreprocessing.standardize(lightcurve)\nPreprocessing.unstandardize(lightcurve)\n</pre> # Remove any NaNs in the time, flux, or error arrays Preprocessing.remove_nans(lightcurve)  # Explore outlier removal using a rolling IQR window, or a global IQR (set rolling_window = None) Preprocessing.remove_outliers(lightcurve, threshold=1.5, rolling_window=10, plot=True, verbose=True, save=False) Preprocessing.remove_outliers(lightcurve, threshold=1.5, rolling_window=50, plot=True, verbose=True, save=False)  # Trim the light curve to a specific observing window Preprocessing.trim_time_segment(lightcurve, end_time=56815, plot=True, save=False)  # Detrend the light curve using a polynomial fit (here, degree 3) Preprocessing.polynomial_detrend(lightcurve, degree=3, plot=True, save=False)  # Standardize the light curve (zero mean, unit variance) Preprocessing.standardize(lightcurve) Preprocessing.unstandardize(lightcurve) <pre>Removed 0 NaN points.\n(0 NaN rates, 0 NaN errors)\n===================\nRemoved 11 outliers (4.14% of data).\n===================\n</pre> <pre>Removed 3 outliers (1.13% of data).\n===================\n</pre> In\u00a0[5]: Copied! <pre># the Q-Q plot already appears reasonable, although the tails here seem nonnormal, or our normal is skewed\nPreprocessing.generate_qq_plot(lightcurve)\n\n# test indicates that the data is not normal\nPreprocessing.check_normal(lightcurve, plot=False)\n\n# our boxcox transformation helps!\nPreprocessing.check_boxcox_normal(lightcurve, plot=True)\n</pre> # the Q-Q plot already appears reasonable, although the tails here seem nonnormal, or our normal is skewed Preprocessing.generate_qq_plot(lightcurve)  # test indicates that the data is not normal Preprocessing.check_normal(lightcurve, plot=False)  # our boxcox transformation helps! Preprocessing.check_boxcox_normal(lightcurve, plot=True) <pre>Using Lilliefors test (for n &gt;= 50)\nLilliefors (modified KS) test p-value: 0.001\n  -&gt; Very strong evidence against normality (p = 0.001)\n     - Consider running `check_boxcox_normal()` to see if a Box-Cox transformation can help.\n     - Often checking normality via a Q-Q plot (run `generate_qq_plot(lightcurve)`) is sufficient.\n===================\nBefore Box-Cox:\n----------------\nUsing Lilliefors test (for n &gt;= 50)\nLilliefors (modified KS) test p-value: 0.001\n  -&gt; Very strong evidence against normality (p = 0.001)\n     - Consider running `check_boxcox_normal()` to see if a Box-Cox transformation can help.\n     - Often checking normality via a Q-Q plot (run `generate_qq_plot(lightcurve)`) is sufficient.\n===================\nAfter Box-Cox:\n----------------\nUsing Lilliefors test (for n &gt;= 50)\nLilliefors (modified KS) test p-value: 0.0549\n  -&gt; Little to no evidence against normality (p = 0.0549)\n===================\n</pre> Out[5]: <pre>(np.True_, np.float64(0.05493661113859104))</pre> In\u00a0[6]: Copied! <pre>gp_model = GaussianProcess(lightcurve, \n                           kernel_form=\"Matern32\", \n                           white_noise=True, \n                           enforce_normality=True, \n                           run_training=False\n                        )\n</pre> gp_model = GaussianProcess(lightcurve,                             kernel_form=\"Matern32\",                             white_noise=True,                             enforce_normality=True,                             run_training=False                         ) <pre>Checking normality of input light curve...\n\n - Light curve is not normal (p = 0.0010). Applying Box-Cox transformation...\n - Normality sufficiently achieved after Box-Cox (p = 0.0549)! Proceed as normal!\n\n</pre> In\u00a0[7]: Copied! <pre># Train the model manually after initialization\ngp_model.train(\n    num_iter=500,        # Number of training steps\n    learn_rate=0.1,      # Step size for optimizer\n    plot=True,           # Show NLML evolution\n    verbose=True         # Print progress info\n)\n\n# Alternatively: train automatically during initialization\ngp_model = GaussianProcess(lightcurve, \n                           kernel_form=\"Matern32\", \n                           white_noise=True, \n                           enforce_normality=True, \n                           run_training=True,\n                           plot_training=False,\n                           num_iter=500,        \n                           learn_rate=0.1, \n                           verbose=False)\n\n# To view the hyperparameters of a model at a later time, use the get_hyperparameters method\ngp_model.get_hyperparameters()\n\n# It is good practice to save the model, so that you can load it later for consistency\n# Both can be done with the package's save_model and load_model methods\n# gp_model.save_model('gp_model.pkl')\n# gp_model.load_model('gp_model.pkl')\n</pre> # Train the model manually after initialization gp_model.train(     num_iter=500,        # Number of training steps     learn_rate=0.1,      # Step size for optimizer     plot=True,           # Show NLML evolution     verbose=True         # Print progress info )  # Alternatively: train automatically during initialization gp_model = GaussianProcess(lightcurve,                             kernel_form=\"Matern32\",                             white_noise=True,                             enforce_normality=True,                             run_training=True,                            plot_training=False,                            num_iter=500,                                    learn_rate=0.1,                             verbose=False)  # To view the hyperparameters of a model at a later time, use the get_hyperparameters method gp_model.get_hyperparameters()  # It is good practice to save the model, so that you can load it later for consistency # Both can be done with the package's save_model and load_model methods # gp_model.save_model('gp_model.pkl') # gp_model.load_model('gp_model.pkl') <pre>Iter 1/500 - loss: 1.062   lengthscale: 12.056   noise: 5.0e-01\nIter 26/500 - loss: 0.928   lengthscale: 9.354   noise: 1.5e-01\n</pre> <pre>Iter 51/500 - loss: 0.865   lengthscale: 6.520   noise: 9.3e-02\nIter 76/500 - loss: 0.852   lengthscale: 4.780   noise: 7.1e-02\n</pre> <pre>Iter 101/500 - loss: 0.837   lengthscale: 4.151   noise: 7.2e-02\nIter 126/500 - loss: 0.809   lengthscale: 2.665   noise: 6.7e-02\n</pre> <pre>Iter 151/500 - loss: 0.805   lengthscale: 2.023   noise: 5.3e-02\n</pre> <pre>Iter 176/500 - loss: 0.805   lengthscale: 2.118   noise: 6.0e-02\nIter 201/500 - loss: 0.805   lengthscale: 2.099   noise: 5.8e-02\n</pre> <pre>Iter 226/500 - loss: 0.805   lengthscale: 2.102   noise: 5.8e-02\n</pre> <pre>Iter 251/500 - loss: 0.805   lengthscale: 2.103   noise: 5.8e-02\n</pre> <pre>Iter 276/500 - loss: 0.805   lengthscale: 2.102   noise: 5.8e-02\n</pre> <pre>Iter 301/500 - loss: 0.805   lengthscale: 2.102   noise: 5.8e-02\n</pre> <pre>Iter 326/500 - loss: 0.805   lengthscale: 2.102   noise: 5.8e-02\n</pre> <pre>Iter 351/500 - loss: 0.805   lengthscale: 2.102   noise: 5.8e-02\nIter 376/500 - loss: 0.805   lengthscale: 2.102   noise: 5.8e-02\n</pre> <pre>Iter 401/500 - loss: 0.805   lengthscale: 2.102   noise: 5.8e-02\n</pre> <pre>Iter 426/500 - loss: 0.805   lengthscale: 2.102   noise: 5.8e-02\n</pre> <pre>Iter 451/500 - loss: 0.805   lengthscale: 2.102   noise: 5.8e-02\n</pre> <pre>Iter 476/500 - loss: 0.805   lengthscale: 2.102   noise: 5.8e-02\n</pre> <pre>Iter 500/500 - loss: 0.805   lengthscale: 2.102   noise: 5.8e-02\nTraining complete. \n   - Final loss: 0.80462\n   - Final hyperparameters:\n      likelihood.second_noise_covar.noise       : 0.0582\n      covar_module.outputscale                  : 0.8186\n      covar_module.base_kernel.lengthscale      : 2.102\n</pre> <pre>Checking normality of input light curve...\n\n - Light curve is not normal (p = 0.0010). Applying Box-Cox transformation...\n - Normality sufficiently achieved after Box-Cox (p = 0.0549)! Proceed as normal!\n\n</pre> Out[7]: <pre>{'likelihood.second_noise_covar.noise': 0.0581585057079792,\n 'covar_module.outputscale': 0.8185522556304932,\n 'covar_module.base_kernel.lengthscale': 2.1020381450653076}</pre> In\u00a0[8]: Copied! <pre># Option 1: Let STELA select automatically\n# Will take a bit longer than normal to train, assess all the models\ngp_model = GaussianProcess(lightcurve,\n                           kernel_form=\"auto\", # consider all kernels available\n                         # kernel_form=[\"RBF\", \"Matern32\", \"RQ\", \"Matern52\"],\n                           white_noise=True, \n                           enforce_normality=False, \n                        )  \n\n# Option 2: Compare kernels manually\ngp1 = GaussianProcess(lightcurve, kernel_form=\"RBF\", run_training=True)\ngp2 = GaussianProcess(lightcurve, kernel_form=\"Matern32\", run_training=True)\n\nprint(\"Kernel Statistics (lower is better for AIC/BIC):\")\nprint(\"===============================================\")\n\nprint(f\"{'Kernel':&lt;12} {'AIC':&gt;10} {'BIC':&gt;10}\")\nprint(\"-\" * 34)\nprint(f\"{'RBF':&lt;12} {gp1.aic():&gt;10.3f} {gp1.bic():&gt;10.3f}\")\nprint(f\"{'Matern32':&lt;12} {gp2.aic():&gt;10.3f} {gp2.bic():&gt;10.3f}\")\n</pre> # Option 1: Let STELA select automatically # Will take a bit longer than normal to train, assess all the models gp_model = GaussianProcess(lightcurve,                            kernel_form=\"auto\", # consider all kernels available                          # kernel_form=[\"RBF\", \"Matern32\", \"RQ\", \"Matern52\"],                            white_noise=True,                             enforce_normality=False,                          )    # Option 2: Compare kernels manually gp1 = GaussianProcess(lightcurve, kernel_form=\"RBF\", run_training=True) gp2 = GaussianProcess(lightcurve, kernel_form=\"Matern32\", run_training=True)  print(\"Kernel Statistics (lower is better for AIC/BIC):\") print(\"===============================================\")  print(f\"{'Kernel':&lt;12} {'AIC':&gt;10} {'BIC':&gt;10}\") print(\"-\" * 34) print(f\"{'RBF':&lt;12} {gp1.aic():&gt;10.3f} {gp1.bic():&gt;10.3f}\") print(f\"{'Matern32':&lt;12} {gp2.aic():&gt;10.3f} {gp2.bic():&gt;10.3f}\") <pre>Kernel Statistics (lower is better for AIC/BIC):\n===============================================\nKernel              AIC        BIC\n----------------------------------\nRBF               7.626     18.376\nMatern32          7.570     18.320\n</pre> In\u00a0[9]: Copied! <pre># Visualize the model's prediction, uncertainty, and a sample realization\ngp_model.plot()\n\n# Define a regular time grid for prediction and sampling\nnew_times = np.linspace(lightcurve.times[0], lightcurve.times[-1], 1000, dtype=np.float64)\n\n# Draw 1000 posterior samples from the GP on this grid\n# Each realization is a row in the resulting array\nsamples = gp_model.sample(new_times, num_samples=1000)\n\n# Compute the posterior mean and 2-sigma confidence bounds\npredict_mean, lower, upper = gp_model.predict(new_times)\n</pre> # Visualize the model's prediction, uncertainty, and a sample realization gp_model.plot()  # Define a regular time grid for prediction and sampling new_times = np.linspace(lightcurve.times[0], lightcurve.times[-1], 1000, dtype=np.float64)  # Draw 1000 posterior samples from the GP on this grid # Each realization is a row in the resulting array samples = gp_model.sample(new_times, num_samples=1000)  # Compute the posterior mean and 2-sigma confidence bounds predict_mean, lower, upper = gp_model.predict(new_times) In\u00a0[10]: Copied! <pre># Compute the power spectrum of the light curve\n# norm=True (default) for normalization consistent with PSD, otherwise periodogram\nps = PowerSpectrum(gp_model, fmin='auto', fmax='auto', num_bins=8, bin_type='log', norm=True)\nps.plot()\n\n# Access the frequency and power arrays for your own use\nfreqs = ps.freqs\nfreq_widths = ps.freq_widths\npowers = ps.powers\npower_errors = ps.power_errors\n\n# Show how many frequencies are in each bin\nprint(\"Number of frequencies per bin:\")\nprint(\"===============================\")\nprint(ps.count_frequencies_in_bins())\n</pre> # Compute the power spectrum of the light curve # norm=True (default) for normalization consistent with PSD, otherwise periodogram ps = PowerSpectrum(gp_model, fmin='auto', fmax='auto', num_bins=8, bin_type='log', norm=True) ps.plot()  # Access the frequency and power arrays for your own use freqs = ps.freqs freq_widths = ps.freq_widths powers = ps.powers power_errors = ps.power_errors  # Show how many frequencies are in each bin print(\"Number of frequencies per bin:\") print(\"===============================\") print(ps.count_frequencies_in_bins()) <pre>Detected 1000 samples generated using a Matern12 kernel.\n</pre> <pre>Number of frequencies per bin:\n===============================\n[  2   2   6  12  26  57 124 271]\n</pre> In\u00a0[11]: Copied! <pre>lightcurve_uvw2 = LightCurve(file_path=\"../data/NGC5548_UVW2_swift.dat\")\ngp_model_uvw2 = GaussianProcess(lightcurve_uvw2,\n                           kernel_form=\"auto\", # consider all kernels available\n                           white_noise=True, \n                           enforce_normality=False, \n                        )\n</pre> lightcurve_uvw2 = LightCurve(file_path=\"../data/NGC5548_UVW2_swift.dat\") gp_model_uvw2 = GaussianProcess(lightcurve_uvw2,                            kernel_form=\"auto\", # consider all kernels available                            white_noise=True,                             enforce_normality=False,                          ) In\u00a0[12]: Copied! <pre># Generate samples using the same time grid as the other band (important!!)\ngp_model_uvw2.sample(new_times, num_samples=1000)\n\n# Compute the cross spectrum between two light curves or GP models\ncs = CrossSpectrum(gp_model, gp_model_uvw2, \n                   fmin='auto', fmax='auto',\n                   num_bins=8, bin_type='log')\ncs.plot()\n\n# Compute the lag-frequency spectrum between two bands\n# Positive lag indicates that the first light curve LAGS the second\nlag_freq = LagFrequencySpectrum(gp_model, gp_model_uvw2,\n                                fmin='auto', fmax='auto',\n                                num_bins=8, bin_type='log')\nlag_freq.plot()\n\n# Compute the coherence spectrum\ncoh = Coherence(gp_model, gp_model_uvw2, \n                fmin='auto', fmax='auto',\n                num_bins=8, bin_type='log')\ncoh.plot()\n</pre> # Generate samples using the same time grid as the other band (important!!) gp_model_uvw2.sample(new_times, num_samples=1000)  # Compute the cross spectrum between two light curves or GP models cs = CrossSpectrum(gp_model, gp_model_uvw2,                     fmin='auto', fmax='auto',                    num_bins=8, bin_type='log') cs.plot()  # Compute the lag-frequency spectrum between two bands # Positive lag indicates that the first light curve LAGS the second lag_freq = LagFrequencySpectrum(gp_model, gp_model_uvw2,                                 fmin='auto', fmax='auto',                                 num_bins=8, bin_type='log') lag_freq.plot()  # Compute the coherence spectrum coh = Coherence(gp_model, gp_model_uvw2,                  fmin='auto', fmax='auto',                 num_bins=8, bin_type='log') coh.plot()  <pre>Detected 1000 samples generated using a Matern12 kernel.\nDetected 1000 samples generated using a Matern32 kernel.\n</pre> <pre>Detected 1000 samples generated using a Matern12 kernel.\nDetected 1000 samples generated using a Matern32 kernel.\n</pre> <pre>Detected 1000 samples generated using a Matern12 kernel.\nDetected 1000 samples generated using a Matern32 kernel.\n</pre>"},{"location":"tutorial/#welcome-to-the-stela-toolkit","title":"Welcome to the STELA Toolkit!\u00b6","text":"<p>The STELA Toolkit is designed for studying variability in light curves, mainly through powerful frequency-domain data products, including power and cross spectra, time lags, and coherences.</p> <p>By using Gaussian process (GP) modeling to interpolate uneven light curves onto a regular time grid, STELA allows you to carry out Fourier-based analyses as if your data were perfectly sampled. That said, if your data is already regularly sampled, you can use STELA just as easily\u2014GPs are completely optional.</p> <p>In addition to frequency-domain tools, STELA also supports time-domain lag analysis using the cross-correlation function (CCF). For irregularly sampled data, you can choose between modeling the light curves with GPs or using the widely adopted interpolated cross-correlation function (ICCF) method, which linearly interpolates one time series onto the other\u2019s grid.</p> <p>STELA was designed to be convenient and intuitive, regardless of your background in Python or statistics. If you ever run into trouble\u2014whether it\u2019s a bug, something confusing, or a suggestion for how STELA could serve you better\u2014I\u2019d love to hear from you. Please open an issue on the GitHub Issues Page, or feel free to email me directly: Collin Lewin (clewin@mit.edu).</p>"},{"location":"tutorial/#in-this-tutorial-youll-learn-how-to","title":"In this tutorial, you\u2019ll learn how to...\u00b6","text":"<ol> <li>Import, clean, and plot time series data</li> <li>Model variability using Gaussian processes (GPs)</li> <li>Predict new values of the time series at previously unobserved times</li> <li>Generate powerful frequency-domain and lag-based data products</li> <li>Simulate light curves with custom variability and response/lag properties</li> </ol>"},{"location":"tutorial/#importing-your-data","title":"Importing Your Data\u00b6","text":"<p>All core functionality in the STELA Toolkit begins with time series data, which we represent using the <code>LightCurve</code> class. You can create a <code>LightCurve</code> object in one of two ways:</p> <ol> <li><p>From a file STELA supports text-based formats like <code>.dat</code>, <code>.txt</code>, and <code>.csv</code>, as well as FITS files. By default, it assumes the first three columns in the file contain:</p> <ul> <li>Time</li> <li>Measured values (e.g., flux or count rate)</li> <li>Measurement uncertainties</li> </ul> <p>If your file uses different columns, you can specify which ones to use with the <code>file_columns</code> argument.</p> </li> <li><p>From NumPy arrays You can also construct a light curve directly by passing arrays for time, values, and uncertainties.</p> </li> </ol> <p>Tip: Once you\u2019ve created a <code>LightCurve</code>, you can pass it to any STELA analysis tool: modeling, frequency-domain transforms, or lag computations.</p> <p>Below, we\u2019ll load and plot data from the AGN NGC 5548, observed by the Swift Observatory as part of the AGN STORM campaign.</p>"},{"location":"tutorial/#plotting-data-and-results","title":"Plotting Data and Results\u00b6","text":"<p>STELA makes it easy to preview and customize plots of your light curve\u2014and not just light curves. Every main class in the toolkit (from power spectra to lag measurements) includes a <code>.plot()</code> method with a consistent structure, so once you learn it, you can apply it everywhere.</p> <p>Almost every visual element of the plot can be customized using keyword arguments:</p> <ul> <li>Basic plot settings like <code>xlabel</code>, <code>ylabel</code>, <code>title</code>, <code>figsize</code>, <code>xscale</code>, and <code>yscale</code></li> <li>Advanced customization via <code>plot_kwargs</code>, <code>fig_kwargs</code>, <code>tick_kwargs</code>, etc.</li> <li>Save options using <code>save</code> and <code>save_kwargs</code></li> </ul> <p>This makes it easy to generate quick diagnostic plots or high-quality figures for publications.</p> <p>Let\u2019s start with a simple plot, followed by a fully customized one.</p>"},{"location":"tutorial/#data-preprocessing-and-cleaning","title":"Data Preprocessing and Cleaning\u00b6","text":"<p>Before modeling or computing spectral products, it\u2019s often helpful to inspect and clean your light curve. STELA provides a number of tools to help you do this safely and flexibly.</p> <p>All preprocessing tools live in the <code>Preprocessing</code> class. You don\u2019t need to create an instance\u2014just call the methods directly.</p> <p>Most of these functions let you:</p> <ul> <li>Visualize changes with <code>plot=True</code></li> <li>Avoid committing changes by setting <code>save=False</code> (recommended for first passes)</li> <li>Clean data without modifying the original file or LightCurve object</li> </ul> <p>Common tasks include:</p> <ul> <li><code>remove_nans()</code> \u2014 drop entries with missing data</li> <li><code>remove_outliers()</code> \u2014 exclude extreme points using the interquartile range (IQR)</li> <li><code>trim_time_segment()</code> \u2014 keep only part of the light curve (e.g., for campaign segmentation)</li> <li><code>polynomial_detrend()</code> \u2014 subtract long-term trends before analysis</li> <li><code>standardize()</code> and <code>unstandardize()</code> \u2014 mean-center and rescale your data</li> </ul> <p>Here\u2019s how to explore these features in practice:</p>"},{"location":"tutorial/#checking-for-normality","title":"Checking for Normality\u00b6","text":"<p>Before fitting a Gaussian Process model, it's important to check whether your light curve's flux distribution is approximately normal. Gaussian processes assume the data is drawn from a Gaussian distribution, so significant departures from normality can hurt performance or lead to unstable fits.</p> <p>STELA gives you a few tools in the <code>Preprocessing</code> class to assess and correct this via transformations:</p> <ul> <li><code>generate_qq_plot()</code>: Plots a Q-Q (quantile-quantile) comparison of your data against a normal distribution. If the points fall roughly on a straight 1:1 line, the data is reasonably normal.</li> <li><code>check_normal()</code>: Performs a formal statistical test for normality at significance level of 0.05. STELA automatically chooses the most appropriate test depending on your sample size, using Shapiro-Wilk for small sample (n&lt;50) and Lilliefors for larger ones (n&gt;50). A Q-Q plot can also be produced here using <code>plot=True</code>.</li> <li><code>check_boxcox_normal()</code>: Applies a Box-Cox transformation and re-runs the normality test to see the degree to which the transformation improves normality. A Q-Q plot with both the original and transformed data overlaid can also be produced here using <code>plot=True</code>.</li> </ul> <p>Note: The Box-Cox transform is a power-law transformation that reshapes the data to better resemble a normal distribution, and STELA automatically optimizes the transformation parameter (\u03bb) for your dataset.</p> <p>Don't want to do all this beforehand? Use the <code>enforce_normality=True</code> in the <code>GaussianProcess</code> class (see below).</p> <p>Let\u2019s try it out and see how our light curve looks.</p>"},{"location":"tutorial/#gaussian-process-models-in-stela","title":"Gaussian Process Models in STELA\u00b6","text":"<p>To model light curve variability in STELA, we use the <code>GaussianProcess</code> class. In summary, a Gaussian Process (GP) is a flexible, non-parametric model that treats your data as samples drawn from a multivariate normal distribution, with covariance determined by a kernel function. This allows us to infer what the light curve may have looked like between or beyond the observed times, while also accounting for uncertainty in a principled way.</p> <p>If you\u2019d rather not check for normality yourself (as in the previous section), simply set <code>enforce_normality=True</code>. STELA will automatically assess whether your light curve\u2019s flux distribution is sufficiently Gaussian, and if not, apply a Box-Cox transformation. It will then recheck the transformed data to confirm whether normality has improved.</p> <p>Want to learn more on GPs? Read the Introduction to Gaussian Processes part of the documentation (under construction, this won't work yet) to understand the theory behind what we\u2019re doing here.</p> <p>In STELA, you pass your <code>LightCurve</code> object to the <code>GaussianProcess</code> class to create the model.</p>"},{"location":"tutorial/#kernel-functions","title":"Kernel Functions\u00b6","text":"<p>The covariance function\u2014or kernel\u2014controls how the model expects different points in time to relate to one another. Later in the tutorial we introduce method for how to go about choosing a functional form. STELA includes several kernel options, specified via the <code>kernel_form</code> argument:</p> <ol> <li>Radial Basis Function (RBF) \u2013 very smooth</li> <li>Rational Quadratic (RQ) \u2013 similar to RBF, but with variable smoothness and an additional hyperparameter</li> <li>Matern kernels \u2013 less smooth; controlled by \u03bd:<ul> <li><code>Matern12</code> (\u03bd = 1/2)</li> <li><code>Matern32</code> (\u03bd = 3/2)</li> <li><code>Matern52</code> (\u03bd = 5/2)</li> </ul> <p>(Note: \u03bd is a fixed kernel parameter, not a hyperparameter to be optimized)</p> </li> <li>Spectral Mixture kernel \u2013 captures periodic or quasi-periodic behavior<ul> <li>Syntax: <code>\"SpectralMixture, N\"</code> sets the number of mixtures to <code>N</code> (e.g., <code>\"SpectralMixture, 4\"</code>)</li> </ul> </li> </ol>"},{"location":"tutorial/#error-and-noise-handling","title":"Error and Noise Handling\u00b6","text":"<p>STELA\u2019s GP models automatically incorporate your observational errors if you\u2019ve provided them in the <code>LightCurve</code>. These act as fixed noise levels for each data point in the likelihood.</p> <p>In addition, you can optionally fit an extra white noise component to account for unmodeled variability. This is enabled via <code>white_noise=True</code>.</p>"},{"location":"tutorial/#setting-up-a-gp-model","title":"Setting Up a GP Model\u00b6","text":"<p>You can create a <code>GaussianProcess</code> model by passing in your <code>LightCurve</code> object, along with options for kernel choice, noise modeling, and normality checking. This step only sets up the model\u2014it doesn\u2019t train it yet.</p> <p>Let\u2019s initialize a model and prepare it for training, although we could just set <code>run_training=True</code> for future reference.</p>"},{"location":"tutorial/#training-the-model","title":"Training the Model\u00b6","text":"<p>Once your Gaussian Process model is initialized, you can train it to fit your data. This step adjusts the kernel\u2019s hyperparameters (e.g., length scale, amplitude, white noise) so the model best represents the variability in your light curve.</p> <p>Training is performed by minimizing the Negative Log Marginal Likelihood (NLML) \u2014 a standard loss function for GPs. It measures how likely your observed data is under the current model.</p> <p>More precisely, the marginal likelihood is the probability of observing your data given the kernel hyperparameters (e.g., length scale, amplitude), after integrating over all possible functions the GP could represent. Lower NLML means the model is assigning higher probability to your observed data.</p>"},{"location":"tutorial/#running-training","title":"Running Training\u00b6","text":"<p>To train your model, call the <code>.train()</code> method on your <code>GaussianProcess</code> object. This adjusts the kernel\u2019s hyperparameters so the model best fits your light curve.</p> <p>If you don\u2019t know much about optimization, don\u2019t worry! While the default training settings (<code>learn_rate=0.1</code>, <code>num_iter=500</code>) are usually sufficient for most light curves and kernels, it's good practice to plot the NLML:</p> <p>Recommended: Set <code>plot_training=True</code>. This will show you how the training loss (NLML) evolves. You want to see the curve decrease and then flatten out by the end \u2014 that means the model has reached a stable fit. Otherwise see the subsection below.</p>"},{"location":"tutorial/#training-parameters","title":"Training Parameters\u00b6","text":"<ul> <li><code>num_iter</code>: how many training steps to take (optional, default 500)</li> <li><code>learn_rate</code>: how fast the optimizer updates the parameters (optional, default 0.1)</li> <li><code>plot</code>: show a plot of the loss improving over time (optional, default False)</li> <li><code>verbose</code>: print optimization details (optional, default False)</li> </ul>"},{"location":"tutorial/#what-to-try-if-things-dont-look-right","title":"What to Try if Things Don\u2019t Look Right\u00b6","text":"<ul> <li>If the loss curve is very slowly decreasing, try increasing <code>learn_rate</code> (e.g., to <code>0.3</code> or <code>0.5</code>)</li> <li>If the curve is bouncy or erratic, lower <code>learn_rate</code> (e.g., to <code>0.01</code>)</li> <li>If the curve is still going down at the end, increase <code>num_iter</code> (e.g., to <code>1000</code>)</li> </ul>"},{"location":"tutorial/#selecting-a-kernel-form","title":"Selecting a Kernel Form\u00b6","text":"<p>When you create a Gaussian Process model, a key decision is which kernel to use. The kernel affects how the model captures the observed variability, including how smooth, erratic, or periodic the signal is.</p> <p>This choice directly affects how well the GP can capture the underlying structure of your light curve and thus should be assessed on each light curve independently.</p>"},{"location":"tutorial/#option-1-let-stela-select-automatically","title":"Option 1: Let STELA Select Automatically\u00b6","text":"<p>If you're unsure which kernel is best, you can pass <code>'auto'</code> or a list of kernel names to <code>kernel_form</code>. STELA will try each one, train a model for each (even if <code>run_training=False</code>), and select the best based on Akaike Information Criterion (AIC). AIC rewards higher likelihood, and punishes model complexity based on the number of kernel hyperparameters.</p>"},{"location":"tutorial/#option-2-compare-kernels-manually","title":"Option 2: Compare Kernels Manually\u00b6","text":"<p>If you'd like to explore different kernels yourself, you can train them one at a time and compare their AIC/BIC scores manually.</p>"},{"location":"tutorial/#predicting-missing-data-in-the-gaps","title":"Predicting Missing Data in the Gaps\u00b6","text":"<p>Once your Gaussian Process model is trained, there are two main ways to evaluate it:</p>"},{"location":"tutorial/#1-predict-posterior-mean-and-standard-deviation","title":"1. <code>.predict()</code> : Posterior Mean and Standard Deviation\u00b6","text":"<p>The <code>.predict()</code> method gives you the posterior mean and standard deviation of the GP at each time point. These represent the best guess of the true underlying light curve and the uncertainty in that guess.</p> <p>This is useful for:</p> <ul> <li>Visualizing the smoothed light curve</li> <li>Understanding where the model is confident vs. uncertain</li> </ul> <p>But: we don't typically use <code>.predict()</code> results directly in downstream analyses like power spectrum or lag estimation, because those calculations require full realizations of plausible light curves \u2014 not just the mean prediction.</p>"},{"location":"tutorial/#2-sample-draw-posterior-realizations","title":"2. <code>.sample()</code> : Draw Posterior Realizations\u00b6","text":"<p>The <code>.sample()</code> method draws full realizations from the trained GP\u2019s posterior distribution \u2014 that is, plausible versions of what the true light curve could have looked like, given your data and the learned model.</p> <p>These samples allow STELA to intuitively propagate uncertainties from the GP method onto the data products below:</p> <p>Specifically, instead of computing these data products just once, STELA:</p> <ol> <li>Computes the result for each individual realization</li> <li>Aggregates the results across all samples</li> <li>Reports the mean and spread (standard deviation) of the final measurement</li> </ol>"},{"location":"tutorial/#how-samples-are-used-internally","title":"How Samples Are Used Internally\u00b6","text":"<p>Many STELA classes \u2014 including:</p> <ul> <li><code>PowerSpectrum</code></li> <li><code>LagFrequencySpectrum</code></li> <li><code>CrossSpectrum</code></li> <li><code>Coherence</code></li> <li><code>CrossCorrelation</code> (for GP mode)</li> </ul> <p>will automatically use the most recently generated samples from the model when you use the model as an input in these classes. You do not need to pass the samples in manually.</p> <p>If you don\u2019t generate samples yourself, STELA will do it for you upon input of the model into any of the classes above (default: 1000 samples).</p>"},{"location":"tutorial/#frequency-resolved-analysis-tools","title":"Frequency-Resolved Analysis Tools\u00b6","text":"<p>STELA includes several classes for computing main frequency-resolved data products. These tools help quantify variability, correlation, and time delays between light curves across different timescales/temporal frequencies.</p> <p>All of the classes listed below follow a similar interface and workflow:</p>"},{"location":"tutorial/#shared-structure-and-workflow","title":"Shared Structure and Workflow\u00b6","text":"<p>Each class takes as input:</p> <ul> <li>Either a pair of light curves (<code>LightCurve</code> objects), or</li> <li>A pair of trained GP models (<code>GaussianProcess</code> objects)</li> </ul> <p>If you pass GP models, STELA will automatically use the most recently generated GP samples. If you haven\u2019t generated samples yet, the class will generate 1000 by default.</p> <p>All classes support (via user-defined inputs):</p> <ul> <li>Frequency binning (log or linear)</li> <li>Custom bin edges</li> <li>Frequency range controls (<code>fmin</code>, <code>fmax</code>)<ul> <li>Use <code>fmin=\"auto\"</code>, <code>fmax=\"auto\"</code> to use the minimum and maximum frequency possible given the lightcurve duration, sampling rate.</li> </ul> </li> <li>A <code>.plot()</code> method to quickly visualize the result, including the coherence spectrum for lags.</li> <li>An optional <code>.count_frequencies_in_bins()</code> method (for diagnostics)</li> </ul>"},{"location":"tutorial/#what-each-class-computes","title":"What Each Class Computes\u00b6","text":"<ul> <li><p>PowerSpectrum Quantifies the variability amplitude in a single light curve as a function of frequency.</p> </li> <li><p>CrossSpectrum Measures the complex correlation between two light curves in the frequency domain.</p> </li> <li><p>LagFrequencySpectrum Extracts the phase lag between two light curves as a function of frequency. Coherence spectrum also computed.</p> </li> <li><p>LagEnergySpectrum Computes the time lag between a broad reference band and several comparison bands (e.g., energy bins), averaged over a specified frequency range. Coherence spectrum also computed.</p> </li> <li><p>Coherence Measures how strongly two light curves are linearly related at each frequency (ranges from 0 to 1). Additional coherence from correlated noise can be subtracted using <code>subtract_bias=True</code>. This is not needed for GP realizations, which effectively removes the coherence from noise.</p> </li> </ul> <p>In the following examples, we\u2019ll show how to use each of these classes:</p>"},{"location":"reference/check_inputs/","title":"_check_inputs","text":""},{"location":"reference/clarify_warnings/","title":"_clarify_warnings","text":""},{"location":"reference/coherence/","title":"coherence","text":""},{"location":"reference/coherence/#stela_toolkit.coherence.Coherence","title":"<code>Coherence</code>","text":"<p>Compute the frequency-dependent coherence between two light curves or GP models.</p> <p>This class estimates the coherence spectrum, which quantifies the degree of linear correlation between two time series as a function of frequency. Coherence values range from 0 to 1, with values near 1 indicating a strong linear relationship at that frequency.</p> <p>Inputs can be either LightCurve objects or trained GaussianProcess models from this package. If GP models are provided and posterior samples already exist, those are used. If no samples exist, 1000 GP realizations will be generated automatically on a 1000-point grid.</p> <p>If both inputs are GP models, the coherence is computed for each sample pair and the mean and standard deviation across samples are returned. Otherwise, coherence is computed on the raw input light curves.</p> <p>Poisson noise bias correction is supported and may be enabled to correct for uncorrelated noise.</p>"},{"location":"reference/coherence/#stela_toolkit.coherence.Coherence--parameters","title":"Parameters","text":"<p>lc_or_model1 : LightCurve or GaussianProcess     First input light curve or trained GP model. lc_or_model2 : LightCurve or GaussianProcess     Second input light curve or trained GP model. fmin : float or 'auto', optional     Minimum frequency for the coherence spectrum. If 'auto', uses the lowest nonzero FFT frequency. fmax : float or 'auto', optional     Maximum frequency. If 'auto', uses the Nyquist frequency. num_bins : int, optional     Number of frequency bins. bin_type : str, optional     Type of frequency binning ('log' or 'linear'). bin_edges : array-like, optional     Custom frequency bin edges. subtract_noise_bias : bool, optional     Whether to subtract Poisson noise bias from the coherence spectrum. bkg1 : float, optional     Background count rate for lightcurve 1 (used in noise bias correction). bkg2 : float, optional     Background count rate for lightcurve 2.</p>"},{"location":"reference/coherence/#stela_toolkit.coherence.Coherence--attributes","title":"Attributes","text":"<p>freqs : array-like     Frequency bin centers. freq_widths : array-like     Widths of each frequency bin. cohs : array-like     Coherence values. coh_errors : array-like     Uncertainties in the coherence values.</p> Source code in <code>stela_toolkit/coherence.py</code> <pre><code>class Coherence:\n    \"\"\"\n    Compute the frequency-dependent coherence between two light curves or GP models.\n\n    This class estimates the coherence spectrum, which quantifies the degree of linear correlation\n    between two time series as a function of frequency. Coherence values range from 0 to 1,\n    with values near 1 indicating a strong linear relationship at that frequency.\n\n    Inputs can be either LightCurve objects or trained GaussianProcess models from this package.\n    If GP models are provided and posterior samples already exist, those are used.\n    If no samples exist, 1000 GP realizations will be generated automatically on a 1000-point grid.\n\n    If both inputs are GP models, the coherence is computed for each sample pair and the\n    mean and standard deviation across samples are returned. Otherwise, coherence is computed\n    on the raw input light curves.\n\n    Poisson noise bias correction is supported and may be enabled to correct for uncorrelated noise.\n\n    Parameters\n    ----------\n    lc_or_model1 : LightCurve or GaussianProcess\n        First input light curve or trained GP model.\n    lc_or_model2 : LightCurve or GaussianProcess\n        Second input light curve or trained GP model.\n    fmin : float or 'auto', optional\n        Minimum frequency for the coherence spectrum. If 'auto', uses the lowest nonzero FFT frequency.\n    fmax : float or 'auto', optional\n        Maximum frequency. If 'auto', uses the Nyquist frequency.\n    num_bins : int, optional\n        Number of frequency bins.\n    bin_type : str, optional\n        Type of frequency binning ('log' or 'linear').\n    bin_edges : array-like, optional\n        Custom frequency bin edges.\n    subtract_noise_bias : bool, optional\n        Whether to subtract Poisson noise bias from the coherence spectrum.\n    bkg1 : float, optional\n        Background count rate for lightcurve 1 (used in noise bias correction).\n    bkg2 : float, optional\n        Background count rate for lightcurve 2.\n\n    Attributes\n    ----------\n    freqs : array-like\n        Frequency bin centers.\n    freq_widths : array-like\n        Widths of each frequency bin.\n    cohs : array-like\n        Coherence values.\n    coh_errors : array-like\n        Uncertainties in the coherence values.\n    \"\"\"\n\n    def __init__(self,\n                 lc_or_model1,\n                 lc_or_model2,\n                 fmin='auto',\n                 fmax='auto',\n                 num_bins=None,\n                 bin_type=\"log\",\n                 bin_edges=[],\n                 subtract_noise_bias=True,\n                 bkg1=0,\n                 bkg2=0):\n\n        # To do: determine if or if not Poisson statistics for the user\n        input_data = _CheckInputs._check_lightcurve_or_model(lc_or_model1)\n        if input_data['type'] == 'model':\n            self.times1, self.rates1 = input_data['data']\n        else:\n            self.times1, self.rates1, _ = input_data['data']\n\n        input_data = _CheckInputs._check_lightcurve_or_model(lc_or_model2)\n        if input_data['type'] == 'model':\n            self.times2, self.rates2 = input_data['data']\n        else:\n            self.times2, self.rates2, _ = input_data['data']\n        _CheckInputs._check_input_bins(num_bins, bin_type, bin_edges)\n\n        if not np.allclose(self.times1, self.times2):\n            raise ValueError(\"The time arrays of the two light curves must be identical.\")\n\n        # Use absolute min and max frequencies if set to 'auto'\n        self.dt = np.diff(self.times1)[0]\n        self.fmin = np.fft.rfftfreq(len(self.rates1), d=self.dt)[1] if fmin == 'auto' else fmin\n        self.fmax = np.fft.rfftfreq(len(self.rates1), d=self.dt)[-1] if fmax == 'auto' else fmax  # nyquist frequency\n\n        self.num_bins = num_bins\n        self.bin_type = bin_type\n        self.bin_edges = bin_edges\n\n        self.bkg1 = bkg1\n        self.bkg2 = bkg2\n\n        # Check if the input rates are for multiple realizations\n        # this needs to be corrected for handling different shapes and dim val1 != dim val2\n        # namely for multiple observations\n        if len(self.rates1.shape) == 2 and len(self.rates2.shape) == 2:\n            coherence_spectrum = self.compute_stacked_coherence()\n        else:\n            coherence_spectrum = self.compute_coherence(subtract_noise_bias=subtract_noise_bias)\n\n        self.freqs, self.freq_widths, self.cohs, self.coh_errors = coherence_spectrum\n\n    def compute_coherence(self, times1=None, rates1=None, times2=None, rates2=None, subtract_noise_bias=True):\n        \"\"\"\n        Compute the coherence spectrum between two light curves.\n\n        Parameters\n        ----------\n        times1, rates1 : array-like, optional\n            Time and rate values for the first light curve. Defaults to object attributes.\n        times2, rates2 : array-like, optional\n            Time and rate values for the second light curve. Defaults to object attributes.\n        subtract_noise_bias : bool, optional\n            Whether to subtract the estimated noise bias.\n\n        Returns\n        -------\n        freqs : array-like\n            Frequency bin centers.\n        freq_widths : array-like\n            Frequency bin widths.\n        coherence : array-like\n            Coherence spectrum.\n        None\n            Reserved for compatibility (with the stacked method).\n        \"\"\"\n\n        times1 = self.times1 if times1 is None else times1\n        rates1 = self.rates1 if rates1 is None else rates1\n        times2 = self.times2 if times2 is None else times2\n        rates2 = self.rates2 if rates2 is None else rates2\n\n        lc1 = LightCurve(times=times1, rates=rates1)\n        lc2 = LightCurve(times=times2, rates=rates2)\n        cross_spectrum = CrossSpectrum(\n            lc1, lc2,\n            fmin=self.fmin, fmax=self.fmax,\n            num_bins=self.num_bins, bin_type=self.bin_type, bin_edges=self.bin_edges\n        )\n\n        power_spectrum1 = PowerSpectrum(\n            lc1,\n            fmin=self.fmin, fmax=self.fmax,\n            num_bins=self.num_bins, bin_type=self.bin_type, bin_edges=self.bin_edges\n        )\n        power_spectrum2 = PowerSpectrum(\n            lc2,\n            fmin=self.fmin, fmax=self.fmax,\n            num_bins=self.num_bins, bin_type=self.bin_type, bin_edges=self.bin_edges\n        )\n\n        ps1 = power_spectrum1.powers\n        ps2 = power_spectrum2.powers\n        cs = cross_spectrum.cs\n\n        if subtract_noise_bias:\n            bias = self.compute_bias(ps1, ps2)\n        else:\n            bias = 0\n\n        coherence = (np.abs(cs) ** 2 - bias) / (ps1 * ps2)\n        return power_spectrum1.freqs, power_spectrum1.freq_widths, coherence, None\n\n    def compute_stacked_coherence(self):\n        \"\"\"\n        Compute the coherence from stacked realizations of the light curves.\n\n        For multiple realizations (GP samples), this method computes the\n        coherence for each pair of realizations and returns the mean and standard deviation.\n\n        Parameters\n        ----------\n        subtract_noise_bias : bool, optional\n            Whether to subtract noise bias from each realization.\n\n        Returns\n        -------\n        freqs : array-like\n            Frequency bin centers.\n        freq_widths : array-like\n            Frequency bin widths.\n        coherence_mean : array-like\n            Mean coherence spectrum across realizations.\n        coherence_std : array-like\n            Standard deviation of the coherence across realizations.\n        \"\"\"\n\n        coherences = []\n        for i in range(self.rates1.shape[0]):\n            coherence_spectrum = self.compute_coherence(times1=self.times1, rates1=self.rates1[i],\n                                                        times2=self.times2, rates2=self.rates2[i],\n                                                        subtract_noise_bias=False\n                                                    )\n            freqs, freq_widths, coherence, _ = coherence_spectrum\n            coherences.append(coherence)\n\n        coherences = np.vstack(coherences)\n        coherences_mean = np.mean(coherences, axis=0)\n        coherences_std = np.std(coherences, axis=0)\n\n        return freqs, freq_widths, coherences_mean, coherences_std\n\n    def compute_bias(self, power_spectrum1, power_spectrum2):\n        \"\"\"\n        Estimate the Poisson noise bias for the coherence calculation. \n\n        Parameters\n        ----------\n        power_spectrum1 : array-like\n            Power spectrum of the first light curve.\n        power_spectrum2 : array-like\n            Power spectrum of the second light curve.\n\n        Returns\n        -------\n        bias : array-like\n            Estimated noise bias per frequency bin.\n        \"\"\"\n\n        mean1 = np.mean(self.rates1)\n        mean2 = np.mean(self.rates2)\n\n        pnoise1 = 2 * (mean1 + self.bkg1) / mean1 ** 2\n        pnoise2 = 2 * (mean2 + self.bkg2) / mean2 ** 2\n\n        bias = (\n            pnoise2 * (power_spectrum1 - pnoise1)\n            + pnoise1 * (power_spectrum2 - pnoise2)\n            + pnoise1 * pnoise2\n        )\n        num_freq = self.count_frequencies_in_bins()\n        bias /= num_freq\n        return bias\n\n    def plot(self, freqs=None, freq_widths=None, cohs=None, coh_errors=None, **kwargs):\n        \"\"\"\n        Plot the coherence spectrum.\n\n        Parameters\n        ----------\n        **kwargs : dict\n            Additional keyword arguments for plot customization (e.g., xlabel, xscale).\n        \"\"\"\n\n        freqs = self.freqs if freqs is None else freqs\n        freq_widths = self.freq_widths if freq_widths is None else freq_widths\n        cohs = self.cohs if cohs is None else cohs\n        coh_errors = self.coh_errors if coh_errors is None else coh_errors\n\n        kwargs.setdefault('xlabel', 'Frequency')\n        kwargs.setdefault('ylabel', 'Coherence')\n        kwargs.setdefault('xscale', 'log')\n        Plotter.plot(\n            x=freqs, y=cohs, xerr=freq_widths, yerr=coh_errors, **kwargs\n        )\n\n    def count_frequencies_in_bins(self, fmin=None, fmax=None, num_bins=None, bin_type=None, bin_edges=[]):\n        \"\"\"\n        Counts the number of frequencies in each frequency bin.\n        Wrapper method to use FrequencyBinning.count_frequencies_in_bins with class attributes.\n        \"\"\"\n\n        return FrequencyBinning.count_frequencies_in_bins(\n            self, fmin=fmin, fmax=fmax, num_bins=num_bins, bin_type=bin_type, bin_edges=bin_edges\n        )\n</code></pre>"},{"location":"reference/coherence/#stela_toolkit.coherence.Coherence.compute_bias","title":"<code>compute_bias(power_spectrum1, power_spectrum2)</code>","text":"<p>Estimate the Poisson noise bias for the coherence calculation. </p>"},{"location":"reference/coherence/#stela_toolkit.coherence.Coherence.compute_bias--parameters","title":"Parameters","text":"<p>power_spectrum1 : array-like     Power spectrum of the first light curve. power_spectrum2 : array-like     Power spectrum of the second light curve.</p>"},{"location":"reference/coherence/#stela_toolkit.coherence.Coherence.compute_bias--returns","title":"Returns","text":"<p>bias : array-like     Estimated noise bias per frequency bin.</p> Source code in <code>stela_toolkit/coherence.py</code> <pre><code>def compute_bias(self, power_spectrum1, power_spectrum2):\n    \"\"\"\n    Estimate the Poisson noise bias for the coherence calculation. \n\n    Parameters\n    ----------\n    power_spectrum1 : array-like\n        Power spectrum of the first light curve.\n    power_spectrum2 : array-like\n        Power spectrum of the second light curve.\n\n    Returns\n    -------\n    bias : array-like\n        Estimated noise bias per frequency bin.\n    \"\"\"\n\n    mean1 = np.mean(self.rates1)\n    mean2 = np.mean(self.rates2)\n\n    pnoise1 = 2 * (mean1 + self.bkg1) / mean1 ** 2\n    pnoise2 = 2 * (mean2 + self.bkg2) / mean2 ** 2\n\n    bias = (\n        pnoise2 * (power_spectrum1 - pnoise1)\n        + pnoise1 * (power_spectrum2 - pnoise2)\n        + pnoise1 * pnoise2\n    )\n    num_freq = self.count_frequencies_in_bins()\n    bias /= num_freq\n    return bias\n</code></pre>"},{"location":"reference/coherence/#stela_toolkit.coherence.Coherence.compute_coherence","title":"<code>compute_coherence(times1=None, rates1=None, times2=None, rates2=None, subtract_noise_bias=True)</code>","text":"<p>Compute the coherence spectrum between two light curves.</p>"},{"location":"reference/coherence/#stela_toolkit.coherence.Coherence.compute_coherence--parameters","title":"Parameters","text":"<p>times1, rates1 : array-like, optional     Time and rate values for the first light curve. Defaults to object attributes. times2, rates2 : array-like, optional     Time and rate values for the second light curve. Defaults to object attributes. subtract_noise_bias : bool, optional     Whether to subtract the estimated noise bias.</p>"},{"location":"reference/coherence/#stela_toolkit.coherence.Coherence.compute_coherence--returns","title":"Returns","text":"<p>freqs : array-like     Frequency bin centers. freq_widths : array-like     Frequency bin widths. coherence : array-like     Coherence spectrum. None     Reserved for compatibility (with the stacked method).</p> Source code in <code>stela_toolkit/coherence.py</code> <pre><code>def compute_coherence(self, times1=None, rates1=None, times2=None, rates2=None, subtract_noise_bias=True):\n    \"\"\"\n    Compute the coherence spectrum between two light curves.\n\n    Parameters\n    ----------\n    times1, rates1 : array-like, optional\n        Time and rate values for the first light curve. Defaults to object attributes.\n    times2, rates2 : array-like, optional\n        Time and rate values for the second light curve. Defaults to object attributes.\n    subtract_noise_bias : bool, optional\n        Whether to subtract the estimated noise bias.\n\n    Returns\n    -------\n    freqs : array-like\n        Frequency bin centers.\n    freq_widths : array-like\n        Frequency bin widths.\n    coherence : array-like\n        Coherence spectrum.\n    None\n        Reserved for compatibility (with the stacked method).\n    \"\"\"\n\n    times1 = self.times1 if times1 is None else times1\n    rates1 = self.rates1 if rates1 is None else rates1\n    times2 = self.times2 if times2 is None else times2\n    rates2 = self.rates2 if rates2 is None else rates2\n\n    lc1 = LightCurve(times=times1, rates=rates1)\n    lc2 = LightCurve(times=times2, rates=rates2)\n    cross_spectrum = CrossSpectrum(\n        lc1, lc2,\n        fmin=self.fmin, fmax=self.fmax,\n        num_bins=self.num_bins, bin_type=self.bin_type, bin_edges=self.bin_edges\n    )\n\n    power_spectrum1 = PowerSpectrum(\n        lc1,\n        fmin=self.fmin, fmax=self.fmax,\n        num_bins=self.num_bins, bin_type=self.bin_type, bin_edges=self.bin_edges\n    )\n    power_spectrum2 = PowerSpectrum(\n        lc2,\n        fmin=self.fmin, fmax=self.fmax,\n        num_bins=self.num_bins, bin_type=self.bin_type, bin_edges=self.bin_edges\n    )\n\n    ps1 = power_spectrum1.powers\n    ps2 = power_spectrum2.powers\n    cs = cross_spectrum.cs\n\n    if subtract_noise_bias:\n        bias = self.compute_bias(ps1, ps2)\n    else:\n        bias = 0\n\n    coherence = (np.abs(cs) ** 2 - bias) / (ps1 * ps2)\n    return power_spectrum1.freqs, power_spectrum1.freq_widths, coherence, None\n</code></pre>"},{"location":"reference/coherence/#stela_toolkit.coherence.Coherence.compute_stacked_coherence","title":"<code>compute_stacked_coherence()</code>","text":"<p>Compute the coherence from stacked realizations of the light curves.</p> <p>For multiple realizations (GP samples), this method computes the coherence for each pair of realizations and returns the mean and standard deviation.</p>"},{"location":"reference/coherence/#stela_toolkit.coherence.Coherence.compute_stacked_coherence--parameters","title":"Parameters","text":"<p>subtract_noise_bias : bool, optional     Whether to subtract noise bias from each realization.</p>"},{"location":"reference/coherence/#stela_toolkit.coherence.Coherence.compute_stacked_coherence--returns","title":"Returns","text":"<p>freqs : array-like     Frequency bin centers. freq_widths : array-like     Frequency bin widths. coherence_mean : array-like     Mean coherence spectrum across realizations. coherence_std : array-like     Standard deviation of the coherence across realizations.</p> Source code in <code>stela_toolkit/coherence.py</code> <pre><code>def compute_stacked_coherence(self):\n    \"\"\"\n    Compute the coherence from stacked realizations of the light curves.\n\n    For multiple realizations (GP samples), this method computes the\n    coherence for each pair of realizations and returns the mean and standard deviation.\n\n    Parameters\n    ----------\n    subtract_noise_bias : bool, optional\n        Whether to subtract noise bias from each realization.\n\n    Returns\n    -------\n    freqs : array-like\n        Frequency bin centers.\n    freq_widths : array-like\n        Frequency bin widths.\n    coherence_mean : array-like\n        Mean coherence spectrum across realizations.\n    coherence_std : array-like\n        Standard deviation of the coherence across realizations.\n    \"\"\"\n\n    coherences = []\n    for i in range(self.rates1.shape[0]):\n        coherence_spectrum = self.compute_coherence(times1=self.times1, rates1=self.rates1[i],\n                                                    times2=self.times2, rates2=self.rates2[i],\n                                                    subtract_noise_bias=False\n                                                )\n        freqs, freq_widths, coherence, _ = coherence_spectrum\n        coherences.append(coherence)\n\n    coherences = np.vstack(coherences)\n    coherences_mean = np.mean(coherences, axis=0)\n    coherences_std = np.std(coherences, axis=0)\n\n    return freqs, freq_widths, coherences_mean, coherences_std\n</code></pre>"},{"location":"reference/coherence/#stela_toolkit.coherence.Coherence.count_frequencies_in_bins","title":"<code>count_frequencies_in_bins(fmin=None, fmax=None, num_bins=None, bin_type=None, bin_edges=[])</code>","text":"<p>Counts the number of frequencies in each frequency bin. Wrapper method to use FrequencyBinning.count_frequencies_in_bins with class attributes.</p> Source code in <code>stela_toolkit/coherence.py</code> <pre><code>def count_frequencies_in_bins(self, fmin=None, fmax=None, num_bins=None, bin_type=None, bin_edges=[]):\n    \"\"\"\n    Counts the number of frequencies in each frequency bin.\n    Wrapper method to use FrequencyBinning.count_frequencies_in_bins with class attributes.\n    \"\"\"\n\n    return FrequencyBinning.count_frequencies_in_bins(\n        self, fmin=fmin, fmax=fmax, num_bins=num_bins, bin_type=bin_type, bin_edges=bin_edges\n    )\n</code></pre>"},{"location":"reference/coherence/#stela_toolkit.coherence.Coherence.plot","title":"<code>plot(freqs=None, freq_widths=None, cohs=None, coh_errors=None, **kwargs)</code>","text":"<p>Plot the coherence spectrum.</p>"},{"location":"reference/coherence/#stela_toolkit.coherence.Coherence.plot--parameters","title":"Parameters","text":"<p>**kwargs : dict     Additional keyword arguments for plot customization (e.g., xlabel, xscale).</p> Source code in <code>stela_toolkit/coherence.py</code> <pre><code>def plot(self, freqs=None, freq_widths=None, cohs=None, coh_errors=None, **kwargs):\n    \"\"\"\n    Plot the coherence spectrum.\n\n    Parameters\n    ----------\n    **kwargs : dict\n        Additional keyword arguments for plot customization (e.g., xlabel, xscale).\n    \"\"\"\n\n    freqs = self.freqs if freqs is None else freqs\n    freq_widths = self.freq_widths if freq_widths is None else freq_widths\n    cohs = self.cohs if cohs is None else cohs\n    coh_errors = self.coh_errors if coh_errors is None else coh_errors\n\n    kwargs.setdefault('xlabel', 'Frequency')\n    kwargs.setdefault('ylabel', 'Coherence')\n    kwargs.setdefault('xscale', 'log')\n    Plotter.plot(\n        x=freqs, y=cohs, xerr=freq_widths, yerr=coh_errors, **kwargs\n    )\n</code></pre>"},{"location":"reference/cross_correlation/","title":"cross_correlation","text":""},{"location":"reference/cross_correlation/#stela_toolkit.cross_correlation.CrossCorrelation","title":"<code>CrossCorrelation</code>","text":"<p>Compute the time-domain cross-correlation function (CCF) between two light curves or GP models.</p> <p>This class supports three primary use cases:</p> <ol> <li> <p>Regularly sampled LightCurve objects: The CCF is computed via direct shifting using Pearson correlation coefficients across lag values.</p> </li> <li> <p>Irregularly sampled LightCurve objects: Uses the interpolated cross-correlation method (ICCF), introduced by Gaskell &amp; Peterson (1987), which linearly interpolates one light curve onto the other's grid to allow for lag estimation despite gaps.</p> </li> <li> <p>GaussianProcess models: If both inputs are trained GP models, the CCF is computed across all sampled realizations and averaged. If no samples exist, 1000 will be generated automatically on a 1000-point grid. Lag uncertainties are derived from the spread in lag values across realizations.</p> </li> </ol> <p>Monte Carlo resampling is also available for estimating confidence intervals using observational error bars.</p>"},{"location":"reference/cross_correlation/#stela_toolkit.cross_correlation.CrossCorrelation--parameters","title":"Parameters","text":"<p>lc_or_model1 : LightCurve or GaussianProcess     First input light curve or trained GP model. lc_or_model2 : LightCurve or GaussianProcess     Second input light curve or trained GP model. run_monte_carlo : bool, optional     Whether to estimate lag uncertainties using Monte Carlo resampling. n_trials : int, optional     Number of Monte Carlo trials. min_lag : float, optional     Minimum lag to evaluate, default \"auto\" results in min_lag = - duration / 2 max_lag : float, optional     Maximum lag to evaluate, default \"auto\" results in max_lag = duration / 2 dt : float, optional     Time step to use for interpolation, less than sampling rate of the data, default of      \"auto\" results in dt = average sampling rate / 5.</p> float, optional <p>Threshold (fraction of peak CCF) for defining centroid lag region.</p> <p>mode : {'regular', 'interp'}, optional     CCF computation mode. Use 'regular' for direct shifting (requires aligned time grids),     or 'interp' for ICCF-style interpolation. rmax_threshold : float, optional     Monte Carlo trials with maximum correlation below this value are discarded.</p>"},{"location":"reference/cross_correlation/#stela_toolkit.cross_correlation.CrossCorrelation--attributes","title":"Attributes","text":"<p>lags : ndarray     Array of lag values evaluated. ccf : ndarray     Cross-correlation coefficients. peak_lag : float     Lag corresponding to the peak correlation. centroid_lag : float     Centroid lag from the high-correlation region. rmax : float     Maximum correlation coefficient. peak_lags_mc : ndarray or None     Peak lags from Monte Carlo trials. centroid_lags_mc : ndarray or None     Centroid lags from Monte Carlo trials. peak_lag_ci : tuple or None     Confidence interval (16<sup>th</sup>\u201384<sup>th</sup> percentile) on peak lag. centroid_lag_ci : tuple or None     Confidence interval on centroid lag.</p> Source code in <code>stela_toolkit/cross_correlation.py</code> <pre><code>class CrossCorrelation:\n    \"\"\"\n    Compute the time-domain cross-correlation function (CCF) between two light curves or GP models.\n\n    This class supports three primary use cases:\n\n    1. **Regularly sampled LightCurve objects**: The CCF is computed via direct shifting\n    using Pearson correlation coefficients across lag values.\n\n    2. **Irregularly sampled LightCurve objects**: Uses the interpolated cross-correlation\n    method (ICCF), introduced by Gaskell &amp; Peterson (1987), which linearly interpolates\n    one light curve onto the other's grid to allow for lag estimation despite gaps.\n\n    3. **GaussianProcess models**: If both inputs are trained GP models, the CCF is computed\n    across all sampled realizations and averaged. If no samples exist, 1000 will be generated\n    automatically on a 1000-point grid. Lag uncertainties are derived from the spread in\n    lag values across realizations.\n\n    Monte Carlo resampling is also available for estimating confidence intervals using observational\n    error bars.\n\n    Parameters\n    ----------\n    lc_or_model1 : LightCurve or GaussianProcess\n        First input light curve or trained GP model.\n    lc_or_model2 : LightCurve or GaussianProcess\n        Second input light curve or trained GP model.\n    run_monte_carlo : bool, optional\n        Whether to estimate lag uncertainties using Monte Carlo resampling.\n    n_trials : int, optional\n        Number of Monte Carlo trials.\n    min_lag : float, optional\n        Minimum lag to evaluate, default \"auto\" results in min_lag = - duration / 2\n    max_lag : float, optional\n        Maximum lag to evaluate, default \"auto\" results in max_lag = duration / 2\n    dt : float, optional\n        Time step to use for interpolation, less than sampling rate of the data, default of \n        \"auto\" results in dt = average sampling rate / 5.\n\n    centroid_threshold : float, optional\n        Threshold (fraction of peak CCF) for defining centroid lag region.\n    mode : {'regular', 'interp'}, optional\n        CCF computation mode. Use 'regular' for direct shifting (requires aligned time grids),\n        or 'interp' for ICCF-style interpolation.\n    rmax_threshold : float, optional\n        Monte Carlo trials with maximum correlation below this value are discarded.\n\n    Attributes\n    ----------\n    lags : ndarray\n        Array of lag values evaluated.\n    ccf : ndarray\n        Cross-correlation coefficients.\n    peak_lag : float\n        Lag corresponding to the peak correlation.\n    centroid_lag : float\n        Centroid lag from the high-correlation region.\n    rmax : float\n        Maximum correlation coefficient.\n    peak_lags_mc : ndarray or None\n        Peak lags from Monte Carlo trials.\n    centroid_lags_mc : ndarray or None\n        Centroid lags from Monte Carlo trials.\n    peak_lag_ci : tuple or None\n        Confidence interval (16th\u201384th percentile) on peak lag.\n    centroid_lag_ci : tuple or None\n        Confidence interval on centroid lag.\n    \"\"\"\n\n    def __init__(self,\n                 lc_or_model1,\n                 lc_or_model2,\n                 run_monte_carlo=False,\n                 n_trials=1000,\n                 min_lag=\"auto\",\n                 max_lag=\"auto\",\n                 dt=\"auto\",\n                 centroid_threshold=0.8,\n                 mode=\"regular\",\n                 rmax_threshold=0.0):\n\n        req_reg_samp = True if mode==\"regular\" else False\n        data1 = _CheckInputs._check_lightcurve_or_model(lc_or_model1, req_reg_samp=req_reg_samp)\n        data2 = _CheckInputs._check_lightcurve_or_model(lc_or_model2, req_reg_samp=req_reg_samp)\n\n        if data1['type'] == 'model':\n            if not hasattr(lc_or_model1, 'samples'):\n                raise ValueError(\"Model 1 must have generated samples via GP.sample().\")\n            self.times = lc_or_model1.pred_times.numpy()\n            self.rates1 = lc_or_model1.samples\n            self.is_model1 = True\n        else:\n            self.times, self.rates1, self.errors1 = data1['data']\n            self.is_model1 = False\n\n        if data2['type'] == 'model':\n            if not hasattr(lc_or_model2, 'samples'):\n                raise ValueError(\"Model 2 must have generated samples via GP.sample().\")\n            self.times = lc_or_model2.pred_times.numpy()\n            self.rates2 = lc_or_model2.samples\n            self.is_model2 = True\n        else:\n            self.times, self.rates2, self.errors2 = data2['data']\n            self.is_model2 = False\n\n        self.n_trials = n_trials\n        self.centroid_threshold = centroid_threshold\n        self.mode = mode\n        self.rmax_threshold = rmax_threshold\n\n        duration = self.times[-1] - self.times[0]\n        self.min_lag = -duration / 2 if min_lag==\"auto\" else min_lag\n        self.max_lag = duration / 2 if max_lag==\"auto\" else max_lag\n\n        if mode == \"regular\":\n            self.dt = np.diff(self.times)[0]\n            self.lags = np.arange(self.min_lag, self.max_lag + self.dt, self.dt)\n\n            if self.is_model1 and self.is_model2:\n                if self.rates1.shape[0] != self.rates2.shape[0]:\n                    raise ValueError(\"Model sample shapes do not match.\")\n                self.ccf = np.mean([\n                    self.compute_ccf(self.rates1[i], self.rates2[i])[1]\n                    for i in range(self.rates1.shape[0])\n                ], axis=0)\n\n            else:\n                self.ccf = self.compute_ccf(self.rates1, self.rates2)\n\n        else:\n            self.dt = np.mean(np.diff(self.times)) / 5 if dt==\"auto\" else dt\n            self.lags = np.arange(self.min_lag, self.max_lag + self.dt, self.dt)\n            self.ccf = self.compute_ccf_interp()\n\n        self.rmax = np.max(self.ccf)\n        self.peak_lag, self.centroid_lag = self.find_peak_and_centroid(self.lags, self.ccf)\n\n        self.peak_lags_mc = None\n        self.centroid_lags_mc = None\n        self.peak_lag_ci = None\n        self.centroid_lag_ci = None\n\n        if run_monte_carlo:\n            if np.all(self.errors1 == 0) or np.all(self.errors2 == 0):\n                print(\"Skipping Monte Carlo: zero errors for all points in one or both light curves.\")\n            else:\n                self.peak_lags_mc, self.centroid_lags_mc = self.run_monte_carlo()\n                self.compute_confidence_intervals()\n\n    def compute_ccf(self, rates1, rates2):\n        \"\"\"\n        Compute the cross-correlation function (CCF) via direct shifting.\n\n        Parameters\n        ----------\n        rates1 : ndarray\n            First time series.\n        rates2 : ndarray\n            Second time series.\n\n        Returns\n        -------\n        lags : ndarray\n            Lag values.\n        ccf : ndarray\n            Pearson correlation coefficients at each lag.\n        \"\"\"\n\n        ccf = []\n\n        for lag in self.lags:\n            shift = int(round(lag / self.dt))\n\n            if shift &lt; 0:\n                x = rates1[:shift]\n                y = rates2[-shift:]\n            elif shift &gt; 0:\n                x = rates1[shift:]\n                y = rates2[:-shift]\n            else:\n                x = rates1\n                y = rates2\n\n            if len(x) &lt; 2:\n                ccf.append(0.0)\n            else:\n                r = np.corrcoef(x, y)[0, 1]\n                ccf.append(r)\n\n        return np.array(ccf)\n\n    def compute_ccf_interp(self):\n        \"\"\"\n        Compute the cross-correlation function using symmetric linear interpolation.\n\n        Returns\n        -------\n        ccf : ndarray\n            Interpolated cross-correlation values for each lag.\n        \"\"\"\n\n        interp1 = interp1d(self.times, self.rates1, bounds_error=False, fill_value=0.0)\n        interp2 = interp1d(self.times, self.rates2, bounds_error=False, fill_value=0.0)\n        ccf = []\n\n        for lag in self.lags:\n            t_shift1 = self.times + lag\n            t_shift2 = self.times - lag\n\n            mask1 = (t_shift1 &gt;= self.times[0]) &amp; (t_shift1 &lt;= self.times[-1])\n            mask2 = (t_shift2 &gt;= self.times[0]) &amp; (t_shift2 &lt;= self.times[-1])\n\n            if np.sum(mask1) &lt; 2 or np.sum(mask2) &lt; 2:\n                ccf.append(0.0)\n                continue\n\n            r1 = np.corrcoef(self.rates1[mask1], interp2(t_shift1[mask1]))[0, 1]\n            r2 = np.corrcoef(self.rates2[mask2], interp1(t_shift2[mask2]))[0, 1]\n            ccf.append((r1 + r2) / 2)\n\n        return np.array(ccf)\n\n    def find_peak_and_centroid(self, lags, ccf):\n        \"\"\"\n        Compute the peak and centroid lag of a cross-correlation function.\n\n        The peak lag corresponds to the lag with the maximum correlation value.\n        The centroid lag is computed using a weighted average of lag values\n        in a contiguous region around the peak where the correlation exceeds\n        a fraction of the peak value.\n\n        Parameters\n        ----------\n        lags : ndarray\n            Array of lag values (assumed sorted).\n        ccf : ndarray\n            Cross-correlation values at each lag.\n\n        Returns\n        -------\n        peak_lag : float\n            Lag corresponding to the maximum correlation.\n        centroid_lag : float or np.nan\n            Correlation-weighted centroid lag near the peak.\n            Returns NaN if a valid centroid region cannot be identified.\n        \"\"\"\n        if len(lags) != len(ccf) or len(ccf) == 0:\n            raise ValueError(\"lags and ccf must be the same nonzero length\")\n\n        # Locate the peak correlation and corresponding lag\n        peak_idx = np.nanargmax(ccf)\n        peak_lag = lags[peak_idx]\n        peak_val = ccf[peak_idx]\n\n        # Define a local region around the peak above a fractional threshold\n        threshold = self.centroid_threshold\n        cutoff = threshold * peak_val\n\n        # Expand to left of peak\n        i_left = peak_idx\n        while i_left &gt; 0 and ccf[i_left - 1] &gt;= cutoff:\n            i_left -= 1\n\n        # Expand to right of peak\n        i_right = peak_idx\n        while i_right &lt; len(ccf) - 1 and ccf[i_right + 1] &gt;= cutoff:\n            i_right += 1\n\n        # Compute centroid if region is valid\n        if i_right &gt;= i_left:\n            lags_subset = lags[i_left:i_right + 1]\n            ccf_subset = ccf[i_left:i_right + 1]\n            weight_sum = np.sum(ccf_subset)\n            if weight_sum &gt; 0:\n                centroid_lag = np.sum(lags_subset * ccf_subset) / weight_sum\n            else:\n                centroid_lag = np.nan\n        else:\n            centroid_lag = np.nan\n\n        return peak_lag, centroid_lag\n\n\n    def run_monte_carlo(self):\n        \"\"\"\n        Run Monte Carlo simulations to estimate lag uncertainties.\n\n        Perturbs input light curves based on their errors and computes peak and centroid\n        lags for each realization.\n\n        Returns\n        -------\n        peak_lags : ndarray\n            Peak lag values from all trials.\n        centroid_lags : ndarray\n            Centroid lag values from all trials.\n        \"\"\"\n\n        peak_lags = []\n        centroid_lags = []\n\n        for _ in range(self.n_trials):\n            r1_pert = np.random.normal(self.rates1, self.errors1)\n            r2_pert = np.random.normal(self.rates2, self.errors2)\n\n            if self.mode == \"interp\":\n                interp1 = interp1d(self.times, r1_pert, bounds_error=False, fill_value=0.0)\n                interp2 = interp1d(self.times, r2_pert, bounds_error=False, fill_value=0.0)\n                ccf = []\n\n                for lag in self.lags:\n                    t_shift1 = self.times + lag\n                    t_shift2 = self.times - lag\n\n                    mask1 = (t_shift1 &gt;= self.times[0]) &amp; (t_shift1 &lt;= self.times[-1])\n                    mask2 = (t_shift2 &gt;= self.times[0]) &amp; (t_shift2 &lt;= self.times[-1])\n\n                    if np.sum(mask1) &lt; 2 or np.sum(mask2) &lt; 2:\n                        ccf.append(0.0)\n                        continue\n\n                    r1 = np.corrcoef(r1_pert[mask1], interp2(t_shift1[mask1]))[0, 1]\n                    r2 = np.corrcoef(r2_pert[mask2], interp1(t_shift2[mask2]))[0, 1]\n                    ccf_val = (r1 + r2) / 2\n                    ccf.append(ccf_val)\n                ccf = np.array(ccf)\n            else:\n                ccf = self.compute_ccf(r1_pert, r2_pert)\n\n            if np.max(ccf) &lt; self.rmax_threshold:\n                continue\n\n            peak, centroid = self.find_peak_and_centroid(self.lags, ccf)\n            peak_lags.append(peak)\n            centroid_lags.append(centroid)\n\n        return np.array(peak_lags), np.array(centroid_lags)\n\n    def compute_confidence_intervals(self, lower_percentile=16, upper_percentile=84):\n        \"\"\"\n        Compute percentile-based confidence intervals for Monte Carlo lag distributions.\n\n        Parameters\n        ----------\n        lower_percentile : float\n            Lower percentile bound (default is 16).\n        upper_percentile : float\n            Upper percentile bound (default is 84).\n        \"\"\"\n\n        if self.peak_lags_mc is None or self.centroid_lags_mc is None:\n            print(\"No Monte Carlo results available to compute confidence intervals.\")\n            return\n\n        self.peak_lag_ci = (\n            np.percentile(self.peak_lags_mc, lower_percentile),\n            np.percentile(self.peak_lags_mc, upper_percentile),\n        )\n        self.centroid_lag_ci = (\n            np.percentile(self.centroid_lags_mc, lower_percentile),\n            np.percentile(self.centroid_lags_mc, upper_percentile),\n        )\n\n    def plot(self, show_mc=True):\n        \"\"\"\n        Plot the cross-correlation function and optional Monte Carlo lag distributions.\n\n        Parameters\n        ----------\n        **kwargs : dict\n            Keyword arguments for customizing the plot.\n        \"\"\"\n\n        fig, ax = plt.subplots(1, 1, figsize=(7, 4))\n        ax.plot(self.lags, self.ccf, label=\"CCF\", color='black')\n        ax.axvline(self.peak_lag, color='red', linestyle='--',\n                   label=f\"Peak lag = {self.peak_lag:.2f}\")\n        ax.axvline(self.centroid_lag, color='blue', linestyle=':',\n                   label=f\"Centroid lag = {self.centroid_lag:.2f}\")\n        ax.set_xlabel(\"Lag (same unit as input)\")\n        ax.set_ylabel(\"Correlation coefficient\")\n        ax.grid(True)\n        ax.legend()\n\n        if show_mc and self.peak_lags_mc is not None:\n            fig_mc, ax_mc = plt.subplots(1, 2, figsize=(10, 4))\n            ax_mc[0].hist(self.peak_lags_mc, bins=30, color='red', alpha=0.7)\n            ax_mc[0].set_title(\"Peak Lag Distribution (MC)\")\n            ax_mc[0].set_xlabel(\"Lag\")\n            ax_mc[0].set_ylabel(\"Count\")\n            ax_mc[0].grid(True)\n\n            ax_mc[1].hist(self.centroid_lags_mc, bins=30, color='blue', alpha=0.7)\n            ax_mc[1].set_title(\"Centroid Lag Distribution (MC)\")\n            ax_mc[1].set_xlabel(\"Lag\")\n            ax_mc[1].set_ylabel(\"Count\")\n            ax_mc[1].grid(True)\n\n        plt.tight_layout()\n        plt.show()\n</code></pre>"},{"location":"reference/cross_correlation/#stela_toolkit.cross_correlation.CrossCorrelation.compute_ccf","title":"<code>compute_ccf(rates1, rates2)</code>","text":"<p>Compute the cross-correlation function (CCF) via direct shifting.</p>"},{"location":"reference/cross_correlation/#stela_toolkit.cross_correlation.CrossCorrelation.compute_ccf--parameters","title":"Parameters","text":"<p>rates1 : ndarray     First time series. rates2 : ndarray     Second time series.</p>"},{"location":"reference/cross_correlation/#stela_toolkit.cross_correlation.CrossCorrelation.compute_ccf--returns","title":"Returns","text":"<p>lags : ndarray     Lag values. ccf : ndarray     Pearson correlation coefficients at each lag.</p> Source code in <code>stela_toolkit/cross_correlation.py</code> <pre><code>def compute_ccf(self, rates1, rates2):\n    \"\"\"\n    Compute the cross-correlation function (CCF) via direct shifting.\n\n    Parameters\n    ----------\n    rates1 : ndarray\n        First time series.\n    rates2 : ndarray\n        Second time series.\n\n    Returns\n    -------\n    lags : ndarray\n        Lag values.\n    ccf : ndarray\n        Pearson correlation coefficients at each lag.\n    \"\"\"\n\n    ccf = []\n\n    for lag in self.lags:\n        shift = int(round(lag / self.dt))\n\n        if shift &lt; 0:\n            x = rates1[:shift]\n            y = rates2[-shift:]\n        elif shift &gt; 0:\n            x = rates1[shift:]\n            y = rates2[:-shift]\n        else:\n            x = rates1\n            y = rates2\n\n        if len(x) &lt; 2:\n            ccf.append(0.0)\n        else:\n            r = np.corrcoef(x, y)[0, 1]\n            ccf.append(r)\n\n    return np.array(ccf)\n</code></pre>"},{"location":"reference/cross_correlation/#stela_toolkit.cross_correlation.CrossCorrelation.compute_ccf_interp","title":"<code>compute_ccf_interp()</code>","text":"<p>Compute the cross-correlation function using symmetric linear interpolation.</p>"},{"location":"reference/cross_correlation/#stela_toolkit.cross_correlation.CrossCorrelation.compute_ccf_interp--returns","title":"Returns","text":"<p>ccf : ndarray     Interpolated cross-correlation values for each lag.</p> Source code in <code>stela_toolkit/cross_correlation.py</code> <pre><code>def compute_ccf_interp(self):\n    \"\"\"\n    Compute the cross-correlation function using symmetric linear interpolation.\n\n    Returns\n    -------\n    ccf : ndarray\n        Interpolated cross-correlation values for each lag.\n    \"\"\"\n\n    interp1 = interp1d(self.times, self.rates1, bounds_error=False, fill_value=0.0)\n    interp2 = interp1d(self.times, self.rates2, bounds_error=False, fill_value=0.0)\n    ccf = []\n\n    for lag in self.lags:\n        t_shift1 = self.times + lag\n        t_shift2 = self.times - lag\n\n        mask1 = (t_shift1 &gt;= self.times[0]) &amp; (t_shift1 &lt;= self.times[-1])\n        mask2 = (t_shift2 &gt;= self.times[0]) &amp; (t_shift2 &lt;= self.times[-1])\n\n        if np.sum(mask1) &lt; 2 or np.sum(mask2) &lt; 2:\n            ccf.append(0.0)\n            continue\n\n        r1 = np.corrcoef(self.rates1[mask1], interp2(t_shift1[mask1]))[0, 1]\n        r2 = np.corrcoef(self.rates2[mask2], interp1(t_shift2[mask2]))[0, 1]\n        ccf.append((r1 + r2) / 2)\n\n    return np.array(ccf)\n</code></pre>"},{"location":"reference/cross_correlation/#stela_toolkit.cross_correlation.CrossCorrelation.compute_confidence_intervals","title":"<code>compute_confidence_intervals(lower_percentile=16, upper_percentile=84)</code>","text":"<p>Compute percentile-based confidence intervals for Monte Carlo lag distributions.</p>"},{"location":"reference/cross_correlation/#stela_toolkit.cross_correlation.CrossCorrelation.compute_confidence_intervals--parameters","title":"Parameters","text":"<p>lower_percentile : float     Lower percentile bound (default is 16). upper_percentile : float     Upper percentile bound (default is 84).</p> Source code in <code>stela_toolkit/cross_correlation.py</code> <pre><code>def compute_confidence_intervals(self, lower_percentile=16, upper_percentile=84):\n    \"\"\"\n    Compute percentile-based confidence intervals for Monte Carlo lag distributions.\n\n    Parameters\n    ----------\n    lower_percentile : float\n        Lower percentile bound (default is 16).\n    upper_percentile : float\n        Upper percentile bound (default is 84).\n    \"\"\"\n\n    if self.peak_lags_mc is None or self.centroid_lags_mc is None:\n        print(\"No Monte Carlo results available to compute confidence intervals.\")\n        return\n\n    self.peak_lag_ci = (\n        np.percentile(self.peak_lags_mc, lower_percentile),\n        np.percentile(self.peak_lags_mc, upper_percentile),\n    )\n    self.centroid_lag_ci = (\n        np.percentile(self.centroid_lags_mc, lower_percentile),\n        np.percentile(self.centroid_lags_mc, upper_percentile),\n    )\n</code></pre>"},{"location":"reference/cross_correlation/#stela_toolkit.cross_correlation.CrossCorrelation.find_peak_and_centroid","title":"<code>find_peak_and_centroid(lags, ccf)</code>","text":"<p>Compute the peak and centroid lag of a cross-correlation function.</p> <p>The peak lag corresponds to the lag with the maximum correlation value. The centroid lag is computed using a weighted average of lag values in a contiguous region around the peak where the correlation exceeds a fraction of the peak value.</p>"},{"location":"reference/cross_correlation/#stela_toolkit.cross_correlation.CrossCorrelation.find_peak_and_centroid--parameters","title":"Parameters","text":"<p>lags : ndarray     Array of lag values (assumed sorted). ccf : ndarray     Cross-correlation values at each lag.</p>"},{"location":"reference/cross_correlation/#stela_toolkit.cross_correlation.CrossCorrelation.find_peak_and_centroid--returns","title":"Returns","text":"<p>peak_lag : float     Lag corresponding to the maximum correlation. centroid_lag : float or np.nan     Correlation-weighted centroid lag near the peak.     Returns NaN if a valid centroid region cannot be identified.</p> Source code in <code>stela_toolkit/cross_correlation.py</code> <pre><code>def find_peak_and_centroid(self, lags, ccf):\n    \"\"\"\n    Compute the peak and centroid lag of a cross-correlation function.\n\n    The peak lag corresponds to the lag with the maximum correlation value.\n    The centroid lag is computed using a weighted average of lag values\n    in a contiguous region around the peak where the correlation exceeds\n    a fraction of the peak value.\n\n    Parameters\n    ----------\n    lags : ndarray\n        Array of lag values (assumed sorted).\n    ccf : ndarray\n        Cross-correlation values at each lag.\n\n    Returns\n    -------\n    peak_lag : float\n        Lag corresponding to the maximum correlation.\n    centroid_lag : float or np.nan\n        Correlation-weighted centroid lag near the peak.\n        Returns NaN if a valid centroid region cannot be identified.\n    \"\"\"\n    if len(lags) != len(ccf) or len(ccf) == 0:\n        raise ValueError(\"lags and ccf must be the same nonzero length\")\n\n    # Locate the peak correlation and corresponding lag\n    peak_idx = np.nanargmax(ccf)\n    peak_lag = lags[peak_idx]\n    peak_val = ccf[peak_idx]\n\n    # Define a local region around the peak above a fractional threshold\n    threshold = self.centroid_threshold\n    cutoff = threshold * peak_val\n\n    # Expand to left of peak\n    i_left = peak_idx\n    while i_left &gt; 0 and ccf[i_left - 1] &gt;= cutoff:\n        i_left -= 1\n\n    # Expand to right of peak\n    i_right = peak_idx\n    while i_right &lt; len(ccf) - 1 and ccf[i_right + 1] &gt;= cutoff:\n        i_right += 1\n\n    # Compute centroid if region is valid\n    if i_right &gt;= i_left:\n        lags_subset = lags[i_left:i_right + 1]\n        ccf_subset = ccf[i_left:i_right + 1]\n        weight_sum = np.sum(ccf_subset)\n        if weight_sum &gt; 0:\n            centroid_lag = np.sum(lags_subset * ccf_subset) / weight_sum\n        else:\n            centroid_lag = np.nan\n    else:\n        centroid_lag = np.nan\n\n    return peak_lag, centroid_lag\n</code></pre>"},{"location":"reference/cross_correlation/#stela_toolkit.cross_correlation.CrossCorrelation.plot","title":"<code>plot(show_mc=True)</code>","text":"<p>Plot the cross-correlation function and optional Monte Carlo lag distributions.</p>"},{"location":"reference/cross_correlation/#stela_toolkit.cross_correlation.CrossCorrelation.plot--parameters","title":"Parameters","text":"<p>**kwargs : dict     Keyword arguments for customizing the plot.</p> Source code in <code>stela_toolkit/cross_correlation.py</code> <pre><code>def plot(self, show_mc=True):\n    \"\"\"\n    Plot the cross-correlation function and optional Monte Carlo lag distributions.\n\n    Parameters\n    ----------\n    **kwargs : dict\n        Keyword arguments for customizing the plot.\n    \"\"\"\n\n    fig, ax = plt.subplots(1, 1, figsize=(7, 4))\n    ax.plot(self.lags, self.ccf, label=\"CCF\", color='black')\n    ax.axvline(self.peak_lag, color='red', linestyle='--',\n               label=f\"Peak lag = {self.peak_lag:.2f}\")\n    ax.axvline(self.centroid_lag, color='blue', linestyle=':',\n               label=f\"Centroid lag = {self.centroid_lag:.2f}\")\n    ax.set_xlabel(\"Lag (same unit as input)\")\n    ax.set_ylabel(\"Correlation coefficient\")\n    ax.grid(True)\n    ax.legend()\n\n    if show_mc and self.peak_lags_mc is not None:\n        fig_mc, ax_mc = plt.subplots(1, 2, figsize=(10, 4))\n        ax_mc[0].hist(self.peak_lags_mc, bins=30, color='red', alpha=0.7)\n        ax_mc[0].set_title(\"Peak Lag Distribution (MC)\")\n        ax_mc[0].set_xlabel(\"Lag\")\n        ax_mc[0].set_ylabel(\"Count\")\n        ax_mc[0].grid(True)\n\n        ax_mc[1].hist(self.centroid_lags_mc, bins=30, color='blue', alpha=0.7)\n        ax_mc[1].set_title(\"Centroid Lag Distribution (MC)\")\n        ax_mc[1].set_xlabel(\"Lag\")\n        ax_mc[1].set_ylabel(\"Count\")\n        ax_mc[1].grid(True)\n\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"reference/cross_correlation/#stela_toolkit.cross_correlation.CrossCorrelation.run_monte_carlo","title":"<code>run_monte_carlo()</code>","text":"<p>Run Monte Carlo simulations to estimate lag uncertainties.</p> <p>Perturbs input light curves based on their errors and computes peak and centroid lags for each realization.</p>"},{"location":"reference/cross_correlation/#stela_toolkit.cross_correlation.CrossCorrelation.run_monte_carlo--returns","title":"Returns","text":"<p>peak_lags : ndarray     Peak lag values from all trials. centroid_lags : ndarray     Centroid lag values from all trials.</p> Source code in <code>stela_toolkit/cross_correlation.py</code> <pre><code>def run_monte_carlo(self):\n    \"\"\"\n    Run Monte Carlo simulations to estimate lag uncertainties.\n\n    Perturbs input light curves based on their errors and computes peak and centroid\n    lags for each realization.\n\n    Returns\n    -------\n    peak_lags : ndarray\n        Peak lag values from all trials.\n    centroid_lags : ndarray\n        Centroid lag values from all trials.\n    \"\"\"\n\n    peak_lags = []\n    centroid_lags = []\n\n    for _ in range(self.n_trials):\n        r1_pert = np.random.normal(self.rates1, self.errors1)\n        r2_pert = np.random.normal(self.rates2, self.errors2)\n\n        if self.mode == \"interp\":\n            interp1 = interp1d(self.times, r1_pert, bounds_error=False, fill_value=0.0)\n            interp2 = interp1d(self.times, r2_pert, bounds_error=False, fill_value=0.0)\n            ccf = []\n\n            for lag in self.lags:\n                t_shift1 = self.times + lag\n                t_shift2 = self.times - lag\n\n                mask1 = (t_shift1 &gt;= self.times[0]) &amp; (t_shift1 &lt;= self.times[-1])\n                mask2 = (t_shift2 &gt;= self.times[0]) &amp; (t_shift2 &lt;= self.times[-1])\n\n                if np.sum(mask1) &lt; 2 or np.sum(mask2) &lt; 2:\n                    ccf.append(0.0)\n                    continue\n\n                r1 = np.corrcoef(r1_pert[mask1], interp2(t_shift1[mask1]))[0, 1]\n                r2 = np.corrcoef(r2_pert[mask2], interp1(t_shift2[mask2]))[0, 1]\n                ccf_val = (r1 + r2) / 2\n                ccf.append(ccf_val)\n            ccf = np.array(ccf)\n        else:\n            ccf = self.compute_ccf(r1_pert, r2_pert)\n\n        if np.max(ccf) &lt; self.rmax_threshold:\n            continue\n\n        peak, centroid = self.find_peak_and_centroid(self.lags, ccf)\n        peak_lags.append(peak)\n        centroid_lags.append(centroid)\n\n    return np.array(peak_lags), np.array(centroid_lags)\n</code></pre>"},{"location":"reference/cross_spectrum/","title":"cross_spectrum","text":""},{"location":"reference/cross_spectrum/#stela_toolkit.cross_spectrum.CrossSpectrum","title":"<code>CrossSpectrum</code>","text":"<p>Compute the cross-spectrum between two light curves or trained Gaussian Process models.</p> <p>This class accepts LightCurve objects or GaussianProcess models from this package. For GP models, if posterior samples have already been generated, those are used. If not, the class automatically generates 1000 samples across a 1000-point grid.</p> <p>The cross-spectrum is computed using the Fourier transform of one time series multiplied by the complex conjugate of the other, yielding frequency-dependent phase and amplitude information.</p> <p>If both inputs are GP models, the cross-spectrum is computed across all sample pairs, and the mean and standard deviation across realizations are returned.</p> <p>Frequency binning is available with options for logarithmic, linear, or custom spacing.</p>"},{"location":"reference/cross_spectrum/#stela_toolkit.cross_spectrum.CrossSpectrum--parameters","title":"Parameters","text":"<p>lc_or_model1 : LightCurve or GaussianProcess     First input light curve or trained GP model. lc_or_model2 : LightCurve or GaussianProcess     Second input light curve or trained GP model. fmin : float or 'auto', optional     Minimum frequency to include. If 'auto', uses lowest nonzero FFT frequency. fmax : float or 'auto', optional     Maximum frequency to include. If 'auto', uses the Nyquist frequency. num_bins : int, optional     Number of frequency bins. bin_type : str, optional     Binning type: 'log' or 'linear'. bin_edges : array-like, optional     Custom frequency bin edges. Overrides <code>num_bins</code> and <code>bin_type</code> if provided. norm : bool, optional     Whether to normalize the cross-spectrum to variance units (i.e., PSD units).</p>"},{"location":"reference/cross_spectrum/#stela_toolkit.cross_spectrum.CrossSpectrum--attributes","title":"Attributes","text":"<p>freqs : array-like     Frequency bin centers. freq_widths : array-like     Frequency bin widths. cs : array-like     Complex cross-spectrum values. cs_errors : array-like     Uncertainties in the binned cross-spectrum (if stacked).</p> Source code in <code>stela_toolkit/cross_spectrum.py</code> <pre><code>class CrossSpectrum:\n    \"\"\"\n    Compute the cross-spectrum between two light curves or trained Gaussian Process models.\n\n    This class accepts LightCurve objects or GaussianProcess models from this package.\n    For GP models, if posterior samples have already been generated, those are used.\n    If not, the class automatically generates 1000 samples across a 1000-point grid.\n\n    The cross-spectrum is computed using the Fourier transform of one time series\n    multiplied by the complex conjugate of the other, yielding frequency-dependent phase\n    and amplitude information.\n\n    If both inputs are GP models, the cross-spectrum is computed across all sample pairs,\n    and the mean and standard deviation across realizations are returned.\n\n    Frequency binning is available with options for logarithmic, linear, or custom spacing.\n\n    Parameters\n    ----------\n    lc_or_model1 : LightCurve or GaussianProcess\n        First input light curve or trained GP model.\n    lc_or_model2 : LightCurve or GaussianProcess\n        Second input light curve or trained GP model.\n    fmin : float or 'auto', optional\n        Minimum frequency to include. If 'auto', uses lowest nonzero FFT frequency.\n    fmax : float or 'auto', optional\n        Maximum frequency to include. If 'auto', uses the Nyquist frequency.\n    num_bins : int, optional\n        Number of frequency bins.\n    bin_type : str, optional\n        Binning type: 'log' or 'linear'.\n    bin_edges : array-like, optional\n        Custom frequency bin edges. Overrides `num_bins` and `bin_type` if provided.\n    norm : bool, optional\n        Whether to normalize the cross-spectrum to variance units (i.e., PSD units).\n\n    Attributes\n    ----------\n    freqs : array-like\n        Frequency bin centers.\n    freq_widths : array-like\n        Frequency bin widths.\n    cs : array-like\n        Complex cross-spectrum values.\n    cs_errors : array-like\n        Uncertainties in the binned cross-spectrum (if stacked).\n    \"\"\"\n\n    def __init__(self,\n                 lc_or_model1,\n                 lc_or_model2,\n                 fmin='auto',\n                 fmax='auto',\n                 num_bins=None,\n                 bin_type=\"log\",\n                 bin_edges=[],\n                 norm=True):\n\n        # To do: update main docstring\n        input_data = _CheckInputs._check_lightcurve_or_model(lc_or_model1)\n        if input_data['type'] == 'model':\n            self.times1, self.rates1 = input_data['data']\n        else:\n            self.times1, self.rates1, _ = input_data['data']\n\n        input_data = _CheckInputs._check_lightcurve_or_model(lc_or_model2)\n        if input_data['type'] == 'model':\n            self.times2, self.rates2 = input_data['data']\n        else:\n            self.times2, self.rates2, _ = input_data['data']\n\n        _CheckInputs._check_input_bins(num_bins, bin_type, bin_edges)\n\n        if not np.allclose(self.times1, self.times2):\n            raise ValueError(\"The time arrays of the two light curves must be identical.\")\n\n        # Use absolute min and max frequencies if set to 'auto'\n        self.dt = np.diff(self.times1)[0]\n        self.fmin = np.fft.rfftfreq(len(self.rates1), d=self.dt)[1] if fmin == 'auto' else fmin\n        self.fmax = np.fft.rfftfreq(len(self.rates1), d=self.dt)[-1] if fmax == 'auto' else fmax  # nyquist frequency\n\n        self.num_bins = num_bins\n        self.bin_type = bin_type\n        self.bin_edges = bin_edges\n\n        # Check if the input rates are for multiple realizations\n        # this needs to be corrected for handling different shapes and dim val1 != dim val2\n        if len(self.rates1.shape) == 2 and len(self.rates2.shape) == 2:\n            cross_spectrum = self.compute_stacked_cross_spectrum(norm=norm)\n        else:\n            cross_spectrum = self.compute_cross_spectrum(norm=norm)\n\n        self.freqs, self.freq_widths, self.cs, self.cs_errors = cross_spectrum\n\n    def compute_cross_spectrum(self, times1=None, rates1=None, times2=None, rates2=None, norm=True):\n        \"\"\"\n        Compute the cross-spectrum for a single pair of light curves.\n\n        Parameters\n        ----------\n        times1, rates1 : array-like, optional\n            Time and rate arrays for the first light curve.\n        times2, rates2 : array-like, optional\n            Time and rate arrays for the second light curve.\n        norm : bool, optional\n            Whether to normalize the result to power spectral density units.\n\n        Returns\n        -------\n        freqs : array-like\n            Frequencies of the cross-spectrum.\n        freq_widths : array-like\n            Widths of frequency bins.\n        cross_spectrum : array-like\n            Complex cross-spectrum values.\n        cross_spectrum_errors : array-like or None\n            Uncertainties in the binned cross-spectrum (None if not binned).\n        \"\"\"\n\n        times1 = self.times1 if times1 is None else times1\n        rates1 = self.rates1 if rates1 is None else rates1\n        times2 = self.times2 if times2 is None else times2\n        rates2 = self.rates2 if rates2 is None else rates2\n\n        freqs, fft1 = LightCurve(times=times1, rates=rates1).fft()\n        _, fft2 = LightCurve(times=times2, rates=rates2).fft()\n\n        cross_spectrum = np.conj(fft2) * fft1\n\n        # Filter frequencies within [fmin, fmax]\n        valid_mask = (freqs &gt;= self.fmin) &amp; (freqs &lt;= self.fmax)\n        freqs = freqs[valid_mask]\n        cross_spectrum = cross_spectrum[valid_mask]\n\n        # Normalize power spectrum to units of variance (PSD)\n        if norm:\n            length = len(rates1)\n            norm_factor = length * np.mean(rates1) * np.mean(rates2) / (2 * self.dt)\n            cross_spectrum /= norm_factor\n\n            # negative norm factor shifts the phase by pi\n            if norm_factor &lt; 0:\n                phase = np.angle(cross_spectrum)\n                cross_spectrum = np.abs(cross_spectrum) * np.exp(1j * phase)\n\n        # Apply binning\n        if self.num_bins or self.bin_edges:\n            if self.bin_edges:\n                # use custom bin edges\n                bin_edges = FrequencyBinning.define_bins(\n                    self.fmin, self.fmax, num_bins=self.num_bins,\n                    bin_type=self.bin_type, bin_edges=self.bin_edges\n                )\n            elif self.num_bins:\n                # use equal-width bins in log or linear space\n                bin_edges = FrequencyBinning.define_bins(\n                    self.fmin, self.fmax, num_bins=self.num_bins, bin_type=self.bin_type\n                )\n            else:\n                raise ValueError(\"Either num_bins or bin_edges must be provided.\\n\"\n                                 \"In other words, you must specify the number of bins or the bin edges.\")\n\n            binned_cross_spectrum = FrequencyBinning.bin_data(freqs, cross_spectrum, bin_edges)\n            freqs, freq_widths, cross_spectrum, cross_spectrum_errors = binned_cross_spectrum\n        else:\n            freq_widths, cross_spectrum_errors = None, None\n\n        return freqs, freq_widths, cross_spectrum, cross_spectrum_errors\n\n    def compute_stacked_cross_spectrum(self, norm=True):\n        \"\"\"\n        Compute the cross-spectrum across stacked GP samples.\n\n        Computes the cross-spectrum for each realization and returns the mean and\n        standard deviation across samples.\n\n        Parameters\n        ----------\n        norm : bool, optional\n            Whether to normalize the result to power spectral density units.\n\n        Returns\n        -------\n        freqs : array-like\n            Frequencies of the cross-spectrum.\n        freq_widths : array-like\n            Widths of frequency bins.\n        cross_spectra_mean : array-like\n            Mean cross-spectrum across GP samples.\n        cross_spectra_std : array-like\n            Standard deviation of the cross-spectrum across samples.\n        \"\"\"\n\n        cross_spectra = []\n        for i in range(self.rates1.shape[0]):\n            cross_spectrum = self.compute_cross_spectrum(\n                times1=self.times1, rates1=self.rates1[i],\n                times2=self.times2, rates2=self.rates2[i],\n                norm=norm\n            )\n            cross_spectra.append(cross_spectrum[2])\n\n        cross_spectra = np.vstack(cross_spectra)\n        # Real and imaginary std devs\n        cs_real_mean = np.mean(cross_spectra.real, axis=0)\n        cs_imag_mean = np.mean(cross_spectra.imag, axis=0)\n        cs_real_std = np.std(cross_spectra.real, axis=0)\n        cs_imag_std = np.std(cross_spectra.imag, axis=0)\n\n        cross_spectra_mean = cs_real_mean + 1j * cs_imag_mean\n        cross_spectra_std = cs_real_std + 1j * cs_imag_std\n        freqs, freq_widths = cross_spectrum[0], cross_spectrum[1]\n\n        return freqs, freq_widths, cross_spectra_mean, cross_spectra_std\n\n    def plot(self, freqs=None, freq_widths=None, cs=None, cs_errors=None, **kwargs):\n        \"\"\"\n        Plot the real and imaginary parts of the cross-spectrum.\n\n        Parameters\n        ----------\n        freqs : array-like, optional\n            Frequencies at which the cross-spectrum is evaluated.\n        freq_widths : array-like, optional\n            Widths of the frequency bins.\n        cs : array-like, optional\n            Cross-spectrum values.\n        cs_errors : array-like, optional\n            Uncertainties in the cross-spectrum.\n        **kwargs : dict\n            Additional keyword arguments for plot customization.\n        \"\"\"\n        import matplotlib.pyplot as plt\n\n        freqs = self.freqs if freqs is None else freqs\n        freq_widths = self.freq_widths if freq_widths is None else freq_widths\n        cs = self.cs if cs is None else cs\n        cs_errors = self.cs_errors if cs_errors is None else cs_errors\n\n        figsize = kwargs.get('figsize', (8, 4.5))\n        xlabel = kwargs.get('xlabel', 'Frequency')\n        ylabel = kwargs.get('ylabel', 'Cross-Spectrum')\n        xscale = kwargs.get('xscale', 'log')\n        yscale = kwargs.get('yscale', 'log')\n\n        plt.figure(figsize=figsize)\n\n        # Real part\n        if cs_errors is not None:\n            plt.errorbar(freqs, cs.real, xerr=freq_widths, yerr=cs_errors.real,\n                         fmt='o', color='black', ms=3, lw=1.5, label='Real')\n        else:\n            plt.plot(freqs, cs.real, 'o-', color='black', ms=3, lw=1.5, label='Real')\n\n        # Imaginary part\n        if cs_errors is not None:\n            plt.errorbar(freqs, cs.imag, xerr=freq_widths, yerr=cs_errors.imag,\n                         fmt='o', color='red', ms=3, lw=1.5, label='Imag')\n        else:\n            plt.plot(freqs, cs.imag, 'o-', color='red', ms=3, lw=1.5, label='Imag')\n\n        plt.xlabel(xlabel, fontsize=12)\n        plt.ylabel(ylabel, fontsize=12)\n        plt.xscale(xscale)\n        plt.yscale(yscale)\n        plt.legend(loc='best')\n        plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n        plt.tick_params(which='both', direction='in', length=6, width=1,\n                        top=True, right=True, labelsize=12)\n        plt.show()\n\n    def count_frequencies_in_bins(self, fmin=None, fmax=None, num_bins=None, bin_type=None, bin_edges=[]):\n        \"\"\"\n        Counts the number of frequencies in each frequency bin.\n        Wrapper method to use FrequencyBinning.count_frequencies_in_bins with class attributes.\n        \"\"\"\n\n        return FrequencyBinning.count_frequencies_in_bins(\n            self, fmin=fmin, fmax=fmax, num_bins=num_bins, bin_type=bin_type, bin_edges=bin_edges\n        )\n</code></pre>"},{"location":"reference/cross_spectrum/#stela_toolkit.cross_spectrum.CrossSpectrum.compute_cross_spectrum","title":"<code>compute_cross_spectrum(times1=None, rates1=None, times2=None, rates2=None, norm=True)</code>","text":"<p>Compute the cross-spectrum for a single pair of light curves.</p>"},{"location":"reference/cross_spectrum/#stela_toolkit.cross_spectrum.CrossSpectrum.compute_cross_spectrum--parameters","title":"Parameters","text":"<p>times1, rates1 : array-like, optional     Time and rate arrays for the first light curve. times2, rates2 : array-like, optional     Time and rate arrays for the second light curve. norm : bool, optional     Whether to normalize the result to power spectral density units.</p>"},{"location":"reference/cross_spectrum/#stela_toolkit.cross_spectrum.CrossSpectrum.compute_cross_spectrum--returns","title":"Returns","text":"<p>freqs : array-like     Frequencies of the cross-spectrum. freq_widths : array-like     Widths of frequency bins. cross_spectrum : array-like     Complex cross-spectrum values. cross_spectrum_errors : array-like or None     Uncertainties in the binned cross-spectrum (None if not binned).</p> Source code in <code>stela_toolkit/cross_spectrum.py</code> <pre><code>def compute_cross_spectrum(self, times1=None, rates1=None, times2=None, rates2=None, norm=True):\n    \"\"\"\n    Compute the cross-spectrum for a single pair of light curves.\n\n    Parameters\n    ----------\n    times1, rates1 : array-like, optional\n        Time and rate arrays for the first light curve.\n    times2, rates2 : array-like, optional\n        Time and rate arrays for the second light curve.\n    norm : bool, optional\n        Whether to normalize the result to power spectral density units.\n\n    Returns\n    -------\n    freqs : array-like\n        Frequencies of the cross-spectrum.\n    freq_widths : array-like\n        Widths of frequency bins.\n    cross_spectrum : array-like\n        Complex cross-spectrum values.\n    cross_spectrum_errors : array-like or None\n        Uncertainties in the binned cross-spectrum (None if not binned).\n    \"\"\"\n\n    times1 = self.times1 if times1 is None else times1\n    rates1 = self.rates1 if rates1 is None else rates1\n    times2 = self.times2 if times2 is None else times2\n    rates2 = self.rates2 if rates2 is None else rates2\n\n    freqs, fft1 = LightCurve(times=times1, rates=rates1).fft()\n    _, fft2 = LightCurve(times=times2, rates=rates2).fft()\n\n    cross_spectrum = np.conj(fft2) * fft1\n\n    # Filter frequencies within [fmin, fmax]\n    valid_mask = (freqs &gt;= self.fmin) &amp; (freqs &lt;= self.fmax)\n    freqs = freqs[valid_mask]\n    cross_spectrum = cross_spectrum[valid_mask]\n\n    # Normalize power spectrum to units of variance (PSD)\n    if norm:\n        length = len(rates1)\n        norm_factor = length * np.mean(rates1) * np.mean(rates2) / (2 * self.dt)\n        cross_spectrum /= norm_factor\n\n        # negative norm factor shifts the phase by pi\n        if norm_factor &lt; 0:\n            phase = np.angle(cross_spectrum)\n            cross_spectrum = np.abs(cross_spectrum) * np.exp(1j * phase)\n\n    # Apply binning\n    if self.num_bins or self.bin_edges:\n        if self.bin_edges:\n            # use custom bin edges\n            bin_edges = FrequencyBinning.define_bins(\n                self.fmin, self.fmax, num_bins=self.num_bins,\n                bin_type=self.bin_type, bin_edges=self.bin_edges\n            )\n        elif self.num_bins:\n            # use equal-width bins in log or linear space\n            bin_edges = FrequencyBinning.define_bins(\n                self.fmin, self.fmax, num_bins=self.num_bins, bin_type=self.bin_type\n            )\n        else:\n            raise ValueError(\"Either num_bins or bin_edges must be provided.\\n\"\n                             \"In other words, you must specify the number of bins or the bin edges.\")\n\n        binned_cross_spectrum = FrequencyBinning.bin_data(freqs, cross_spectrum, bin_edges)\n        freqs, freq_widths, cross_spectrum, cross_spectrum_errors = binned_cross_spectrum\n    else:\n        freq_widths, cross_spectrum_errors = None, None\n\n    return freqs, freq_widths, cross_spectrum, cross_spectrum_errors\n</code></pre>"},{"location":"reference/cross_spectrum/#stela_toolkit.cross_spectrum.CrossSpectrum.compute_stacked_cross_spectrum","title":"<code>compute_stacked_cross_spectrum(norm=True)</code>","text":"<p>Compute the cross-spectrum across stacked GP samples.</p> <p>Computes the cross-spectrum for each realization and returns the mean and standard deviation across samples.</p>"},{"location":"reference/cross_spectrum/#stela_toolkit.cross_spectrum.CrossSpectrum.compute_stacked_cross_spectrum--parameters","title":"Parameters","text":"<p>norm : bool, optional     Whether to normalize the result to power spectral density units.</p>"},{"location":"reference/cross_spectrum/#stela_toolkit.cross_spectrum.CrossSpectrum.compute_stacked_cross_spectrum--returns","title":"Returns","text":"<p>freqs : array-like     Frequencies of the cross-spectrum. freq_widths : array-like     Widths of frequency bins. cross_spectra_mean : array-like     Mean cross-spectrum across GP samples. cross_spectra_std : array-like     Standard deviation of the cross-spectrum across samples.</p> Source code in <code>stela_toolkit/cross_spectrum.py</code> <pre><code>def compute_stacked_cross_spectrum(self, norm=True):\n    \"\"\"\n    Compute the cross-spectrum across stacked GP samples.\n\n    Computes the cross-spectrum for each realization and returns the mean and\n    standard deviation across samples.\n\n    Parameters\n    ----------\n    norm : bool, optional\n        Whether to normalize the result to power spectral density units.\n\n    Returns\n    -------\n    freqs : array-like\n        Frequencies of the cross-spectrum.\n    freq_widths : array-like\n        Widths of frequency bins.\n    cross_spectra_mean : array-like\n        Mean cross-spectrum across GP samples.\n    cross_spectra_std : array-like\n        Standard deviation of the cross-spectrum across samples.\n    \"\"\"\n\n    cross_spectra = []\n    for i in range(self.rates1.shape[0]):\n        cross_spectrum = self.compute_cross_spectrum(\n            times1=self.times1, rates1=self.rates1[i],\n            times2=self.times2, rates2=self.rates2[i],\n            norm=norm\n        )\n        cross_spectra.append(cross_spectrum[2])\n\n    cross_spectra = np.vstack(cross_spectra)\n    # Real and imaginary std devs\n    cs_real_mean = np.mean(cross_spectra.real, axis=0)\n    cs_imag_mean = np.mean(cross_spectra.imag, axis=0)\n    cs_real_std = np.std(cross_spectra.real, axis=0)\n    cs_imag_std = np.std(cross_spectra.imag, axis=0)\n\n    cross_spectra_mean = cs_real_mean + 1j * cs_imag_mean\n    cross_spectra_std = cs_real_std + 1j * cs_imag_std\n    freqs, freq_widths = cross_spectrum[0], cross_spectrum[1]\n\n    return freqs, freq_widths, cross_spectra_mean, cross_spectra_std\n</code></pre>"},{"location":"reference/cross_spectrum/#stela_toolkit.cross_spectrum.CrossSpectrum.count_frequencies_in_bins","title":"<code>count_frequencies_in_bins(fmin=None, fmax=None, num_bins=None, bin_type=None, bin_edges=[])</code>","text":"<p>Counts the number of frequencies in each frequency bin. Wrapper method to use FrequencyBinning.count_frequencies_in_bins with class attributes.</p> Source code in <code>stela_toolkit/cross_spectrum.py</code> <pre><code>def count_frequencies_in_bins(self, fmin=None, fmax=None, num_bins=None, bin_type=None, bin_edges=[]):\n    \"\"\"\n    Counts the number of frequencies in each frequency bin.\n    Wrapper method to use FrequencyBinning.count_frequencies_in_bins with class attributes.\n    \"\"\"\n\n    return FrequencyBinning.count_frequencies_in_bins(\n        self, fmin=fmin, fmax=fmax, num_bins=num_bins, bin_type=bin_type, bin_edges=bin_edges\n    )\n</code></pre>"},{"location":"reference/cross_spectrum/#stela_toolkit.cross_spectrum.CrossSpectrum.plot","title":"<code>plot(freqs=None, freq_widths=None, cs=None, cs_errors=None, **kwargs)</code>","text":"<p>Plot the real and imaginary parts of the cross-spectrum.</p>"},{"location":"reference/cross_spectrum/#stela_toolkit.cross_spectrum.CrossSpectrum.plot--parameters","title":"Parameters","text":"<p>freqs : array-like, optional     Frequencies at which the cross-spectrum is evaluated. freq_widths : array-like, optional     Widths of the frequency bins. cs : array-like, optional     Cross-spectrum values. cs_errors : array-like, optional     Uncertainties in the cross-spectrum. **kwargs : dict     Additional keyword arguments for plot customization.</p> Source code in <code>stela_toolkit/cross_spectrum.py</code> <pre><code>def plot(self, freqs=None, freq_widths=None, cs=None, cs_errors=None, **kwargs):\n    \"\"\"\n    Plot the real and imaginary parts of the cross-spectrum.\n\n    Parameters\n    ----------\n    freqs : array-like, optional\n        Frequencies at which the cross-spectrum is evaluated.\n    freq_widths : array-like, optional\n        Widths of the frequency bins.\n    cs : array-like, optional\n        Cross-spectrum values.\n    cs_errors : array-like, optional\n        Uncertainties in the cross-spectrum.\n    **kwargs : dict\n        Additional keyword arguments for plot customization.\n    \"\"\"\n    import matplotlib.pyplot as plt\n\n    freqs = self.freqs if freqs is None else freqs\n    freq_widths = self.freq_widths if freq_widths is None else freq_widths\n    cs = self.cs if cs is None else cs\n    cs_errors = self.cs_errors if cs_errors is None else cs_errors\n\n    figsize = kwargs.get('figsize', (8, 4.5))\n    xlabel = kwargs.get('xlabel', 'Frequency')\n    ylabel = kwargs.get('ylabel', 'Cross-Spectrum')\n    xscale = kwargs.get('xscale', 'log')\n    yscale = kwargs.get('yscale', 'log')\n\n    plt.figure(figsize=figsize)\n\n    # Real part\n    if cs_errors is not None:\n        plt.errorbar(freqs, cs.real, xerr=freq_widths, yerr=cs_errors.real,\n                     fmt='o', color='black', ms=3, lw=1.5, label='Real')\n    else:\n        plt.plot(freqs, cs.real, 'o-', color='black', ms=3, lw=1.5, label='Real')\n\n    # Imaginary part\n    if cs_errors is not None:\n        plt.errorbar(freqs, cs.imag, xerr=freq_widths, yerr=cs_errors.imag,\n                     fmt='o', color='red', ms=3, lw=1.5, label='Imag')\n    else:\n        plt.plot(freqs, cs.imag, 'o-', color='red', ms=3, lw=1.5, label='Imag')\n\n    plt.xlabel(xlabel, fontsize=12)\n    plt.ylabel(ylabel, fontsize=12)\n    plt.xscale(xscale)\n    plt.yscale(yscale)\n    plt.legend(loc='best')\n    plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n    plt.tick_params(which='both', direction='in', length=6, width=1,\n                    top=True, right=True, labelsize=12)\n    plt.show()\n</code></pre>"},{"location":"reference/data_loader/","title":"data_loader","text":""},{"location":"reference/data_loader/#stela_toolkit.data_loader.LightCurve","title":"<code>LightCurve</code>","text":"<p>Container for light curve data, including time, rate, and optional error arrays.</p> <p>This class is the standard format for handling time series in the STELA Toolkit. Light curves can be initialized directly from arrays or loaded from supported file formats (FITS, CSV, or plain text). Many analysis modules assume regular time sampling, which is  enforced or checked as needed.</p> <p>Supports basic arithmetic operations (addition, subtraction, division) with other LightCurve  objects and provides utilities for plotting and computing Fourier transforms.</p>"},{"location":"reference/data_loader/#stela_toolkit.data_loader.LightCurve--parameters","title":"Parameters","text":"<p>times : array-like, optional     Array of time values. rates : array-like, optional     Array of measured rates (e.g., flux, count rate). errors : array-like, optional     Array of uncertainties on the rates. Optional but recommended. file_path : str, optional     Path to a file to load light curve data from. Supports FITS and text formats. file_columns : list of int or str, optional     List specifying the columns to read as [time, rate, error]. Column names or indices allowed.</p>"},{"location":"reference/data_loader/#stela_toolkit.data_loader.LightCurve--attributes","title":"Attributes","text":"<p>times : ndarray     Array of time values. rates : ndarray     Array of rate values. errors : ndarray     Array of errors, if provided.</p> Source code in <code>stela_toolkit/data_loader.py</code> <pre><code>class LightCurve:\n    \"\"\"\n    Container for light curve data, including time, rate, and optional error arrays.\n\n    This class is the standard format for handling time series in the STELA Toolkit.\n    Light curves can be initialized directly from arrays or loaded from supported file formats\n    (FITS, CSV, or plain text). Many analysis modules assume regular time sampling, which is \n    enforced or checked as needed.\n\n    Supports basic arithmetic operations (addition, subtraction, division) with other LightCurve \n    objects and provides utilities for plotting and computing Fourier transforms.\n\n    Parameters\n    ----------\n    times : array-like, optional\n        Array of time values.\n    rates : array-like, optional\n        Array of measured rates (e.g., flux, count rate).\n    errors : array-like, optional\n        Array of uncertainties on the rates. Optional but recommended.\n    file_path : str, optional\n        Path to a file to load light curve data from. Supports FITS and text formats.\n    file_columns : list of int or str, optional\n        List specifying the columns to read as [time, rate, error]. Column names or indices allowed.\n\n    Attributes\n    ----------\n    times : ndarray\n        Array of time values.\n    rates : ndarray\n        Array of rate values.\n    errors : ndarray\n        Array of errors, if provided.\n    \"\"\"\n\n    def __init__(self,\n                 times=[],\n                 rates=[],\n                 errors=[],\n                 file_path=None,\n                 file_columns=[0, 1, 2]):\n\n        if file_path:\n            if not (2 &lt;= len(file_columns) &lt;= 3):\n                raise ValueError(\n                    \"The 'file_columns' parameter must be a list with 2 or 3 items: \"\n                    \"[time_column, rate_column, optional error_column].\"\n                )\n\n            file_data = self.load_file(file_path, file_columns=file_columns)\n            times, rates, errors = file_data\n\n        elif len(times) &gt; 0 and len(rates) &gt; 0:\n            pass\n\n        else:\n            raise ValueError(\n                \"Please provide time and rate arrays or a file path.\"\n            )\n\n        self.times, self.rates, self.errors = _CheckInputs._check_input_data(lightcurve=None,\n                                                                             times=times,\n                                                                             rates=rates,\n                                                                             errors=errors,\n                                                                             req_reg_samp=False\n                                                                             )\n\n    @property\n    def mean(self):\n        \"\"\"\n        Return the mean of the light curve rates.\n        \"\"\"\n        return np.mean(self.rates)\n\n    @property\n    def std(self):\n        \"\"\"\n        Return the standard deviation of the light curve rates.\n        \"\"\"\n        return np.std(self.rates)\n\n    def load_file(self, file_path, file_columns=[0, 1, 2]):\n        \"\"\"\n        Load light curve data from a FITS or text-based file.\n\n        Parameters\n        ----------\n        file_path : str\n            Path to the input file.\n        file_columns : list of int or str\n            Columns to use for [time, rate, error].\n\n        Returns\n        -------\n        times, rates, errors : tuple of ndarrays\n            Loaded arrays of time, rate, and error.\n        \"\"\"\n\n        try:\n            times, rates, errors = self.load_fits(file_path, file_columns)\n\n        except:\n            try:\n                times, rates, errors = self.load_text_file(file_path, file_columns)\n            except Exception as e:\n                raise RuntimeError(\n                    f\"Failed to read the file '{file_path}' with fits or text-based loader.\\n\"\n                    \"Verify the file path and file_columns, or file format unsupported.\\n\"\n                    f\"Error message: {e}\"\n                )\n\n        return times, rates, errors\n\n    def load_fits(self, file_path, file_columns=[0, 1, 2], hdu=1):\n        \"\"\"\n        Load light curve data from a specified HDU of a FITS file.\n\n        Parameters\n        ----------\n        file_path : str\n            Path to the FITS file.\n        file_columns : list\n            Columns to extract as [time, rate, error].\n        hdu : int\n            Header/Data Unit (HDU) index to read from.\n\n        Returns\n        -------\n        times, rates, errors : tuple of ndarrays\n            Arrays of time, rate, and error values.\n        \"\"\"\n\n        time_column, rate_column = file_columns[0], file_columns[1]\n        error_column = file_columns[2] if len(file_columns) == 3 else None\n\n        with fits.open(file_path) as hdul:\n            try:\n                data = hdul[hdu].data\n            except IndexError:\n                raise ValueError(f\"HDU {hdu} does not exist in the FITS file.\")\n\n            try:\n                times = np.array(\n                    data.field(time_column) if isinstance(time_column, int)\n                    else data[time_column]\n                ).astype(float)\n\n                rates = np.array(\n                    data.field(rate_column) if isinstance(rate_column, int)\n                    else data[rate_column]\n                ).astype(float)\n\n                if error_column:\n                    errors = np.array(\n                        data.field(error_column) if isinstance(error_column, int)\n                        else data[error_column]\n                    ).astype(float)\n                else:\n                    errors = []\n\n            except KeyError:\n                raise ValueError(\n                    \"Specified column/s not found in the FITS file.\"\n                )\n\n        return times, rates, errors\n\n    def load_text_file(self, file_path, file_columns=[0, 1, 2], delimiter=None):\n        \"\"\"\n        Load light curve data from a CSV or plain-text file.\n\n        Parameters\n        ----------\n        file_path : str\n            Path to the input text file.\n        file_columns : list\n            Columns to extract as [time, rate, error].\n        delimiter : str, optional\n            Delimiter used in the file (guessed from extension if not provided).\n\n        Returns\n        -------\n        times, rates, errors : tuple of ndarrays\n            Arrays of time, rate, and error values.\n        \"\"\"\n\n        time_column, rate_column = file_columns[0], file_columns[1]\n        error_column = file_columns[2] if len(file_columns) == 3 else None\n\n        # Load data, assuming delimiter based on file extension if unspecified\n        if delimiter is None:\n            delimiter = ',' if file_path.endswith('.csv') else None\n\n        try:\n            data = np.genfromtxt(\n                file_path,\n                delimiter=delimiter,\n            )\n\n        except Exception as e:\n            raise (f\"Failed to read the file '{file_path}' with np.genfromtxt.\")\n\n        # Retrieve file_columns by name or index directly, simplifying access\n        times = np.array(\n            data[time_column] if isinstance(time_column, str)\n            else data[:, time_column]\n        ).astype(float)\n\n        rates = np.array(\n            data[rate_column] if isinstance(rate_column, str)\n            else data[:, rate_column]\n        ).astype(float)\n\n        if error_column:\n            errors = np.array(\n                data[error_column]if isinstance(error_column, str)\n                else data[:, error_column]\n            ).astype(float)\n\n        else:\n            errors = []\n\n        return times, rates, errors\n\n    def plot(self, **kwargs):\n        \"\"\"\n        Plot the light curve.\n\n        Parameters\n        ----------\n        **kwargs : dict\n            Additional keyword arguments for customizing the plot.\n        \"\"\"\n\n        kwargs.setdefault('xlabel', 'Time')\n        kwargs.setdefault('ylabel', 'Rate')\n        Plotter.plot(x=self.times, y=self.rates, yerr=self.errors, **kwargs)\n\n    def fft(self):\n        \"\"\"\n        Compute the Fast Fourier Transform (FFT) of the light curve.\n\n        Returns\n        -------\n        freqs : ndarray\n            Frequencies of the FFT.\n        fft_values : ndarray\n            Complex FFT values.\n\n        Raises\n        ------\n        ValueError\n            If the time sampling is not uniform. Interpolation is required before applying FFT.\n        \"\"\"\n\n        time_diffs = np.round(np.diff(self.times), 10)\n        if np.unique(time_diffs).size &gt; 1:\n            raise ValueError(\"Light curve must have a uniform sampling interval.\\n\"\n                             \"Interpolate the data to a uniform grid first.\"\n                             )\n        dt = np.diff(self.times)[0]\n        length = len(self.rates)\n\n        fft_values = np.fft.rfft(self.rates)\n        freqs = np.fft.rfftfreq(length, d=dt)\n\n        return freqs, fft_values\n\n    def __add__(self, other_lightcurve):\n        \"\"\"\n        Add two LightCurve objects element-wise.\n\n        Returns\n        -------\n        LightCurve\n            New LightCurve with summed rates and propagated uncertainties.\n        \"\"\"\n\n        if not isinstance(other_lightcurve, LightCurve):\n            raise TypeError(\n                \"Both light curve must be an instance of the LightCurve class.\"\n            )\n\n        if not np.array_equal(self.times, other_lightcurve.times):\n            raise ValueError(\"Time arrays do not match.\")\n\n        new_rates = self.rates + other_lightcurve.rates\n        if self.errors.size == 0 or other_lightcurve.errors.size == 0:\n            new_errors = []\n\n        else:\n            new_errors = np.sqrt(self.errors**2 + other_lightcurve.errors**2)\n\n        return LightCurve(times=self.times,\n                          rates=new_rates,\n                          errors=new_errors)\n\n    def __sub__(self, other_lightcurve):\n        \"\"\"\n        Subtract one LightCurve from another element-wise.\n\n        Returns\n        -------\n        LightCurve\n            New LightCurve with difference of rates and propagated uncertainties.\n        \"\"\"\n\n        if not isinstance(other_lightcurve, LightCurve):\n            raise TypeError(\n                \"Both light curve must be an instance of the LightCurve class.\"\n            )\n\n        if not np.array_equal(self.times, other_lightcurve.times):\n            raise ValueError(\"Time arrays do not match.\")\n\n        new_rates = self.rates - other_lightcurve.rates\n        if self.errors.size == 0 or other_lightcurve.errors.size == 0:\n            new_errors = []\n\n        else:\n            new_errors = np.sqrt(self.errors**2 + other_lightcurve.errors**2)\n\n        return LightCurve(times=self.times,\n                          rates=new_rates,\n                          errors=new_errors\n                          )\n\n    def __truediv__(self, other_lightcurve):\n        \"\"\"\n        Divide one LightCurve by another element-wise.\n\n        Returns\n        -------\n        LightCurve\n            New LightCurve with element-wise division and propagated relative uncertainties.\n        \"\"\"\n\n        if not isinstance(other_lightcurve, LightCurve):\n            raise TypeError(\n                \"Both light curve must be an instance of the LightCurve class.\"\n            )\n\n        if not np.array_equal(self.times, other_lightcurve.times):\n            raise ValueError(\"Time arrays do not match.\")\n\n        new_rates = self.rates / other_lightcurve.rates\n        if self.errors.size == 0 or other_lightcurve.errors.size == 0:\n            new_errors = []\n\n        else:\n            new_errors = np.sqrt(\n                (self.errors / self.rates) ** 2\n                + (other_lightcurve.errors / other_lightcurve.rates) ** 2\n            )\n\n        return LightCurve(times=self.times,\n                          rates=new_rates,\n                          errors=new_errors)\n</code></pre>"},{"location":"reference/data_loader/#stela_toolkit.data_loader.LightCurve.mean","title":"<code>mean</code>  <code>property</code>","text":"<p>Return the mean of the light curve rates.</p>"},{"location":"reference/data_loader/#stela_toolkit.data_loader.LightCurve.std","title":"<code>std</code>  <code>property</code>","text":"<p>Return the standard deviation of the light curve rates.</p>"},{"location":"reference/data_loader/#stela_toolkit.data_loader.LightCurve.__add__","title":"<code>__add__(other_lightcurve)</code>","text":"<p>Add two LightCurve objects element-wise.</p>"},{"location":"reference/data_loader/#stela_toolkit.data_loader.LightCurve.__add__--returns","title":"Returns","text":"<p>LightCurve     New LightCurve with summed rates and propagated uncertainties.</p> Source code in <code>stela_toolkit/data_loader.py</code> <pre><code>def __add__(self, other_lightcurve):\n    \"\"\"\n    Add two LightCurve objects element-wise.\n\n    Returns\n    -------\n    LightCurve\n        New LightCurve with summed rates and propagated uncertainties.\n    \"\"\"\n\n    if not isinstance(other_lightcurve, LightCurve):\n        raise TypeError(\n            \"Both light curve must be an instance of the LightCurve class.\"\n        )\n\n    if not np.array_equal(self.times, other_lightcurve.times):\n        raise ValueError(\"Time arrays do not match.\")\n\n    new_rates = self.rates + other_lightcurve.rates\n    if self.errors.size == 0 or other_lightcurve.errors.size == 0:\n        new_errors = []\n\n    else:\n        new_errors = np.sqrt(self.errors**2 + other_lightcurve.errors**2)\n\n    return LightCurve(times=self.times,\n                      rates=new_rates,\n                      errors=new_errors)\n</code></pre>"},{"location":"reference/data_loader/#stela_toolkit.data_loader.LightCurve.__sub__","title":"<code>__sub__(other_lightcurve)</code>","text":"<p>Subtract one LightCurve from another element-wise.</p>"},{"location":"reference/data_loader/#stela_toolkit.data_loader.LightCurve.__sub__--returns","title":"Returns","text":"<p>LightCurve     New LightCurve with difference of rates and propagated uncertainties.</p> Source code in <code>stela_toolkit/data_loader.py</code> <pre><code>def __sub__(self, other_lightcurve):\n    \"\"\"\n    Subtract one LightCurve from another element-wise.\n\n    Returns\n    -------\n    LightCurve\n        New LightCurve with difference of rates and propagated uncertainties.\n    \"\"\"\n\n    if not isinstance(other_lightcurve, LightCurve):\n        raise TypeError(\n            \"Both light curve must be an instance of the LightCurve class.\"\n        )\n\n    if not np.array_equal(self.times, other_lightcurve.times):\n        raise ValueError(\"Time arrays do not match.\")\n\n    new_rates = self.rates - other_lightcurve.rates\n    if self.errors.size == 0 or other_lightcurve.errors.size == 0:\n        new_errors = []\n\n    else:\n        new_errors = np.sqrt(self.errors**2 + other_lightcurve.errors**2)\n\n    return LightCurve(times=self.times,\n                      rates=new_rates,\n                      errors=new_errors\n                      )\n</code></pre>"},{"location":"reference/data_loader/#stela_toolkit.data_loader.LightCurve.__truediv__","title":"<code>__truediv__(other_lightcurve)</code>","text":"<p>Divide one LightCurve by another element-wise.</p>"},{"location":"reference/data_loader/#stela_toolkit.data_loader.LightCurve.__truediv__--returns","title":"Returns","text":"<p>LightCurve     New LightCurve with element-wise division and propagated relative uncertainties.</p> Source code in <code>stela_toolkit/data_loader.py</code> <pre><code>def __truediv__(self, other_lightcurve):\n    \"\"\"\n    Divide one LightCurve by another element-wise.\n\n    Returns\n    -------\n    LightCurve\n        New LightCurve with element-wise division and propagated relative uncertainties.\n    \"\"\"\n\n    if not isinstance(other_lightcurve, LightCurve):\n        raise TypeError(\n            \"Both light curve must be an instance of the LightCurve class.\"\n        )\n\n    if not np.array_equal(self.times, other_lightcurve.times):\n        raise ValueError(\"Time arrays do not match.\")\n\n    new_rates = self.rates / other_lightcurve.rates\n    if self.errors.size == 0 or other_lightcurve.errors.size == 0:\n        new_errors = []\n\n    else:\n        new_errors = np.sqrt(\n            (self.errors / self.rates) ** 2\n            + (other_lightcurve.errors / other_lightcurve.rates) ** 2\n        )\n\n    return LightCurve(times=self.times,\n                      rates=new_rates,\n                      errors=new_errors)\n</code></pre>"},{"location":"reference/data_loader/#stela_toolkit.data_loader.LightCurve.fft","title":"<code>fft()</code>","text":"<p>Compute the Fast Fourier Transform (FFT) of the light curve.</p>"},{"location":"reference/data_loader/#stela_toolkit.data_loader.LightCurve.fft--returns","title":"Returns","text":"<p>freqs : ndarray     Frequencies of the FFT. fft_values : ndarray     Complex FFT values.</p>"},{"location":"reference/data_loader/#stela_toolkit.data_loader.LightCurve.fft--raises","title":"Raises","text":"<p>ValueError     If the time sampling is not uniform. Interpolation is required before applying FFT.</p> Source code in <code>stela_toolkit/data_loader.py</code> <pre><code>def fft(self):\n    \"\"\"\n    Compute the Fast Fourier Transform (FFT) of the light curve.\n\n    Returns\n    -------\n    freqs : ndarray\n        Frequencies of the FFT.\n    fft_values : ndarray\n        Complex FFT values.\n\n    Raises\n    ------\n    ValueError\n        If the time sampling is not uniform. Interpolation is required before applying FFT.\n    \"\"\"\n\n    time_diffs = np.round(np.diff(self.times), 10)\n    if np.unique(time_diffs).size &gt; 1:\n        raise ValueError(\"Light curve must have a uniform sampling interval.\\n\"\n                         \"Interpolate the data to a uniform grid first.\"\n                         )\n    dt = np.diff(self.times)[0]\n    length = len(self.rates)\n\n    fft_values = np.fft.rfft(self.rates)\n    freqs = np.fft.rfftfreq(length, d=dt)\n\n    return freqs, fft_values\n</code></pre>"},{"location":"reference/data_loader/#stela_toolkit.data_loader.LightCurve.load_file","title":"<code>load_file(file_path, file_columns=[0, 1, 2])</code>","text":"<p>Load light curve data from a FITS or text-based file.</p>"},{"location":"reference/data_loader/#stela_toolkit.data_loader.LightCurve.load_file--parameters","title":"Parameters","text":"<p>file_path : str     Path to the input file. file_columns : list of int or str     Columns to use for [time, rate, error].</p>"},{"location":"reference/data_loader/#stela_toolkit.data_loader.LightCurve.load_file--returns","title":"Returns","text":"<p>times, rates, errors : tuple of ndarrays     Loaded arrays of time, rate, and error.</p> Source code in <code>stela_toolkit/data_loader.py</code> <pre><code>def load_file(self, file_path, file_columns=[0, 1, 2]):\n    \"\"\"\n    Load light curve data from a FITS or text-based file.\n\n    Parameters\n    ----------\n    file_path : str\n        Path to the input file.\n    file_columns : list of int or str\n        Columns to use for [time, rate, error].\n\n    Returns\n    -------\n    times, rates, errors : tuple of ndarrays\n        Loaded arrays of time, rate, and error.\n    \"\"\"\n\n    try:\n        times, rates, errors = self.load_fits(file_path, file_columns)\n\n    except:\n        try:\n            times, rates, errors = self.load_text_file(file_path, file_columns)\n        except Exception as e:\n            raise RuntimeError(\n                f\"Failed to read the file '{file_path}' with fits or text-based loader.\\n\"\n                \"Verify the file path and file_columns, or file format unsupported.\\n\"\n                f\"Error message: {e}\"\n            )\n\n    return times, rates, errors\n</code></pre>"},{"location":"reference/data_loader/#stela_toolkit.data_loader.LightCurve.load_fits","title":"<code>load_fits(file_path, file_columns=[0, 1, 2], hdu=1)</code>","text":"<p>Load light curve data from a specified HDU of a FITS file.</p>"},{"location":"reference/data_loader/#stela_toolkit.data_loader.LightCurve.load_fits--parameters","title":"Parameters","text":"<p>file_path : str     Path to the FITS file. file_columns : list     Columns to extract as [time, rate, error]. hdu : int     Header/Data Unit (HDU) index to read from.</p>"},{"location":"reference/data_loader/#stela_toolkit.data_loader.LightCurve.load_fits--returns","title":"Returns","text":"<p>times, rates, errors : tuple of ndarrays     Arrays of time, rate, and error values.</p> Source code in <code>stela_toolkit/data_loader.py</code> <pre><code>def load_fits(self, file_path, file_columns=[0, 1, 2], hdu=1):\n    \"\"\"\n    Load light curve data from a specified HDU of a FITS file.\n\n    Parameters\n    ----------\n    file_path : str\n        Path to the FITS file.\n    file_columns : list\n        Columns to extract as [time, rate, error].\n    hdu : int\n        Header/Data Unit (HDU) index to read from.\n\n    Returns\n    -------\n    times, rates, errors : tuple of ndarrays\n        Arrays of time, rate, and error values.\n    \"\"\"\n\n    time_column, rate_column = file_columns[0], file_columns[1]\n    error_column = file_columns[2] if len(file_columns) == 3 else None\n\n    with fits.open(file_path) as hdul:\n        try:\n            data = hdul[hdu].data\n        except IndexError:\n            raise ValueError(f\"HDU {hdu} does not exist in the FITS file.\")\n\n        try:\n            times = np.array(\n                data.field(time_column) if isinstance(time_column, int)\n                else data[time_column]\n            ).astype(float)\n\n            rates = np.array(\n                data.field(rate_column) if isinstance(rate_column, int)\n                else data[rate_column]\n            ).astype(float)\n\n            if error_column:\n                errors = np.array(\n                    data.field(error_column) if isinstance(error_column, int)\n                    else data[error_column]\n                ).astype(float)\n            else:\n                errors = []\n\n        except KeyError:\n            raise ValueError(\n                \"Specified column/s not found in the FITS file.\"\n            )\n\n    return times, rates, errors\n</code></pre>"},{"location":"reference/data_loader/#stela_toolkit.data_loader.LightCurve.load_text_file","title":"<code>load_text_file(file_path, file_columns=[0, 1, 2], delimiter=None)</code>","text":"<p>Load light curve data from a CSV or plain-text file.</p>"},{"location":"reference/data_loader/#stela_toolkit.data_loader.LightCurve.load_text_file--parameters","title":"Parameters","text":"<p>file_path : str     Path to the input text file. file_columns : list     Columns to extract as [time, rate, error]. delimiter : str, optional     Delimiter used in the file (guessed from extension if not provided).</p>"},{"location":"reference/data_loader/#stela_toolkit.data_loader.LightCurve.load_text_file--returns","title":"Returns","text":"<p>times, rates, errors : tuple of ndarrays     Arrays of time, rate, and error values.</p> Source code in <code>stela_toolkit/data_loader.py</code> <pre><code>def load_text_file(self, file_path, file_columns=[0, 1, 2], delimiter=None):\n    \"\"\"\n    Load light curve data from a CSV or plain-text file.\n\n    Parameters\n    ----------\n    file_path : str\n        Path to the input text file.\n    file_columns : list\n        Columns to extract as [time, rate, error].\n    delimiter : str, optional\n        Delimiter used in the file (guessed from extension if not provided).\n\n    Returns\n    -------\n    times, rates, errors : tuple of ndarrays\n        Arrays of time, rate, and error values.\n    \"\"\"\n\n    time_column, rate_column = file_columns[0], file_columns[1]\n    error_column = file_columns[2] if len(file_columns) == 3 else None\n\n    # Load data, assuming delimiter based on file extension if unspecified\n    if delimiter is None:\n        delimiter = ',' if file_path.endswith('.csv') else None\n\n    try:\n        data = np.genfromtxt(\n            file_path,\n            delimiter=delimiter,\n        )\n\n    except Exception as e:\n        raise (f\"Failed to read the file '{file_path}' with np.genfromtxt.\")\n\n    # Retrieve file_columns by name or index directly, simplifying access\n    times = np.array(\n        data[time_column] if isinstance(time_column, str)\n        else data[:, time_column]\n    ).astype(float)\n\n    rates = np.array(\n        data[rate_column] if isinstance(rate_column, str)\n        else data[:, rate_column]\n    ).astype(float)\n\n    if error_column:\n        errors = np.array(\n            data[error_column]if isinstance(error_column, str)\n            else data[:, error_column]\n        ).astype(float)\n\n    else:\n        errors = []\n\n    return times, rates, errors\n</code></pre>"},{"location":"reference/data_loader/#stela_toolkit.data_loader.LightCurve.plot","title":"<code>plot(**kwargs)</code>","text":"<p>Plot the light curve.</p>"},{"location":"reference/data_loader/#stela_toolkit.data_loader.LightCurve.plot--parameters","title":"Parameters","text":"<p>**kwargs : dict     Additional keyword arguments for customizing the plot.</p> Source code in <code>stela_toolkit/data_loader.py</code> <pre><code>def plot(self, **kwargs):\n    \"\"\"\n    Plot the light curve.\n\n    Parameters\n    ----------\n    **kwargs : dict\n        Additional keyword arguments for customizing the plot.\n    \"\"\"\n\n    kwargs.setdefault('xlabel', 'Time')\n    kwargs.setdefault('ylabel', 'Rate')\n    Plotter.plot(x=self.times, y=self.rates, yerr=self.errors, **kwargs)\n</code></pre>"},{"location":"reference/data_simulator/","title":"data_simulator","text":""},{"location":"reference/data_simulator/#stela_toolkit.data_simulator.SimulateLightCurve","title":"<code>SimulateLightCurve</code>","text":"<p>Simulates light curves with a specified power spectral density (PSD) and optional lag injection via an impulse response function (IRF). The light curve is generated using the Timmer &amp; Koenig (1995) method: amplitudes in Fourier space are set by the PSD, and phases are randomized uniformly. The signal is then inverse-Fourier transformed to obtain the time-domain light curve.</p> <p>The clean light curve is rescaled to have the desired mean and standard deviation. Poisson noise may be added to simulate counting statistics, including optional background noise.</p> <p>Supports both regularly and irregularly sampled time grids: - For regular grids: the light curve is oversampled (by default 10\u00d7) and then trimmed. - For irregular grids: the light curve is generated on a fine grid and sampled at the closest   points (no interpolation).</p> <p>Optional lag injection is supported by convolving the light curve with an IRF.</p>"},{"location":"reference/data_simulator/#stela_toolkit.data_simulator.SimulateLightCurve--parameters","title":"Parameters","text":"<p>time_grid : ndarray     The array of time stamps for which to simulate the light curve. Can be regular or irregular,     but must be sorted and contain at least two points.</p> str <p>Type of power spectral density (PSD) to use. Options are: - 'powerlaw': a simple power law PSD. - 'broken_powerlaw': a PSD with two slopes joined at a break frequency.</p> dict <p>Parameters for the PSD. Required keys depend on the PSD type: - For 'powerlaw': {'slope', 'plnorm'} - For 'broken_powerlaw': {'slope1', 'f_break', 'slope2', 'plnorm'}</p> float <p>Desired mean count rate of the simulated light curve (after rescaling).</p> float <p>Desired standard deviation of the simulated light curve (after rescaling).</p> str, optional <p>Must define add noise to have uncertainties on the final data. If \"Poisson\": Poisson noise is added to the light curve, used to compute uncertainties. If \"Gaussian\": Gaussian noise is added to the light curve, using the uncertainties specified by frac_error.     - Use gaussian_frac_error to define the errors in this case.</p> ndarray or None, optional <p>Exposure durations to use for adding the Poisson noise. If None, the time spacing is used to approximate exposure times.</p> float, optional <p>Background rate in counts per unit time to include in the noise simulation (default: 0.0). If set, Poisson noise from two background realizations is added and subtracted.</p> int, optional <p>For regular grids: how much to oversample before trimming to the target grid (default: 10).</p> int, optional <p>For irregular grids: how densely to simulate the light curve before selecting closest points (default: 100).</p> bool, optional <p>If True, applies lag injection by convolving with an IRF (default: False).</p> str or None, optional <p>Type of impulse response function to use. Options are: - 'delta': a single delay spike at a fixed lag - 'normal': Gaussian-shaped IRF - 'lognormal': log-normal IRF with skewed tail - 'manual': user-supplied kernel If None, no lag is injected (default: None).</p> dict or None, optional <p>Parameters for the selected response_type: - For 'delta': {'lag': float} - For 'normal': {'mean': float, 'sigma': float, 'duration': float (optional)} - For 'lognormal': {'median': float, 'sigma': float, 'duration': float (optional)} - For 'manual': {'response': array_like}</p> Source code in <code>stela_toolkit/data_simulator.py</code> <pre><code>class SimulateLightCurve:\n    \"\"\"\n    Simulates light curves with a specified power spectral density (PSD) and optional lag injection\n    via an impulse response function (IRF). The light curve is generated using the Timmer &amp; Koenig\n    (1995) method: amplitudes in Fourier space are set by the PSD, and phases are randomized\n    uniformly. The signal is then inverse-Fourier transformed to obtain the time-domain light curve.\n\n    The clean light curve is rescaled to have the desired mean and standard deviation. Poisson noise\n    may be added to simulate counting statistics, including optional background noise.\n\n    Supports both regularly and irregularly sampled time grids:\n    - For regular grids: the light curve is oversampled (by default 10\u00d7) and then trimmed.\n    - For irregular grids: the light curve is generated on a fine grid and sampled at the closest\n      points (no interpolation).\n\n    Optional lag injection is supported by convolving the light curve with an IRF.\n\n    Parameters\n    ----------\n    time_grid : ndarray\n        The array of time stamps for which to simulate the light curve. Can be regular or irregular,\n        but must be sorted and contain at least two points.\n\n    psd_type : str\n        Type of power spectral density (PSD) to use. Options are:\n        - 'powerlaw': a simple power law PSD.\n        - 'broken_powerlaw': a PSD with two slopes joined at a break frequency.\n\n    psd_params : dict\n        Parameters for the PSD. Required keys depend on the PSD type:\n        - For 'powerlaw': {'slope', 'plnorm'}\n        - For 'broken_powerlaw': {'slope1', 'f_break', 'slope2', 'plnorm'}\n\n    mean : float\n        Desired mean count rate of the simulated light curve (after rescaling).\n\n    std : float\n        Desired standard deviation of the simulated light curve (after rescaling).\n\n    add_noise : str, optional\n        Must define add noise to have uncertainties on the final data.\n        If \"Poisson\": Poisson noise is added to the light curve, used to compute uncertainties.\n        If \"Gaussian\": Gaussian noise is added to the light curve, using the uncertainties specified by frac_error.\n            - Use gaussian_frac_error to define the errors in this case.\n\n    exposure_times : ndarray or None, optional\n        Exposure durations to use for adding the Poisson noise. If None,\n        the time spacing is used to approximate exposure times.\n\n    bkg_rate : float, optional\n        Background rate in counts per unit time to include in the noise simulation (default: 0.0).\n        If set, Poisson noise from two background realizations is added and subtracted.\n\n    oversample : int, optional\n        For regular grids: how much to oversample before trimming to the target grid (default: 10).\n\n    fine_factor : int, optional\n        For irregular grids: how densely to simulate the light curve before selecting closest points\n        (default: 100).\n\n    inject_lag : bool, optional\n        If True, applies lag injection by convolving with an IRF (default: False).\n\n    response_type : str or None, optional\n        Type of impulse response function to use. Options are:\n        - 'delta': a single delay spike at a fixed lag\n        - 'normal': Gaussian-shaped IRF\n        - 'lognormal': log-normal IRF with skewed tail\n        - 'manual': user-supplied kernel\n        If None, no lag is injected (default: None).\n\n    response_params : dict or None, optional\n        Parameters for the selected response_type:\n        - For 'delta': {'lag': float}\n        - For 'normal': {'mean': float, 'sigma': float, 'duration': float (optional)}\n        - For 'lognormal': {'median': float, 'sigma': float, 'duration': float (optional)}\n        - For 'manual': {'response': array_like}\n    \"\"\"\n\n    def __init__(self,\n                 time_grid,\n                 psd_type,\n                 psd_params,\n                 mean,\n                 std,\n                 add_noise=None,\n                 gaussian_frac_err=None,\n                 bkg_rate=0.0,\n                 oversample=10,\n                 fine_factor=100,\n                 inject_lag=False,\n                 response_type=None,\n                 response_params=None):\n\n        self.time_grid = np.asarray(time_grid)\n        self.psd_type = psd_type\n        self.psd_params = psd_params\n        self.mean = mean\n        self.std = std\n        self.oversample = oversample\n        self.fine_factor = fine_factor\n        self.bkg_rate = bkg_rate\n        self.inject_lag = inject_lag\n        self.response_type = response_type\n        self.response_params = response_params\n\n        result = self.generate(self.time_grid)\n        if isinstance(result, tuple):\n            rates, rates_lagged = result\n        else:\n            rates = result\n            rates_lagged = None\n\n        errors = np.zeros(len(rates))\n        if add_noise.lower() == \"poisson\":\n            rates, errors = self.add_poisson_noise(rates, self.time_grid, bkg_rate=self.bkg_rate)\n\n            if rates_lagged is not None:\n                rates_lagged, _ = self.add_poisson_noise(rates_lagged, self.time_grid,\n                                                        bkg_rate=self.bkg_rate)\n\n        elif add_noise.lower() == \"gaussian\":\n            rates, errors = self.add_gaussian_noise(rates, frac_err=gaussian_frac_err)\n            if rates_lagged is not None:\n                rates_lagged, _ = self.add_gaussian_noise(rates_lagged, frac_err=gaussian_frac_err)\n\n        self.rates = rates\n        self.errors = errors\n        self.simlc = LightCurve(times=self.time_grid, rates=rates, errors=errors)\n        self.simlc_lagged = (\n            LightCurve(times=self.time_grid, rates=rates_lagged, errors=errors)\n            if rates_lagged is not None else None\n        )\n\n    def generate(self, time_grid):\n        \"\"\"\n        Generate the clean (noise-free) light curve.\n\n        Handles regular vs. irregular time grids and applies normalization.\n        If lag injection is enabled, convolves with a response function.\n\n        Parameters\n        ----------\n        time_grid : array-like\n            Desired output time grid.\n\n        Returns\n        -------\n        rates : ndarray\n            Simulated light curve values.\n        rates_lagged : ndarray or None\n            Lagged version of the light curve if `inject_lag` is True.\n        \"\"\"\n\n        time_grid = np.array(time_grid)\n        n_target = len(time_grid)\n        dt_array = np.diff(time_grid)\n        is_regular = np.allclose(dt_array, dt_array[0], rtol=1e-5)\n\n        if is_regular:\n            n_sim = int(self.oversample * n_target)\n            t_sim = np.linspace(time_grid[0], time_grid[-1], n_sim)\n            lc_sim = self._simulate_on_grid(t_sim)\n\n            start = (n_sim - n_target) // 2\n            end = start + n_target\n            lc = lc_sim[start:end]\n\n            lc -= np.mean(lc)\n            lc /= np.std(lc)\n            lc = lc * self.std + self.mean\n\n            if self.inject_lag:\n                kernel = self._build_impulse_response(time_grid)\n                convolved_full = fftconvolve(lc_sim, kernel, mode=\"full\")\n                convolved = convolved_full[start:end]\n\n                convolved -= np.mean(convolved)\n                convolved /= np.std(convolved)\n                convolved = convolved * self.std + self.mean\n                return lc, convolved\n            else:\n                return lc\n\n        else:\n            n_fine = int(self.fine_factor * len(time_grid))\n            t_fine = np.linspace(time_grid.min(), time_grid.max(), n_fine)\n            lc_fine = self._simulate_on_grid(t_fine)\n\n            lc_fine -= np.mean(lc_fine)\n            lc_fine /= np.std(lc_fine)\n            lc_fine = lc_fine * self.std + self.mean\n\n            if self.inject_lag:\n                kernel = self._build_impulse_response(t_fine)\n                lc_fine_lagged_full = fftconvolve(lc_fine, kernel, mode=\"full\")\n                lc_fine_lagged = lc_fine_lagged_full[:len(t_fine)]\n            else:\n                lc_fine_lagged = None\n\n            indices = np.searchsorted(t_fine, time_grid, side=\"left\")\n            indices = np.clip(indices, 0, n_fine - 1)\n            for i, ti in enumerate(time_grid):\n                if indices[i] &gt; 0 and abs(t_fine[indices[i] - 1] - ti) &lt; abs(t_fine[indices[i]] - ti):\n                    indices[i] -= 1\n\n            lc = lc_fine[indices]\n            if lc_fine_lagged is not None:\n                lc_lagged = lc_fine_lagged[indices]\n                return lc, lc_lagged\n            else:\n                return lc\n\n    def add_poisson_noise(self, lc, time_grid, bkg_rate=0.0, exposure_times=None, min_error_floor=1e-10):\n        \"\"\"\n        Add Poisson noise to a simulated light curve.\n\n        Parameters\n        ----------\n        lc : ndarray\n            Clean light curve values.\n        time_grid : ndarray\n            Time values.\n        bkg_rate : float, optional\n            Background count rate.\n        exposure_times : ndarray or None, optional\n            Exposure duration for each point. If None, use time spacing\n            to approximate integration time per bin.\n        min_error_floor : float, optional\n            Minimum uncertainty to avoid zeros.\n\n        Returns\n        -------\n        noisy_lc : ndarray\n            Noisy light curve.\n        noise_estimate : ndarray\n            Estimated error bars.\n        \"\"\"\n\n        lc = np.asarray(lc)\n        time_grid = np.asarray(time_grid)\n\n        if len(time_grid) &lt; 2:\n            raise ValueError(\"time_grid must have at least two points.\")\n\n        if exposure_times is not None:\n            dt = np.asarray(exposure_times)\n        else:\n            dt_array = np.diff(time_grid)\n            if np.allclose(dt_array, dt_array[0], rtol=1e-5):\n                dt = dt_array[0]\n            else:\n                dt = np.zeros_like(time_grid)\n                dt[1:-1] = (time_grid[2:] - time_grid[:-2]) / 2\n                dt[0] = time_grid[1] - time_grid[0]\n                dt[-1] = time_grid[-1] - time_grid[-2]\n\n        counts = lc * dt\n        counts = np.clip(counts, 0, None)\n\n        noisy_counts = np.random.poisson(counts)\n\n        if bkg_rate &gt; 0:\n            bkg_counts1 = np.random.poisson(bkg_rate * dt)\n            bkg_counts2 = np.random.poisson(bkg_rate * dt)\n            noisy_counts += bkg_counts1\n            noisy_counts -= bkg_counts2\n\n        noisy_lc = noisy_counts / dt\n\n        with np.errstate(divide='ignore', invalid='ignore'):\n            noise_estimate = np.sqrt(np.clip(noisy_counts, 0, None)) / dt\n            noise_estimate = np.where(noise_estimate == 0, min_error_floor, noise_estimate)\n\n        return noisy_lc, noise_estimate\n\n    def add_gaussian_noise(self, lc, frac_err=0.05, min_error_floor=1e-10):\n        \"\"\"\n        Add Gaussian noise to a simulated light curve in flux units.\n\n        Parameters\n        ----------\n        lc : ndarray\n            Simulated light curve values (flux units).\n\n        frac_err : float\n            Fractional error (e.g., 0.05 = 5%).\n\n        min_error_floor : float\n            Minimum allowed error value to prevent zeros.\n\n        Returns\n        -------\n        noisy_lc : ndarray\n            Light curve with added Gaussian noise.\n\n        errors : ndarray\n            Standard deviation of the Gaussian noise at each point.\n        \"\"\"\n\n        lc = np.asarray(lc)\n        errors = np.clip(np.abs(lc) * frac_err, min_error_floor, None)\n        noisy_lc = lc + np.random.normal(0, errors)\n        return noisy_lc, errors\n\n\n    def add_regular_gaps(self, lc, time_grid, gap_period, gap_duration):\n        \"\"\"\n        Simulate regular gaps in the light curve.\n\n        Parameters\n        ----------\n        lc : ndarray\n            Input light curve values.\n        time_grid : ndarray\n            Time values.\n        gap_period : float\n            Period between gaps.\n        gap_duration : float\n            Duration of each gap.\n\n        Returns\n        -------\n        gapped_lc : ndarray\n            Light curve with NaNs inserted for gaps.\n        \"\"\"\n\n        lc = np.asarray(lc)\n        time_grid = np.asarray(time_grid)\n        gapped_lc = lc.copy()\n\n        time_since_start = (time_grid - time_grid[0]) % gap_period\n        in_gap = time_since_start &lt; gap_duration\n        gapped_lc[in_gap] = np.nan\n\n        return gapped_lc\n\n    def create_psd(self, freq):\n        \"\"\"\n        Construct the PSD array based on the selected type and parameters.\n\n        Parameters\n        ----------\n        freq : ndarray\n            Frequency array.\n\n        Returns\n        -------\n        psd : ndarray\n            Power spectral density values.\n        \"\"\"\n\n        freq = np.abs(freq)\n        psd = np.zeros_like(freq)\n        nonzero_mask = freq &gt; 0  # avoid division by 0\n        plnorm = self.psd_params.get(\"plnorm\", 1.0)\n\n        if self.psd_type == \"powerlaw\":\n            slope = self.psd_params.get(\"slope\")\n            psd[nonzero_mask] = plnorm * (2 * np.pi * freq[nonzero_mask]) ** (-slope / 2)\n\n        elif self.psd_type == \"broken_powerlaw\":\n            slope1 = self.psd_params.get(\"slope1\")\n            f_break = self.psd_params.get(\"f_break\")\n            slope2 = self.psd_params.get(\"slope2\")\n            psd[nonzero_mask] = np.where(\n                freq[nonzero_mask] &lt;= f_break,\n                plnorm * (2 * np.pi * freq[nonzero_mask]) ** (-slope1 / 2),\n                plnorm * ((2 * np.pi * f_break) ** ((slope2 - slope1) / 2)) *\n                (2 * np.pi * freq[nonzero_mask]) ** (-slope2 / 2)\n            )\n        else:\n            raise ValueError(f\"Unsupported PSD type: {self.psd_type}\")\n\n        # set freq=0 psd to 0 to avoid infinite psd\n        # the value of this will be adjusted by the mean during rescaling\n        psd[freq==0] = 0\n        return psd\n\n    def plot(self):\n        \"\"\"\n        Plot the simulated light curve/s. Shows both the original and lagged data (if available).\n        \"\"\"\n        plt.figure(figsize=(8, 4.5))\n\n        # Main simlc\n        if hasattr(self.simlc, \"errors\") and np.any(self.simlc.errors &gt; 0):\n            plt.errorbar(self.simlc.times, self.simlc.rates, yerr=self.simlc.errors,\n                        fmt='o', label='Simulated', lw=1.5, capsize=1)\n        else:\n            plt.plot(self.simlc.times, self.simlc.rates, label='Simulated', lw=1.5)\n\n        # Lagged simlc\n        if self.simlc_lagged is not None:\n            if hasattr(self.simlc_lagged, \"errors\") and np.any(self.simlc_lagged.errors &gt; 0):\n                plt.errorbar(self.simlc_lagged.times, self.simlc_lagged.rates,\n                            yerr=self.simlc_lagged.errors,\n                            fmt='o', label='Lagged', lw=1.5, capsize=1, alpha=0.8)\n            else:\n                plt.plot(self.simlc_lagged.times, self.simlc_lagged.rates,\n                        label='Lagged', lw=1.5, alpha=0.8)\n\n        plt.xlabel(\"Time\")\n        plt.ylabel(\"Flux\")\n        plt.grid(True)\n        plt.legend()\n        plt.show()\n\n    def _simulate_on_grid(self, t_sim):\n        \"\"\"\n        Generate a Fourier-based light curve realization on a time grid.\n        \"\"\"\n        n_sim = len(t_sim)\n        dt = t_sim[1] - t_sim[0]\n        freqs = np.fft.fftfreq(n_sim, d=dt)\n\n        psd = self.create_psd(freqs)\n        phases = np.random.uniform(0, 2*np.pi, n_sim)\n        amplitudes = np.sqrt(psd)\n\n        ft = amplitudes * np.exp(1j * phases)\n\n        # check for hermitian symmetry to ensure real-valued IFFT\n        if n_sim % 2 == 0:\n            ft[int(n_sim / 2) + 1:] = np.conj(ft[1:int(n_sim / 2)][::-1])\n        else:\n            ft[int(n_sim / 2) + 1:] = np.conj(ft[1:int(n_sim / 2) + 1][::-1])\n\n        lc_sim = np.fft.ifft(ft).real\n        return lc_sim\n\n    def _build_impulse_response(self, times):\n        \"\"\"\n        Generate an impulse response function for lag injection.\n        Supported types: 'delta', 'normal', 'lognormal', 'manual'.\n        \"\"\"\n        dt = times[1] - times[0]\n\n        if self.response_type == \"delta\":\n            lag = self.response_params[\"lag\"]\n            lag_n = int(round(lag / dt))\n            response = unit_impulse(len(times), lag_n)\n            return response\n\n        elif self.response_type == \"normal\":\n            mu = self.response_params[\"mean\"]\n            sigma = self.response_params[\"sigma\"]\n            duration = self.response_params.get(\"duration\", 5 * sigma)\n            t = np.arange(0, duration, dt)\n            kernel = norm.pdf(t, loc=mu, scale=sigma)\n            return kernel / np.sum(kernel)\n\n        elif self.response_type == \"lognormal\":\n            median = self.response_params[\"median\"]\n            sigma = self.response_params[\"sigma\"]\n            duration = self.response_params.get(\"duration\", 5 * median)\n            t = np.arange(dt, duration, dt)  # starts at dt to avoid log(0)\n\n            # Convert median + sigma to shape and scale for lognorm\n            # lognorm.pdf(x, s, loc=0, scale=median)\n            s = sigma\n            scale = median\n            kernel = lognorm.pdf(t, s=s, scale=scale)\n            return kernel / np.sum(kernel)\n\n        elif self.response_type == \"manual\":\n            return np.asarray(self.response_params[\"response\"])\n\n        else:\n            raise ValueError(f\"Unsupported response_type: {self.response_type}\")\n</code></pre>"},{"location":"reference/data_simulator/#stela_toolkit.data_simulator.SimulateLightCurve.add_gaussian_noise","title":"<code>add_gaussian_noise(lc, frac_err=0.05, min_error_floor=1e-10)</code>","text":"<p>Add Gaussian noise to a simulated light curve in flux units.</p>"},{"location":"reference/data_simulator/#stela_toolkit.data_simulator.SimulateLightCurve.add_gaussian_noise--parameters","title":"Parameters","text":"<p>lc : ndarray     Simulated light curve values (flux units).</p> float <p>Fractional error (e.g., 0.05 = 5%).</p> float <p>Minimum allowed error value to prevent zeros.</p>"},{"location":"reference/data_simulator/#stela_toolkit.data_simulator.SimulateLightCurve.add_gaussian_noise--returns","title":"Returns","text":"<p>noisy_lc : ndarray     Light curve with added Gaussian noise.</p> ndarray <p>Standard deviation of the Gaussian noise at each point.</p> Source code in <code>stela_toolkit/data_simulator.py</code> <pre><code>def add_gaussian_noise(self, lc, frac_err=0.05, min_error_floor=1e-10):\n    \"\"\"\n    Add Gaussian noise to a simulated light curve in flux units.\n\n    Parameters\n    ----------\n    lc : ndarray\n        Simulated light curve values (flux units).\n\n    frac_err : float\n        Fractional error (e.g., 0.05 = 5%).\n\n    min_error_floor : float\n        Minimum allowed error value to prevent zeros.\n\n    Returns\n    -------\n    noisy_lc : ndarray\n        Light curve with added Gaussian noise.\n\n    errors : ndarray\n        Standard deviation of the Gaussian noise at each point.\n    \"\"\"\n\n    lc = np.asarray(lc)\n    errors = np.clip(np.abs(lc) * frac_err, min_error_floor, None)\n    noisy_lc = lc + np.random.normal(0, errors)\n    return noisy_lc, errors\n</code></pre>"},{"location":"reference/data_simulator/#stela_toolkit.data_simulator.SimulateLightCurve.add_poisson_noise","title":"<code>add_poisson_noise(lc, time_grid, bkg_rate=0.0, exposure_times=None, min_error_floor=1e-10)</code>","text":"<p>Add Poisson noise to a simulated light curve.</p>"},{"location":"reference/data_simulator/#stela_toolkit.data_simulator.SimulateLightCurve.add_poisson_noise--parameters","title":"Parameters","text":"<p>lc : ndarray     Clean light curve values. time_grid : ndarray     Time values. bkg_rate : float, optional     Background count rate. exposure_times : ndarray or None, optional     Exposure duration for each point. If None, use time spacing     to approximate integration time per bin. min_error_floor : float, optional     Minimum uncertainty to avoid zeros.</p>"},{"location":"reference/data_simulator/#stela_toolkit.data_simulator.SimulateLightCurve.add_poisson_noise--returns","title":"Returns","text":"<p>noisy_lc : ndarray     Noisy light curve. noise_estimate : ndarray     Estimated error bars.</p> Source code in <code>stela_toolkit/data_simulator.py</code> <pre><code>def add_poisson_noise(self, lc, time_grid, bkg_rate=0.0, exposure_times=None, min_error_floor=1e-10):\n    \"\"\"\n    Add Poisson noise to a simulated light curve.\n\n    Parameters\n    ----------\n    lc : ndarray\n        Clean light curve values.\n    time_grid : ndarray\n        Time values.\n    bkg_rate : float, optional\n        Background count rate.\n    exposure_times : ndarray or None, optional\n        Exposure duration for each point. If None, use time spacing\n        to approximate integration time per bin.\n    min_error_floor : float, optional\n        Minimum uncertainty to avoid zeros.\n\n    Returns\n    -------\n    noisy_lc : ndarray\n        Noisy light curve.\n    noise_estimate : ndarray\n        Estimated error bars.\n    \"\"\"\n\n    lc = np.asarray(lc)\n    time_grid = np.asarray(time_grid)\n\n    if len(time_grid) &lt; 2:\n        raise ValueError(\"time_grid must have at least two points.\")\n\n    if exposure_times is not None:\n        dt = np.asarray(exposure_times)\n    else:\n        dt_array = np.diff(time_grid)\n        if np.allclose(dt_array, dt_array[0], rtol=1e-5):\n            dt = dt_array[0]\n        else:\n            dt = np.zeros_like(time_grid)\n            dt[1:-1] = (time_grid[2:] - time_grid[:-2]) / 2\n            dt[0] = time_grid[1] - time_grid[0]\n            dt[-1] = time_grid[-1] - time_grid[-2]\n\n    counts = lc * dt\n    counts = np.clip(counts, 0, None)\n\n    noisy_counts = np.random.poisson(counts)\n\n    if bkg_rate &gt; 0:\n        bkg_counts1 = np.random.poisson(bkg_rate * dt)\n        bkg_counts2 = np.random.poisson(bkg_rate * dt)\n        noisy_counts += bkg_counts1\n        noisy_counts -= bkg_counts2\n\n    noisy_lc = noisy_counts / dt\n\n    with np.errstate(divide='ignore', invalid='ignore'):\n        noise_estimate = np.sqrt(np.clip(noisy_counts, 0, None)) / dt\n        noise_estimate = np.where(noise_estimate == 0, min_error_floor, noise_estimate)\n\n    return noisy_lc, noise_estimate\n</code></pre>"},{"location":"reference/data_simulator/#stela_toolkit.data_simulator.SimulateLightCurve.add_regular_gaps","title":"<code>add_regular_gaps(lc, time_grid, gap_period, gap_duration)</code>","text":"<p>Simulate regular gaps in the light curve.</p>"},{"location":"reference/data_simulator/#stela_toolkit.data_simulator.SimulateLightCurve.add_regular_gaps--parameters","title":"Parameters","text":"<p>lc : ndarray     Input light curve values. time_grid : ndarray     Time values. gap_period : float     Period between gaps. gap_duration : float     Duration of each gap.</p>"},{"location":"reference/data_simulator/#stela_toolkit.data_simulator.SimulateLightCurve.add_regular_gaps--returns","title":"Returns","text":"<p>gapped_lc : ndarray     Light curve with NaNs inserted for gaps.</p> Source code in <code>stela_toolkit/data_simulator.py</code> <pre><code>def add_regular_gaps(self, lc, time_grid, gap_period, gap_duration):\n    \"\"\"\n    Simulate regular gaps in the light curve.\n\n    Parameters\n    ----------\n    lc : ndarray\n        Input light curve values.\n    time_grid : ndarray\n        Time values.\n    gap_period : float\n        Period between gaps.\n    gap_duration : float\n        Duration of each gap.\n\n    Returns\n    -------\n    gapped_lc : ndarray\n        Light curve with NaNs inserted for gaps.\n    \"\"\"\n\n    lc = np.asarray(lc)\n    time_grid = np.asarray(time_grid)\n    gapped_lc = lc.copy()\n\n    time_since_start = (time_grid - time_grid[0]) % gap_period\n    in_gap = time_since_start &lt; gap_duration\n    gapped_lc[in_gap] = np.nan\n\n    return gapped_lc\n</code></pre>"},{"location":"reference/data_simulator/#stela_toolkit.data_simulator.SimulateLightCurve.create_psd","title":"<code>create_psd(freq)</code>","text":"<p>Construct the PSD array based on the selected type and parameters.</p>"},{"location":"reference/data_simulator/#stela_toolkit.data_simulator.SimulateLightCurve.create_psd--parameters","title":"Parameters","text":"<p>freq : ndarray     Frequency array.</p>"},{"location":"reference/data_simulator/#stela_toolkit.data_simulator.SimulateLightCurve.create_psd--returns","title":"Returns","text":"<p>psd : ndarray     Power spectral density values.</p> Source code in <code>stela_toolkit/data_simulator.py</code> <pre><code>def create_psd(self, freq):\n    \"\"\"\n    Construct the PSD array based on the selected type and parameters.\n\n    Parameters\n    ----------\n    freq : ndarray\n        Frequency array.\n\n    Returns\n    -------\n    psd : ndarray\n        Power spectral density values.\n    \"\"\"\n\n    freq = np.abs(freq)\n    psd = np.zeros_like(freq)\n    nonzero_mask = freq &gt; 0  # avoid division by 0\n    plnorm = self.psd_params.get(\"plnorm\", 1.0)\n\n    if self.psd_type == \"powerlaw\":\n        slope = self.psd_params.get(\"slope\")\n        psd[nonzero_mask] = plnorm * (2 * np.pi * freq[nonzero_mask]) ** (-slope / 2)\n\n    elif self.psd_type == \"broken_powerlaw\":\n        slope1 = self.psd_params.get(\"slope1\")\n        f_break = self.psd_params.get(\"f_break\")\n        slope2 = self.psd_params.get(\"slope2\")\n        psd[nonzero_mask] = np.where(\n            freq[nonzero_mask] &lt;= f_break,\n            plnorm * (2 * np.pi * freq[nonzero_mask]) ** (-slope1 / 2),\n            plnorm * ((2 * np.pi * f_break) ** ((slope2 - slope1) / 2)) *\n            (2 * np.pi * freq[nonzero_mask]) ** (-slope2 / 2)\n        )\n    else:\n        raise ValueError(f\"Unsupported PSD type: {self.psd_type}\")\n\n    # set freq=0 psd to 0 to avoid infinite psd\n    # the value of this will be adjusted by the mean during rescaling\n    psd[freq==0] = 0\n    return psd\n</code></pre>"},{"location":"reference/data_simulator/#stela_toolkit.data_simulator.SimulateLightCurve.generate","title":"<code>generate(time_grid)</code>","text":"<p>Generate the clean (noise-free) light curve.</p> <p>Handles regular vs. irregular time grids and applies normalization. If lag injection is enabled, convolves with a response function.</p>"},{"location":"reference/data_simulator/#stela_toolkit.data_simulator.SimulateLightCurve.generate--parameters","title":"Parameters","text":"<p>time_grid : array-like     Desired output time grid.</p>"},{"location":"reference/data_simulator/#stela_toolkit.data_simulator.SimulateLightCurve.generate--returns","title":"Returns","text":"<p>rates : ndarray     Simulated light curve values. rates_lagged : ndarray or None     Lagged version of the light curve if <code>inject_lag</code> is True.</p> Source code in <code>stela_toolkit/data_simulator.py</code> <pre><code>def generate(self, time_grid):\n    \"\"\"\n    Generate the clean (noise-free) light curve.\n\n    Handles regular vs. irregular time grids and applies normalization.\n    If lag injection is enabled, convolves with a response function.\n\n    Parameters\n    ----------\n    time_grid : array-like\n        Desired output time grid.\n\n    Returns\n    -------\n    rates : ndarray\n        Simulated light curve values.\n    rates_lagged : ndarray or None\n        Lagged version of the light curve if `inject_lag` is True.\n    \"\"\"\n\n    time_grid = np.array(time_grid)\n    n_target = len(time_grid)\n    dt_array = np.diff(time_grid)\n    is_regular = np.allclose(dt_array, dt_array[0], rtol=1e-5)\n\n    if is_regular:\n        n_sim = int(self.oversample * n_target)\n        t_sim = np.linspace(time_grid[0], time_grid[-1], n_sim)\n        lc_sim = self._simulate_on_grid(t_sim)\n\n        start = (n_sim - n_target) // 2\n        end = start + n_target\n        lc = lc_sim[start:end]\n\n        lc -= np.mean(lc)\n        lc /= np.std(lc)\n        lc = lc * self.std + self.mean\n\n        if self.inject_lag:\n            kernel = self._build_impulse_response(time_grid)\n            convolved_full = fftconvolve(lc_sim, kernel, mode=\"full\")\n            convolved = convolved_full[start:end]\n\n            convolved -= np.mean(convolved)\n            convolved /= np.std(convolved)\n            convolved = convolved * self.std + self.mean\n            return lc, convolved\n        else:\n            return lc\n\n    else:\n        n_fine = int(self.fine_factor * len(time_grid))\n        t_fine = np.linspace(time_grid.min(), time_grid.max(), n_fine)\n        lc_fine = self._simulate_on_grid(t_fine)\n\n        lc_fine -= np.mean(lc_fine)\n        lc_fine /= np.std(lc_fine)\n        lc_fine = lc_fine * self.std + self.mean\n\n        if self.inject_lag:\n            kernel = self._build_impulse_response(t_fine)\n            lc_fine_lagged_full = fftconvolve(lc_fine, kernel, mode=\"full\")\n            lc_fine_lagged = lc_fine_lagged_full[:len(t_fine)]\n        else:\n            lc_fine_lagged = None\n\n        indices = np.searchsorted(t_fine, time_grid, side=\"left\")\n        indices = np.clip(indices, 0, n_fine - 1)\n        for i, ti in enumerate(time_grid):\n            if indices[i] &gt; 0 and abs(t_fine[indices[i] - 1] - ti) &lt; abs(t_fine[indices[i]] - ti):\n                indices[i] -= 1\n\n        lc = lc_fine[indices]\n        if lc_fine_lagged is not None:\n            lc_lagged = lc_fine_lagged[indices]\n            return lc, lc_lagged\n        else:\n            return lc\n</code></pre>"},{"location":"reference/data_simulator/#stela_toolkit.data_simulator.SimulateLightCurve.plot","title":"<code>plot()</code>","text":"<p>Plot the simulated light curve/s. Shows both the original and lagged data (if available).</p> Source code in <code>stela_toolkit/data_simulator.py</code> <pre><code>def plot(self):\n    \"\"\"\n    Plot the simulated light curve/s. Shows both the original and lagged data (if available).\n    \"\"\"\n    plt.figure(figsize=(8, 4.5))\n\n    # Main simlc\n    if hasattr(self.simlc, \"errors\") and np.any(self.simlc.errors &gt; 0):\n        plt.errorbar(self.simlc.times, self.simlc.rates, yerr=self.simlc.errors,\n                    fmt='o', label='Simulated', lw=1.5, capsize=1)\n    else:\n        plt.plot(self.simlc.times, self.simlc.rates, label='Simulated', lw=1.5)\n\n    # Lagged simlc\n    if self.simlc_lagged is not None:\n        if hasattr(self.simlc_lagged, \"errors\") and np.any(self.simlc_lagged.errors &gt; 0):\n            plt.errorbar(self.simlc_lagged.times, self.simlc_lagged.rates,\n                        yerr=self.simlc_lagged.errors,\n                        fmt='o', label='Lagged', lw=1.5, capsize=1, alpha=0.8)\n        else:\n            plt.plot(self.simlc_lagged.times, self.simlc_lagged.rates,\n                    label='Lagged', lw=1.5, alpha=0.8)\n\n    plt.xlabel(\"Time\")\n    plt.ylabel(\"Flux\")\n    plt.grid(True)\n    plt.legend()\n    plt.show()\n</code></pre>"},{"location":"reference/frequency_binning/","title":"frequency_binning","text":""},{"location":"reference/frequency_binning/#stela_toolkit.frequency_binning.FrequencyBinning","title":"<code>FrequencyBinning</code>","text":"<p>A utility class for binning data over frequency space.</p> <p>Provides methods for defining bins (linear or logarithmic, or user-defined), binning frequency data and corresponding values, and calculating statistics for binned data.</p> Source code in <code>stela_toolkit/frequency_binning.py</code> <pre><code>class FrequencyBinning:\n    \"\"\"\n    A utility class for binning data over frequency space.\n\n    Provides methods for defining bins (linear or logarithmic, or user-defined), binning frequency\n    data and corresponding values, and calculating statistics for binned data.\n    \"\"\"\n    # To do: Modify count_frequencies_in_bins for already binned data\n\n    @staticmethod\n    def define_bins(fmin, fmax, num_bins=None, bin_type=\"log\", bin_edges=[]):\n        \"\"\"\n        Defines bin edges for the given frequencies based on the specified binning type.\n\n        If custom bins are provided, they are used directly. Otherwise, bins are computed\n        either logarithmically or linearly based on the specified bin type.\n\n        Parameters:\n        - freqs (array-like): Array of frequencies to define bins for.\n        - num_bins (int): Number of bins to create (ignored if `bins` is provided).\n        - bins (array-like): Custom array of bin edges (optional).\n        - bin_type (str): Type of binning (\"log\" for logarithmic, \"linear\" for linear).\n\n        Returns:\n        - bin_edges (array-like): Array of bin edges for the specified binning type.\n        \"\"\"\n\n        if len(bin_edges) &gt; 0:\n            # Use custom bins\n            bin_edges = np.array(bin_edges)\n        else:\n\n            if bin_type == \"log\":\n                # Define logarithmic bins\n                bin_edges = np.logspace(np.log10(fmin), np.log10(fmax), num_bins + 1)\n\n            elif bin_type == \"linear\":\n                # Define linear bins\n                bin_edges = np.linspace(fmin, fmax, num_bins + 1)\n\n            else:\n                raise ValueError(\n                    f\"Unsupported bin_type '{bin_type}'. Choose 'log', 'linear', or provide custom bins.\")\n\n        return bin_edges\n\n    @staticmethod\n    def bin_data(freqs, values, bin_edges):\n        \"\"\"\n        Bins frequencies and corresponding values into specified bins.\n\n        Parameters:\n        - freqs (array-like): Array of frequencies to be binned.\n        - values (array-like): Array of values corresponding to the frequencies.\n        - bin_edges (array-like): Array of bin edges defining the bins.\n\n        Returns:\n        - binned_freqs (array-like): Mean frequency for each bin.\n        - binned_freq_widths (array-like): Half-widths of the frequency bins (for error bars).\n        - binned_values (array-like): Mean value for each bin.\n        - binned_value_errors (array-like): Standard deviation of the values in each bin.\n        \"\"\"\n        binned_freqs = []\n        binned_freq_widths = []\n        binned_values = []\n        binned_value_errors = []\n\n        for i in range(len(bin_edges) - 1):\n            mask = (freqs &gt;= bin_edges[i]) &amp; (freqs &lt; bin_edges[i + 1])\n            num_freqs = np.sum(mask)\n\n            if mask.any():\n                lower_bound = bin_edges[i]\n                upper_bound = bin_edges[i + 1]\n                bin_cent = (upper_bound + lower_bound) / 2\n\n                binned_freqs.append(bin_cent)\n                binned_freq_widths.append(bin_cent - lower_bound)\n                binned_values.append(\n                    np.mean(values[mask])\n                )\n                binned_value_errors.append(\n                    np.std(values[mask]) / np.sqrt(num_freqs)\n                )\n\n        return (\n            np.array(binned_freqs),\n            np.array(binned_freq_widths),\n            np.array(binned_values),\n            np.array(binned_value_errors),\n        )\n\n    @staticmethod\n    def count_frequencies_in_bins(spectrum, fmin=None, fmax=None, num_bins=None, bin_type=None, bin_edges=[]):\n        \"\"\"\n        Counts the number of frequencies in each bin for the power spectrum.\n\n        Parameters:\n        - spectrum: The object containing attributes like `times`, `fmin`, and `fmax`.\n        - fmin: Minimum frequency (optional, falls back to spectrum's attribute).\n        - fmax: Maximum frequency (optional, falls back to spectrum's attribute).\n        - num_bins: Number of bins to create (if bin_edges is not provided).\n        - bin_type: Type of binning (\"log\" or \"linear\").\n        - bin_edges: Custom array of bin edges (optional).\n\n        Returns:\n        - bin_counts: List of counts of frequencies in each bin.\n        \"\"\"\n        # Use spectrum's attributes if not provided\n        fmin = spectrum.fmin if fmin is None else fmin\n        fmax = spectrum.fmax if fmax is None else fmax\n        num_bins = spectrum.num_bins if num_bins is None else num_bins\n        bin_type = spectrum.bin_type if bin_type is None else bin_type\n        bin_edges = spectrum.bin_edges if bin_edges is None else bin_edges\n\n        # Define time array from input class\n        if hasattr(spectrum, 'times'):  \n            times = spectrum.times\n        elif hasattr(spectrum, 'times1'):\n            times = spectrum.times1\n        else:\n            raise AttributeError('Input class object for frequency binning does not have a time array properly defined.')\n\n        length = len(times) \n        dt = np.diff(times)[0]\n        freqs = np.fft.rfftfreq(length, d=dt)\n        freq_mask = (freqs &gt;= fmin) &amp; (freqs &lt;= fmax)\n        freqs = freqs[freq_mask]\n\n        # if neither num_bins nor bin_edges have been provided, no binning\n        if not any([num_bins, bin_edges]):\n            return np.ones(len(freqs))\n\n        # Check if bin_edges or num_bins provided\n        if len(bin_edges) == 0 and fmin and fmax and num_bins:\n            bin_edges = FrequencyBinning.define_bins(fmin, fmax, num_bins=num_bins, \n                                                     bin_type=bin_type, bin_edges=bin_edges\n                                                    )\n        elif len(bin_edges) &gt; 0:\n            bin_edges = np.array(bin_edges)\n        else:\n            raise ValueError(\n                \"Frequency binning requires either 1) defined bin edges, 2) num_bins + fmin + fmax., \\\n                3) all defined as none to leave products unbinned.\"\n            )\n\n        # Count frequencies in bins\n        bin_counts = np.histogram(freqs, bins=bin_edges)[0]\n        return bin_counts\n</code></pre>"},{"location":"reference/frequency_binning/#stela_toolkit.frequency_binning.FrequencyBinning.bin_data","title":"<code>bin_data(freqs, values, bin_edges)</code>  <code>staticmethod</code>","text":"<p>Bins frequencies and corresponding values into specified bins.</p> <p>Parameters: - freqs (array-like): Array of frequencies to be binned. - values (array-like): Array of values corresponding to the frequencies. - bin_edges (array-like): Array of bin edges defining the bins.</p> <p>Returns: - binned_freqs (array-like): Mean frequency for each bin. - binned_freq_widths (array-like): Half-widths of the frequency bins (for error bars). - binned_values (array-like): Mean value for each bin. - binned_value_errors (array-like): Standard deviation of the values in each bin.</p> Source code in <code>stela_toolkit/frequency_binning.py</code> <pre><code>@staticmethod\ndef bin_data(freqs, values, bin_edges):\n    \"\"\"\n    Bins frequencies and corresponding values into specified bins.\n\n    Parameters:\n    - freqs (array-like): Array of frequencies to be binned.\n    - values (array-like): Array of values corresponding to the frequencies.\n    - bin_edges (array-like): Array of bin edges defining the bins.\n\n    Returns:\n    - binned_freqs (array-like): Mean frequency for each bin.\n    - binned_freq_widths (array-like): Half-widths of the frequency bins (for error bars).\n    - binned_values (array-like): Mean value for each bin.\n    - binned_value_errors (array-like): Standard deviation of the values in each bin.\n    \"\"\"\n    binned_freqs = []\n    binned_freq_widths = []\n    binned_values = []\n    binned_value_errors = []\n\n    for i in range(len(bin_edges) - 1):\n        mask = (freqs &gt;= bin_edges[i]) &amp; (freqs &lt; bin_edges[i + 1])\n        num_freqs = np.sum(mask)\n\n        if mask.any():\n            lower_bound = bin_edges[i]\n            upper_bound = bin_edges[i + 1]\n            bin_cent = (upper_bound + lower_bound) / 2\n\n            binned_freqs.append(bin_cent)\n            binned_freq_widths.append(bin_cent - lower_bound)\n            binned_values.append(\n                np.mean(values[mask])\n            )\n            binned_value_errors.append(\n                np.std(values[mask]) / np.sqrt(num_freqs)\n            )\n\n    return (\n        np.array(binned_freqs),\n        np.array(binned_freq_widths),\n        np.array(binned_values),\n        np.array(binned_value_errors),\n    )\n</code></pre>"},{"location":"reference/frequency_binning/#stela_toolkit.frequency_binning.FrequencyBinning.count_frequencies_in_bins","title":"<code>count_frequencies_in_bins(spectrum, fmin=None, fmax=None, num_bins=None, bin_type=None, bin_edges=[])</code>  <code>staticmethod</code>","text":"<p>Counts the number of frequencies in each bin for the power spectrum.</p> <p>Parameters: - spectrum: The object containing attributes like <code>times</code>, <code>fmin</code>, and <code>fmax</code>. - fmin: Minimum frequency (optional, falls back to spectrum's attribute). - fmax: Maximum frequency (optional, falls back to spectrum's attribute). - num_bins: Number of bins to create (if bin_edges is not provided). - bin_type: Type of binning (\"log\" or \"linear\"). - bin_edges: Custom array of bin edges (optional).</p> <p>Returns: - bin_counts: List of counts of frequencies in each bin.</p> Source code in <code>stela_toolkit/frequency_binning.py</code> <pre><code>@staticmethod\ndef count_frequencies_in_bins(spectrum, fmin=None, fmax=None, num_bins=None, bin_type=None, bin_edges=[]):\n    \"\"\"\n    Counts the number of frequencies in each bin for the power spectrum.\n\n    Parameters:\n    - spectrum: The object containing attributes like `times`, `fmin`, and `fmax`.\n    - fmin: Minimum frequency (optional, falls back to spectrum's attribute).\n    - fmax: Maximum frequency (optional, falls back to spectrum's attribute).\n    - num_bins: Number of bins to create (if bin_edges is not provided).\n    - bin_type: Type of binning (\"log\" or \"linear\").\n    - bin_edges: Custom array of bin edges (optional).\n\n    Returns:\n    - bin_counts: List of counts of frequencies in each bin.\n    \"\"\"\n    # Use spectrum's attributes if not provided\n    fmin = spectrum.fmin if fmin is None else fmin\n    fmax = spectrum.fmax if fmax is None else fmax\n    num_bins = spectrum.num_bins if num_bins is None else num_bins\n    bin_type = spectrum.bin_type if bin_type is None else bin_type\n    bin_edges = spectrum.bin_edges if bin_edges is None else bin_edges\n\n    # Define time array from input class\n    if hasattr(spectrum, 'times'):  \n        times = spectrum.times\n    elif hasattr(spectrum, 'times1'):\n        times = spectrum.times1\n    else:\n        raise AttributeError('Input class object for frequency binning does not have a time array properly defined.')\n\n    length = len(times) \n    dt = np.diff(times)[0]\n    freqs = np.fft.rfftfreq(length, d=dt)\n    freq_mask = (freqs &gt;= fmin) &amp; (freqs &lt;= fmax)\n    freqs = freqs[freq_mask]\n\n    # if neither num_bins nor bin_edges have been provided, no binning\n    if not any([num_bins, bin_edges]):\n        return np.ones(len(freqs))\n\n    # Check if bin_edges or num_bins provided\n    if len(bin_edges) == 0 and fmin and fmax and num_bins:\n        bin_edges = FrequencyBinning.define_bins(fmin, fmax, num_bins=num_bins, \n                                                 bin_type=bin_type, bin_edges=bin_edges\n                                                )\n    elif len(bin_edges) &gt; 0:\n        bin_edges = np.array(bin_edges)\n    else:\n        raise ValueError(\n            \"Frequency binning requires either 1) defined bin edges, 2) num_bins + fmin + fmax., \\\n            3) all defined as none to leave products unbinned.\"\n        )\n\n    # Count frequencies in bins\n    bin_counts = np.histogram(freqs, bins=bin_edges)[0]\n    return bin_counts\n</code></pre>"},{"location":"reference/frequency_binning/#stela_toolkit.frequency_binning.FrequencyBinning.define_bins","title":"<code>define_bins(fmin, fmax, num_bins=None, bin_type='log', bin_edges=[])</code>  <code>staticmethod</code>","text":"<p>Defines bin edges for the given frequencies based on the specified binning type.</p> <p>If custom bins are provided, they are used directly. Otherwise, bins are computed either logarithmically or linearly based on the specified bin type.</p> <p>Parameters: - freqs (array-like): Array of frequencies to define bins for. - num_bins (int): Number of bins to create (ignored if <code>bins</code> is provided). - bins (array-like): Custom array of bin edges (optional). - bin_type (str): Type of binning (\"log\" for logarithmic, \"linear\" for linear).</p> <p>Returns: - bin_edges (array-like): Array of bin edges for the specified binning type.</p> Source code in <code>stela_toolkit/frequency_binning.py</code> <pre><code>@staticmethod\ndef define_bins(fmin, fmax, num_bins=None, bin_type=\"log\", bin_edges=[]):\n    \"\"\"\n    Defines bin edges for the given frequencies based on the specified binning type.\n\n    If custom bins are provided, they are used directly. Otherwise, bins are computed\n    either logarithmically or linearly based on the specified bin type.\n\n    Parameters:\n    - freqs (array-like): Array of frequencies to define bins for.\n    - num_bins (int): Number of bins to create (ignored if `bins` is provided).\n    - bins (array-like): Custom array of bin edges (optional).\n    - bin_type (str): Type of binning (\"log\" for logarithmic, \"linear\" for linear).\n\n    Returns:\n    - bin_edges (array-like): Array of bin edges for the specified binning type.\n    \"\"\"\n\n    if len(bin_edges) &gt; 0:\n        # Use custom bins\n        bin_edges = np.array(bin_edges)\n    else:\n\n        if bin_type == \"log\":\n            # Define logarithmic bins\n            bin_edges = np.logspace(np.log10(fmin), np.log10(fmax), num_bins + 1)\n\n        elif bin_type == \"linear\":\n            # Define linear bins\n            bin_edges = np.linspace(fmin, fmax, num_bins + 1)\n\n        else:\n            raise ValueError(\n                f\"Unsupported bin_type '{bin_type}'. Choose 'log', 'linear', or provide custom bins.\")\n\n    return bin_edges\n</code></pre>"},{"location":"reference/gaussian_process/","title":"gaussian_process","text":""},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess","title":"<code>GaussianProcess</code>","text":"<p>Fit and sample from a Gaussian Process (GP) model for light curve data.</p> <p>This class allows you to model a light curve as a continuous, probabilistic function using a Gaussian Process. You provide a LightCurve object, and the model will fit a smooth function to the observed rates, incorporating measurement uncertainties and capturing the underlying variability with flexible kernel choices.</p> <p>By default, the model will try to make things easy for you: - If the flux distribution is not normally distributed, we can apply a Box-Cox transformation to make it more Gaussian. - The data is standardized (zero mean, unit variance) before training to improve numerical stability. - A white noise term can be added to account for extra variance not captured by measurement errors. - If you don\u2019t specify a kernel, the model will try several standard ones and pick the best using AIC.</p> <p>Once trained, the model allows you to generate samples from the posterior predictive distribution\u2014 these are realizations of what the light curve could look like, given the data and uncertainties. These samples are central to downstream STELA analyses like coherence, cross-spectrum, and lag measurements, which will automatically use the most recently generated GP samples if a model is passed in.</p> <p>If you haven\u2019t generated any samples yet, don\u2019t worry\u2014those modules will do it for you using default settings.</p> <p>Noise handling is flexible: - If your light curve has error bars, they\u2019re passed directly into the GP as a fixed noise model. - If not, you can still include a learned white noise term to capture unmodeled variability. - You can control whether the model uses just your errors, or also learns extra noise.</p> <p>Model training uses exact inference with GPyTorch and is done via gradient descent. You can control the number of iterations, the optimizer learning rate, and whether to plot the training progress.</p>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess--parameters","title":"Parameters","text":"<p>lightcurve : LightCurve     The input light curve to model. kernel_form : str or list, optional     Kernel type to use (e.g., 'Matern32', 'RBF', 'SpectralMixture, N'), or list of types for auto-selection.     If 'auto', we try several and choose the best using AIC. white_noise : bool, optional     Whether to include a white noise component in addition to measurement errors. enforce_normality : bool, optional     Whether to apply a Box-Cox transformation to make the flux distribution more Gaussian. run_training : bool, optional     Whether to train the GP model on initialization. plot_training : bool, optional     Whether to plot the training loss during optimization. num_iter : int, optional     Number of iterations for GP training. learn_rate : float, optional     Learning rate for the optimizer. sample_time_grid : array-like, optional     Time grid on which to draw posterior samples after training. num_samples : int, optional     Number of GP samples to draw from the posterior. verbose : bool, optional     Whether to print model selection, training progress, and sampling diagnostics.</p>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess--attributes","title":"Attributes","text":"<p>model : gpytorch.models.ExactGP     The trained GP model used for prediction and sampling. likelihood : gpytorch.likelihoods.Likelihood     The likelihood model used (e.g., Gaussian with or without fixed noise). train_times : torch.Tensor     Time points used for training the GP. train_rates : torch.Tensor     Rate values used for training. train_errors : torch.Tensor     Measurement uncertainties (empty if not provided). samples : ndarray     Posterior samples drawn after training (used by downstream STELA modules). pred_times : torch.Tensor     Time grid on which posterior samples were drawn. kernel_form : str     Name of the kernel used in the final trained model.</p> Source code in <code>stela_toolkit/gaussian_process.py</code> <pre><code>class GaussianProcess:\n    \"\"\"\n    Fit and sample from a Gaussian Process (GP) model for light curve data.\n\n    This class allows you to model a light curve as a continuous, probabilistic function using a Gaussian Process.\n    You provide a LightCurve object, and the model will fit a smooth function to the observed rates,\n    incorporating measurement uncertainties and capturing the underlying variability with flexible kernel choices.\n\n    By default, the model will try to make things easy for you:\n    - If the flux distribution is not normally distributed, we can apply a Box-Cox transformation to make it more Gaussian.\n    - The data is standardized (zero mean, unit variance) before training to improve numerical stability.\n    - A white noise term can be added to account for extra variance not captured by measurement errors.\n    - If you don\u2019t specify a kernel, the model will try several standard ones and pick the best using AIC.\n\n    Once trained, the model allows you to generate samples from the posterior predictive distribution\u2014\n    these are realizations of what the light curve *could* look like, given the data and uncertainties.\n    These samples are central to downstream STELA analyses like coherence, cross-spectrum, and lag measurements,\n    which will automatically use the most recently generated GP samples if a model is passed in.\n\n    If you haven\u2019t generated any samples yet, don\u2019t worry\u2014those modules will do it for you using default settings.\n\n    Noise handling is flexible:\n    - If your light curve has error bars, they\u2019re passed directly into the GP as a fixed noise model.\n    - If not, you can still include a learned white noise term to capture unmodeled variability.\n    - You can control whether the model uses just your errors, or also learns extra noise.\n\n    Model training uses exact inference with GPyTorch and is done via gradient descent.\n    You can control the number of iterations, the optimizer learning rate, and whether to plot the training progress.\n\n    Parameters\n    ----------\n    lightcurve : LightCurve\n        The input light curve to model.\n    kernel_form : str or list, optional\n        Kernel type to use (e.g., 'Matern32', 'RBF', 'SpectralMixture, N'), or list of types for auto-selection.\n        If 'auto', we try several and choose the best using AIC.\n    white_noise : bool, optional\n        Whether to include a white noise component in addition to measurement errors.\n    enforce_normality : bool, optional\n        Whether to apply a Box-Cox transformation to make the flux distribution more Gaussian.\n    run_training : bool, optional\n        Whether to train the GP model on initialization.\n    plot_training : bool, optional\n        Whether to plot the training loss during optimization.\n    num_iter : int, optional\n        Number of iterations for GP training.\n    learn_rate : float, optional\n        Learning rate for the optimizer.\n    sample_time_grid : array-like, optional\n        Time grid on which to draw posterior samples after training.\n    num_samples : int, optional\n        Number of GP samples to draw from the posterior.\n    verbose : bool, optional\n        Whether to print model selection, training progress, and sampling diagnostics.\n\n    Attributes\n    ----------\n    model : gpytorch.models.ExactGP\n        The trained GP model used for prediction and sampling.\n    likelihood : gpytorch.likelihoods.Likelihood\n        The likelihood model used (e.g., Gaussian with or without fixed noise).\n    train_times : torch.Tensor\n        Time points used for training the GP.\n    train_rates : torch.Tensor\n        Rate values used for training.\n    train_errors : torch.Tensor\n        Measurement uncertainties (empty if not provided).\n    samples : ndarray\n        Posterior samples drawn after training (used by downstream STELA modules).\n    pred_times : torch.Tensor\n        Time grid on which posterior samples were drawn.\n    kernel_form : str\n        Name of the kernel used in the final trained model.\n    \"\"\"\n\n    def __init__(self,\n                 lightcurve,\n                 kernel_form='auto',\n                 white_noise=True,\n                 enforce_normality=False,\n                 run_training=True,\n                 plot_training=False,\n                 num_iter=500,\n                 learn_rate=1e-1,\n                 sample_time_grid=[],\n                 num_samples=1000,\n                 verbose=False):\n\n        # To Do: reconsider noise prior, add a mean function function for forecasting, more verbose options\n        _CheckInputs._check_input_data(lightcurve, req_reg_samp=False)\n        self.lc = deepcopy(lightcurve)\n\n        # Save original mean, std, boxcox parameter for reversing standardization\n        self.lc_mean = getattr(self.lc, 'unstandard_mean', np.mean(self.lc.rates))\n        self.lc_std = getattr(self.lc, 'unstandard_std', np.std(self.lc.rates))\n        self.lambda_boxcox = getattr(self.lc, \"lambda_boxcox\", None)\n\n        # Check normality and apply boxcox if user specifies\n        if enforce_normality:\n            self.enforce_normality()\n\n        # Standardize data\n        if not getattr(self.lc, \"is_standard\", False):\n            Preprocessing.standardize(self.lc)\n\n        # Convert light curve data to pytorch tensors\n        self.train_times = torch.tensor(self.lc.times, dtype=torch.float32)\n        self.train_rates = torch.tensor(self.lc.rates, dtype=torch.float32)\n        if self.lc.errors.size &gt; 0:\n            self.train_errors = torch.tensor(self.lc.errors, dtype=torch.float32)\n        else:\n            self.train_errors = torch.tensor([])\n\n        # Training\n        self.white_noise = white_noise\n        if kernel_form == 'auto' or isinstance(kernel_form, list):\n            # Automatically select the best kernel based on AIC\n            if isinstance(kernel_form, list):\n                kernel_list = kernel_form\n            else:\n                kernel_list = ['Matern12', 'Matern32',\n                               'Matern52', 'RQ', 'RBF', 'SpectralMixture, 4']\n\n            best_model, best_likelihood = self.find_best_kernel(\n                kernel_list, num_iter=num_iter, learn_rate=learn_rate, verbose=verbose\n            )\n            self.model = best_model\n            self.likelihood = best_likelihood\n        else:\n            # Use specified kernel\n            self.likelihood = self.set_likelihood(self.white_noise, train_errors=self.train_errors)\n            self.model = self.create_gp_model(self.likelihood, kernel_form)\n\n            # Separate training needed only if kernel not automatically selected\n            if run_training:\n                self.train(num_iter=num_iter, learn_rate=learn_rate, plot=plot_training, verbose=verbose)\n\n        # Generate samples if sample_time_grid is provided\n        if sample_time_grid:\n            self.samples = self.sample(sample_time_grid, num_samples=num_samples)\n            if verbose:\n                print(f\"Samples generated: {self.samples.shape}, access with 'samples' attribute.\")\n\n        # Unstandardize the data\n        Preprocessing.unstandardize(self.lc)\n\n        # Undo boxcox transformation if needed\n        if getattr(self.lc, \"is_boxcox_transformed\", False):\n            Preprocessing.reverse_boxcox_transform(self.lc)\n\n    def enforce_normality(self):\n        \"\"\"\n        Check normality of the input data and apply a Box-Cox transformation if needed.\n\n        This method first checks if the light curve's flux distribution appears normal.\n        If not, a Box-Cox transformation is applied to improve it. STELA automatically\n        selects the most appropriate test (Shapiro-Wilk or Lilliefors) based on sample size.\n        \"\"\"\n        print(\"Checking normality of input light curve...\")\n\n        is_normal_before, pval_before = Preprocessing.check_normal(self.lc, plot=False, verbose=False)\n\n        if is_normal_before:\n            print(f\"\\n - Light curve appears normal (p = {pval_before:.4f}). No transformation applied.\")\n            return\n\n        print(f\"\\n - Light curve is not normal (p = {pval_before:.4f}). Applying Box-Cox transformation...\")\n\n        # Apply Box-Cox transformation\n        Preprocessing.boxcox_transform(self.lc)\n\n        if self.lambda_boxcox is not None:\n            print(\" -- Note: The input was already Box-Cox transformed. No additional transformation made.\")\n        else:\n            self.lambda_boxcox = getattr(self.lc, \"lambda_boxcox\", None)\n\n        # Re-check normality\n        is_normal_after, pval_after = Preprocessing.check_normal(self.lc, plot=False, verbose=False)\n\n        if is_normal_after:\n            print(f\" - Normality sufficiently achieved after Box-Cox (p = {pval_after:.4f})! Proceed as normal!\\n\")\n        else:\n            print(f\" - Data still not normal after Box-Cox (p = {pval_after:.4f}). Proceed with caution.\\n\")\n\n\n    def create_gp_model(self, likelihood, kernel_form):\n        \"\"\"\n        Build a GP model with the specified likelihood and kernel.\n\n        Parameters\n        ----------\n        likelihood : gpytorch.likelihoods.Likelihood\n            The likelihood model to use (e.g., Gaussian or FixedNoise).\n        kernel_form : str\n            The kernel type (e.g., 'Matern32', 'SpectralMixture, 4').\n\n        Returns\n        -------\n        GPModel\n            A subclass of gpytorch.models.ExactGP for training.\n        \"\"\"\n\n        class GPModel(gpytorch.models.ExactGP):\n            def __init__(gp_self, train_times, train_rates, likelihood):\n                super(GPModel, gp_self).__init__(train_times, train_rates, likelihood)\n                gp_self.mean_module = gpytorch.means.ZeroMean()\n                gp_self.covar_module = self.set_kernel(kernel_form)\n\n            def forward(gp_self, x):\n                mean_x = gp_self.mean_module(x)\n                covar_x = gp_self.covar_module(x)\n                return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\n        return GPModel(self.train_times, self.train_rates, likelihood)\n\n    def set_likelihood(self, white_noise, train_errors=torch.tensor([])):\n        \"\"\"\n        Set up the GP likelihood model based on user input and data characteristics.\n\n        If error bars are available, uses a FixedNoiseGaussianLikelihood. Otherwise, defaults to a\n        GaussianLikelihood with optional white noise. If white noise is enabled, the noise level\n        is initialized based on Poisson statistics or variance in the data.\n\n        Parameters\n        ----------\n        white_noise : bool\n            Whether to include a learnable noise term in the model.\n        train_errors : torch.Tensor, optional\n            Measurement errors from the light curve.\n\n        Returns\n        -------\n        likelihood : gpytorch.likelihoods.Likelihood\n            GPyTorch subclass, also used for training.\n        \"\"\"\n\n        if white_noise:\n            noise_constraint = gpytorch.constraints.Interval(1e-5, 1)\n        else:\n            noise_constraint = gpytorch.constraints.Interval(1e-40, 1e-39)\n\n        if train_errors.size(dim=0) &gt; 0:\n            likelihood = gpytorch.likelihoods.FixedNoiseGaussianLikelihood(\n                noise=self.train_errors ** 2,\n                learn_additional_noise=white_noise,\n                noise_constraint=noise_constraint\n            )\n\n        else:\n            likelihood = gpytorch.likelihoods.GaussianLikelihood(\n                noise_constraint=noise_constraint,\n            )\n\n            if white_noise:\n                counts = np.abs(self.train_rates[1:].numpy()) * np.diff(self.train_times.numpy())\n                # begin with a slight underestimation to prevent overfitting\n                norm_poisson_var = 1 / (2 * np.mean(counts))\n                likelihood.noise = norm_poisson_var\n\n        # initialize noise parameter at the variance of the data\n        return likelihood\n\n    def set_kernel(self, kernel_form):\n        \"\"\"\n        Set the GP kernel (covariance function) based on user input.\n\n        Handles spectral mixture, Matern, RBF, and other kernel types supported by GPyTorch.\n        Applies reasonable defaults for lengthscale initialization.\n\n        Parameters\n        ----------\n        kernel_form : str\n            Name of the kernel, or 'SpectralMixture, N' to set the number of components.\n\n        Returns\n        -------\n        covar_module : gpytorch.kernels.Kernel\n        \"\"\"\n\n        kernel_form = kernel_form.strip()\n        if 'SpectralMixture' in kernel_form:\n            if ',' not in kernel_form:\n                raise ValueError(\n                    \"Invalid Spectral Mixture kernel format (use 'SpectralMixture, N').\\n\"\n                    \"N=4 is a good starting point.\"\n                )\n            else:\n                kernel_form, num_mixtures_str = kernel_form.split(',')\n                num_mixtures = int(num_mixtures_str.strip())\n        else:\n            num_mixtures = 4  # set num_mixtures for kernel_mapping when Spectral Mixture kernel not used\n\n        kernel_mapping = {\n            'Matern12': gpytorch.kernels.MaternKernel(nu=0.5),\n            'Matern32': gpytorch.kernels.MaternKernel(nu=1.5),\n            'Matern52': gpytorch.kernels.MaternKernel(nu=2.5),\n            'RQ': gpytorch.kernels.RQKernel(),\n            'RBF': gpytorch.kernels.RBFKernel(),\n            'SpectralMixture': gpytorch.kernels.SpectralMixtureKernel(num_mixtures=num_mixtures),\n            'Periodic': gpytorch.kernels.PeriodicKernel()\n        }\n\n        # Assign kernel if type is valid\n        if kernel_form in kernel_mapping:\n            kernel = kernel_mapping[kernel_form]\n        else:\n            raise ValueError(\n                f\"Invalid kernel functional form '{kernel_form}'. Choose from {list(kernel_mapping.keys())}.\")\n\n        if kernel_form == 'SpectralMixture':\n            kernel.initialize_from_data(self.train_times, self.train_rates)\n        else:\n            init_lengthscale = (self.train_times[-1] - self.train_times[0]) / 10\n            kernel.lengthscale = init_lengthscale\n\n        # Scale the kernel by a constant\n        covar_module = gpytorch.kernels.ScaleKernel(kernel)\n        self.kernel_form = kernel_form\n\n        return covar_module\n\n    def train(self, num_iter=500, learn_rate=1e-1, plot=False, verbose=False):\n        \"\"\"\n        Train the GP model using the Adam optimizer to minimize the negative log marginal likelihood (NLML).\n\n        By default, prints progress periodically and optionally plots the NLML loss curve over training iterations.\n        This function is typically called after initialization unless `run_training=True` was set earlier.\n\n        Parameters\n        ----------\n        num_iter : int, optional\n            Number of optimization steps to perform. Default is 500.\n        learn_rate : float, optional\n            Learning rate for the Adam optimizer. Default is 0.1.\n        plot : bool, optional\n            If True, display a plot of the NLML loss as training progresses.\n        verbose : bool, optional\n            If True, print progress updates at regular intervals during training.\n        \"\"\"\n\n        self.model.train()\n        self.likelihood.train()\n\n        optimizer = torch.optim.Adam(self.model.parameters(), lr=learn_rate)\n        mll = gpytorch.mlls.ExactMarginalLogLikelihood(self.likelihood, self.model)\n\n        print_every = max(1, num_iter // 20)\n\n        if plot:\n            plt.figure(figsize=(8, 5))\n\n        for i in range(num_iter):\n            optimizer.zero_grad()\n            output = self.model(self.train_times)\n            loss = -mll(output, self.train_rates)\n            loss.backward()\n\n            if verbose and (i == num_iter - 1 or i % print_every == 0):\n                if self.white_noise:\n                    if self.train_errors.size(dim=0) &gt; 0:\n                        noise_param = self.model.likelihood.second_noise.item()\n                    else:\n                        noise_param = self.model.likelihood.noise.item()\n\n                if self.kernel_form == 'SpectralMixture':\n                    mixture_scales = self.model.covar_module.base_kernel.mixture_scales\n                    mixture_scales = mixture_scales.detach().numpy().flatten()\n                    mixture_weights = self.model.covar_module.base_kernel.mixture_weights\n                    mixture_weights = mixture_weights.detach().numpy().flatten()\n\n                    if self.white_noise:\n                        print('Iter %d/%d - loss: %.3f   mixture_lengthscales: %s   mixture_weights: %s   noise: %.1e' % (\n                            i + 1, num_iter, loss.item(),\n                            mixture_scales.round(3),\n                            mixture_weights.round(3),\n                            noise_param\n                        ))\n                    else:\n                        print('Iter %d/%d - loss: %.3f   mixture_lengthscales: %s   mixture_weights: %s' % (\n                            i + 1, num_iter, loss.item(),\n                            mixture_scales.round(3),\n                            mixture_weights.round(3)\n                        ))\n\n                elif self.kernel_form == 'Periodic':\n                    if self.white_noise:\n                        print('Iter %d/%d - loss: %.3f   period length: %.3f   lengthscale: %.3f   noise: %.1e' % (\n                            i + 1, num_iter, loss.item(),\n                            self.model.covar_module.base_kernel.period_length.item(),\n                            self.model.covar_module.base_kernel.lengthscale.item(),\n                            noise_param\n                        ))\n                    else:\n                        print('Iter %d/%d - loss: %.3f   lengthscale: %.1e' % (\n                            i + 1, num_iter, loss.item(),\n                            self.model.covar_module.base_kernel.lengthscale.item()\n                        ))\n\n                else:\n                    if self.white_noise:\n                        print('Iter %d/%d - loss: %.3f   lengthscale: %.3f   noise: %.1e' % (\n                            i + 1, num_iter, loss.item(),\n                            self.model.covar_module.base_kernel.lengthscale.item(),\n                            noise_param\n                        ))\n                    else:\n                        print('Iter %d/%d - loss: %.3f   lengthscale: %.1e' % (\n                            i + 1, num_iter, loss.item(),\n                            self.model.covar_module.base_kernel.lengthscale.item()\n                        ))\n\n            optimizer.step()\n\n            if plot:\n                plt.scatter(i, loss.item(), color='black', s=2)\n\n        if verbose:\n            final_hypers = self.get_hyperparameters()\n            print(\n                \"Training complete. \\n\"\n                f\"   - Final loss: {loss.item():0.5}\\n\"\n                f\"   - Final hyperparameters:\")\n            for key, value in final_hypers.items():\n                print(f\"      {key:42}: {np.round(value, 4)}\")\n\n        if plot:\n            plt.xlabel('Iteration')\n            plt.ylabel('Negative Marginal Log Likelihood')\n            plt.title('Training Progress')\n            plt.show()\n\n    def find_best_kernel(self, kernel_list, num_iter=500, learn_rate=1e-1, verbose=False):\n        \"\"\"\n        Search over a list of kernels and return the best one by AIC.\n\n        Trains the model separately with each kernel in the list, computes the AIC,\n        and returns the model with the lowest value.\n\n        Parameters\n        ----------\n        kernel_list : list of str\n            Kernel names to try.\n        num_iter : int\n            Number of iterations per training run.\n        learn_rate : float\n            Learning rate for the optimizer.\n        verbose : bool\n            Whether to print progress for each kernel.\n\n        Returns\n        -------\n        best_model : GPModel\n            The model trained with the best-performing kernel.\n        best_likelihood : gpytorch.likelihoods.Likelihood\n            Corresponding likelihood for the best model.\n        \"\"\"\n\n        aics = []\n        best_model = None\n        for kernel_form in kernel_list:\n            self.likelihood = self.set_likelihood(self.white_noise, train_errors=self.train_errors)\n            self.model = self.create_gp_model(self.likelihood, kernel_form)\n            # suppress output, even for verbose=True\n            self.train(num_iter=num_iter, learn_rate=learn_rate, verbose=False)\n\n            # compute aic and store best model\n            aic = self.aic()\n            aics.append(aic)\n            if aic &lt;= min(aics):\n                best_model = self.model\n                best_likelihood = self.likelihood\n\n        best_aic = min(aics)\n        best_kernel = kernel_list[aics.index(best_aic)]\n\n        if verbose:\n            kernel_results = zip(kernel_list, aics)\n            print(\n                \"Kernel selection complete.\\n\"\n                f\"   Kernel AICs (lower is better):\"\n            )\n            for kernel, aic in kernel_results:\n                print(f\"     - {kernel:15}: {aic:0.5}\")\n\n            print(f\"   Best kernel: {best_kernel} (AIC: {best_aic:0.5})\")\n\n        self.kernel_form = best_kernel\n        return best_model, best_likelihood\n\n    def get_hyperparameters(self):\n        \"\"\"\n        Return the learned GP hyperparameters (lengthscale, noise, weights, etc.).\n\n        Returns\n        -------\n        hyper_dict : dict\n            Dictionary mapping parameter names to their (transformed) values.\n                Note: All rate-associated hyperparameters (e.g., not lengthscale) \n                are in units of the standardized data, not the original flux/time units.\n        \"\"\"\n\n        raw_hypers = self.model.named_parameters()\n        hypers = {}\n        for param_name, param in raw_hypers:\n            # Split the parameter name into hierarchy\n            parts = param_name.split('.')\n            module = self.model\n\n            # Traverse structure of the model to get the constraint\n            for part in parts[:-1]:  # last part is parameter\n                module = getattr(module, part, None)\n                if module is None:\n                    raise AttributeError(\n                        f\"Module '{part}' not found while traversing '{param_name}'.\")\n\n            final_param_name = parts[-1]\n            constraint_name = f\"{final_param_name}_constraint\"\n            constraint = getattr(module, constraint_name, None)\n\n            if constraint is None:\n                raise AttributeError(\n                    f\"Constraint '{constraint_name}' not found in module '{module}'.\")\n\n            # Transform the parameter using the constraint\n            transform_param = constraint.transform(param)\n\n            # Remove 'raw_' prefix from the parameter name for readability\n            param_name_withoutraw = param_name.replace('raw_', '')\n\n            if self.kernel_form == 'SpectralMixture':\n                transform_param = transform_param.detach().numpy().flatten()\n            else:\n                transform_param = transform_param.item()\n\n            hypers[param_name_withoutraw] = transform_param\n\n        return hypers\n\n    def bic(self):\n        \"\"\"\n        Compute the Bayesian Information Criterion (BIC) for the trained model.\n\n        Returns\n        -------\n        bic : float\n            The BIC value (lower is better).\n        \"\"\"\n\n        mll = gpytorch.mlls.ExactMarginalLogLikelihood(self.likelihood, self.model)\n        log_marg_like = mll(\n            self.model(self.train_times), self.train_rates\n        ).item()\n\n        num_params = sum([p.numel() for p in self.model.parameters()])\n        num_data = len(self.train_times)\n\n        bic = -2 * log_marg_like + num_params * np.log(num_data)\n        return bic\n\n    def aic(self):\n        \"\"\"\n        Compute the Akaike Information Criterion (AIC) for the trained model.\n\n        Returns\n        -------\n        aic : float\n            The AIC value (lower is better).\n        \"\"\"\n\n        mll = gpytorch.mlls.ExactMarginalLogLikelihood(self.likelihood, self.model)\n        log_marg_like = mll(\n            self.model(self.train_times), self.train_rates\n        ).item()\n\n        num_params = sum([p.numel() for p in self.model.parameters()])\n\n        aic = -2 * log_marg_like + 2 * num_params\n        return aic\n\n    def sample(self, pred_times, num_samples, save_path=None, _save_to_state=True):\n        \"\"\"\n        Generate posterior samples from the trained GP model.\n\n        These samples represent plausible realizations of the light curve. These are what is used\n        by the coherence, power spectrum, and lag modules when a GP model is passed in.\n\n        Parameters\n        ----------\n        pred_times : array-like\n            Time points where samples should be drawn.\n        num_samples : int\n            Number of realizations to generate.\n        save_path : str, optional\n            File path to save the samples.\n        _save_to_state : bool, optional\n            Whether to store results in the object (used by other classes).\n\n        Returns\n        -------\n        samples : ndarray\n            Array of sampled light curves with shape (num_samples, len(pred_times)).\n        \"\"\"\n\n        pred_times_tensor = torch.tensor(pred_times, dtype=torch.float32)\n        self.model.eval()\n        self.likelihood.eval()\n\n        # Make predictions\n        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n            pred_dist = self.likelihood(self.model(pred_times_tensor))\n            post_samples = pred_dist.sample(sample_shape=torch.Size([num_samples]))\n\n        samples = post_samples.numpy()\n        samples = self._undo_transforms(samples)\n\n        if save_path:\n            samples_with_time = np.insert(pred_times, num_samples, 0)\n            file_ext = save_path.split(\".\")[-1]\n            if file_ext == \"npy\":\n                np.save(save_path, samples_with_time)\n            else:\n                np.savetxt(save_path, samples_with_time)\n\n        if _save_to_state:\n            self.pred_times = pred_times\n            self.samples = samples\n        return samples\n\n    def predict(self, pred_times):\n        \"\"\"\n        Compute the posterior mean and 2-sigma confidence intervals at specified times.\n\n        Parameters\n        ----------\n        pred_times : array-like\n            Time values to predict.\n\n        Returns\n        -------\n        mean, lower, upper : ndarray\n            Predicted mean and lower/upper bounds of the 95 percent confidence interval.\n        \"\"\"\n\n        # Check if pred_times is a torch tensor\n        if not isinstance(pred_times, torch.Tensor):\n            try:\n                pred_times = torch.tensor(pred_times, dtype=torch.float32)\n            except TypeError:\n                raise TypeError(\"pred_times must be a torch tensor or convertible to one.\")\n\n        self.model.eval()\n        self.likelihood.eval()\n\n        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n            pred_dist = self.likelihood(self.model(pred_times))\n            mean = pred_dist.mean\n            lower, upper = pred_dist.confidence_region()\n\n        # Unstandardize/unboxcox\n        mean = self._undo_transforms(mean)\n        lower = self._undo_transforms(lower)\n        upper = self._undo_transforms(upper)\n        return mean.numpy(), lower.numpy(), upper.numpy()\n\n    def plot(self, pred_times=None):\n        \"\"\"\n        Plot the GP fit including mean, confidence intervals, one posterior sample, and data.\n\n        Parameters\n        ----------\n        **kwargs : dict\n            Additional keyword arguments for plot customization.\n        \"\"\"\n\n        if pred_times is None:\n            step = (self.train_times.max() - self.train_times.min()) / 1000\n            pred_times = np.arange(self.train_times.min(), self.train_times.max() + step, step)\n\n        predict_mean, predict_lower, predict_upper = self.predict(pred_times)\n\n        plt.figure(figsize=(8, 4.5))\n\n        plt.fill_between(pred_times, predict_lower, predict_upper,\n                         color='dodgerblue', alpha=0.2, label=r'Prediction 2$\\sigma$ CI')\n        plt.plot(pred_times, predict_mean, color='dodgerblue', label='Prediction Mean')\n\n        sample = self.sample(pred_times, num_samples=1, _save_to_state=False)\n        plt.plot(pred_times, sample[0], color='orange', lw=1, label='Sample')\n\n        if self.train_errors.size(dim=0) &gt; 0:\n            plt.errorbar(self.lc.times, self.lc.rates, yerr=self.lc.errors,\n                         fmt='o', color='black', lw=1.5, ms=3)\n        else:\n            plt.scatter(self.lc.times, self.lc.rates, color='black', s=6)\n\n        plt.xlabel('Time', fontsize=12)\n        plt.ylabel('Rate', fontsize=12)\n        plt.legend()\n        plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n        plt.tick_params(which='both', direction='in', length=6, width=1,\n                        top=True, right=True, labelsize=12)\n        plt.show()\n\n    def save(self, file_path):\n        \"\"\"\n        Save the trained GP model to a file using pickle.\n\n        Parameters\n        ----------\n        file_path : str\n            Path to save the model.\n        \"\"\"\n\n        with open(file_path, \"wb\") as f:\n            pickle.dump(self, f)\n        print(f\"GaussianProcess instance saved to {file_path}.\")\n\n    @staticmethod\n    def load(file_path):\n        \"\"\"\n        Load a saved GaussianProcess model from file.\n\n        Parameters\n        ----------\n        file_path : str\n            Path to the saved file.\n\n        Returns\n        -------\n        GaussianProcess\n            Restored instance of the model.\n        \"\"\"\n\n        with open(file_path, \"rb\") as f:\n            instance = pickle.load(f)\n        print(f\"GaussianProcess instance loaded from {file_path}.\")\n        return instance\n\n    def _undo_transforms(self, array):\n        \"\"\"\n        Reverse Box-Cox and standardization transformations applied to GP outputs.\n\n        Parameters\n        ----------\n        array : ndarray\n            Input values in transformed space.\n\n        Returns\n        -------\n        array : ndarray\n            Values in original flux units.\n        \"\"\"\n\n        if self.lambda_boxcox is not None:\n            if self.lambda_boxcox == 0:\n                array = np.exp(array)\n            else:\n                array = (array * self.lambda_boxcox + 1) ** (1 / self.lambda_boxcox)\n\n        array = array * self.lc_std + self.lc_mean\n        return array\n</code></pre>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess.aic","title":"<code>aic()</code>","text":"<p>Compute the Akaike Information Criterion (AIC) for the trained model.</p>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess.aic--returns","title":"Returns","text":"<p>aic : float     The AIC value (lower is better).</p> Source code in <code>stela_toolkit/gaussian_process.py</code> <pre><code>def aic(self):\n    \"\"\"\n    Compute the Akaike Information Criterion (AIC) for the trained model.\n\n    Returns\n    -------\n    aic : float\n        The AIC value (lower is better).\n    \"\"\"\n\n    mll = gpytorch.mlls.ExactMarginalLogLikelihood(self.likelihood, self.model)\n    log_marg_like = mll(\n        self.model(self.train_times), self.train_rates\n    ).item()\n\n    num_params = sum([p.numel() for p in self.model.parameters()])\n\n    aic = -2 * log_marg_like + 2 * num_params\n    return aic\n</code></pre>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess.bic","title":"<code>bic()</code>","text":"<p>Compute the Bayesian Information Criterion (BIC) for the trained model.</p>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess.bic--returns","title":"Returns","text":"<p>bic : float     The BIC value (lower is better).</p> Source code in <code>stela_toolkit/gaussian_process.py</code> <pre><code>def bic(self):\n    \"\"\"\n    Compute the Bayesian Information Criterion (BIC) for the trained model.\n\n    Returns\n    -------\n    bic : float\n        The BIC value (lower is better).\n    \"\"\"\n\n    mll = gpytorch.mlls.ExactMarginalLogLikelihood(self.likelihood, self.model)\n    log_marg_like = mll(\n        self.model(self.train_times), self.train_rates\n    ).item()\n\n    num_params = sum([p.numel() for p in self.model.parameters()])\n    num_data = len(self.train_times)\n\n    bic = -2 * log_marg_like + num_params * np.log(num_data)\n    return bic\n</code></pre>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess.create_gp_model","title":"<code>create_gp_model(likelihood, kernel_form)</code>","text":"<p>Build a GP model with the specified likelihood and kernel.</p>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess.create_gp_model--parameters","title":"Parameters","text":"<p>likelihood : gpytorch.likelihoods.Likelihood     The likelihood model to use (e.g., Gaussian or FixedNoise). kernel_form : str     The kernel type (e.g., 'Matern32', 'SpectralMixture, 4').</p>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess.create_gp_model--returns","title":"Returns","text":"<p>GPModel     A subclass of gpytorch.models.ExactGP for training.</p> Source code in <code>stela_toolkit/gaussian_process.py</code> <pre><code>def create_gp_model(self, likelihood, kernel_form):\n    \"\"\"\n    Build a GP model with the specified likelihood and kernel.\n\n    Parameters\n    ----------\n    likelihood : gpytorch.likelihoods.Likelihood\n        The likelihood model to use (e.g., Gaussian or FixedNoise).\n    kernel_form : str\n        The kernel type (e.g., 'Matern32', 'SpectralMixture, 4').\n\n    Returns\n    -------\n    GPModel\n        A subclass of gpytorch.models.ExactGP for training.\n    \"\"\"\n\n    class GPModel(gpytorch.models.ExactGP):\n        def __init__(gp_self, train_times, train_rates, likelihood):\n            super(GPModel, gp_self).__init__(train_times, train_rates, likelihood)\n            gp_self.mean_module = gpytorch.means.ZeroMean()\n            gp_self.covar_module = self.set_kernel(kernel_form)\n\n        def forward(gp_self, x):\n            mean_x = gp_self.mean_module(x)\n            covar_x = gp_self.covar_module(x)\n            return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\n    return GPModel(self.train_times, self.train_rates, likelihood)\n</code></pre>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess.enforce_normality","title":"<code>enforce_normality()</code>","text":"<p>Check normality of the input data and apply a Box-Cox transformation if needed.</p> <p>This method first checks if the light curve's flux distribution appears normal. If not, a Box-Cox transformation is applied to improve it. STELA automatically selects the most appropriate test (Shapiro-Wilk or Lilliefors) based on sample size.</p> Source code in <code>stela_toolkit/gaussian_process.py</code> <pre><code>def enforce_normality(self):\n    \"\"\"\n    Check normality of the input data and apply a Box-Cox transformation if needed.\n\n    This method first checks if the light curve's flux distribution appears normal.\n    If not, a Box-Cox transformation is applied to improve it. STELA automatically\n    selects the most appropriate test (Shapiro-Wilk or Lilliefors) based on sample size.\n    \"\"\"\n    print(\"Checking normality of input light curve...\")\n\n    is_normal_before, pval_before = Preprocessing.check_normal(self.lc, plot=False, verbose=False)\n\n    if is_normal_before:\n        print(f\"\\n - Light curve appears normal (p = {pval_before:.4f}). No transformation applied.\")\n        return\n\n    print(f\"\\n - Light curve is not normal (p = {pval_before:.4f}). Applying Box-Cox transformation...\")\n\n    # Apply Box-Cox transformation\n    Preprocessing.boxcox_transform(self.lc)\n\n    if self.lambda_boxcox is not None:\n        print(\" -- Note: The input was already Box-Cox transformed. No additional transformation made.\")\n    else:\n        self.lambda_boxcox = getattr(self.lc, \"lambda_boxcox\", None)\n\n    # Re-check normality\n    is_normal_after, pval_after = Preprocessing.check_normal(self.lc, plot=False, verbose=False)\n\n    if is_normal_after:\n        print(f\" - Normality sufficiently achieved after Box-Cox (p = {pval_after:.4f})! Proceed as normal!\\n\")\n    else:\n        print(f\" - Data still not normal after Box-Cox (p = {pval_after:.4f}). Proceed with caution.\\n\")\n</code></pre>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess.find_best_kernel","title":"<code>find_best_kernel(kernel_list, num_iter=500, learn_rate=0.1, verbose=False)</code>","text":"<p>Search over a list of kernels and return the best one by AIC.</p> <p>Trains the model separately with each kernel in the list, computes the AIC, and returns the model with the lowest value.</p>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess.find_best_kernel--parameters","title":"Parameters","text":"<p>kernel_list : list of str     Kernel names to try. num_iter : int     Number of iterations per training run. learn_rate : float     Learning rate for the optimizer. verbose : bool     Whether to print progress for each kernel.</p>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess.find_best_kernel--returns","title":"Returns","text":"<p>best_model : GPModel     The model trained with the best-performing kernel. best_likelihood : gpytorch.likelihoods.Likelihood     Corresponding likelihood for the best model.</p> Source code in <code>stela_toolkit/gaussian_process.py</code> <pre><code>def find_best_kernel(self, kernel_list, num_iter=500, learn_rate=1e-1, verbose=False):\n    \"\"\"\n    Search over a list of kernels and return the best one by AIC.\n\n    Trains the model separately with each kernel in the list, computes the AIC,\n    and returns the model with the lowest value.\n\n    Parameters\n    ----------\n    kernel_list : list of str\n        Kernel names to try.\n    num_iter : int\n        Number of iterations per training run.\n    learn_rate : float\n        Learning rate for the optimizer.\n    verbose : bool\n        Whether to print progress for each kernel.\n\n    Returns\n    -------\n    best_model : GPModel\n        The model trained with the best-performing kernel.\n    best_likelihood : gpytorch.likelihoods.Likelihood\n        Corresponding likelihood for the best model.\n    \"\"\"\n\n    aics = []\n    best_model = None\n    for kernel_form in kernel_list:\n        self.likelihood = self.set_likelihood(self.white_noise, train_errors=self.train_errors)\n        self.model = self.create_gp_model(self.likelihood, kernel_form)\n        # suppress output, even for verbose=True\n        self.train(num_iter=num_iter, learn_rate=learn_rate, verbose=False)\n\n        # compute aic and store best model\n        aic = self.aic()\n        aics.append(aic)\n        if aic &lt;= min(aics):\n            best_model = self.model\n            best_likelihood = self.likelihood\n\n    best_aic = min(aics)\n    best_kernel = kernel_list[aics.index(best_aic)]\n\n    if verbose:\n        kernel_results = zip(kernel_list, aics)\n        print(\n            \"Kernel selection complete.\\n\"\n            f\"   Kernel AICs (lower is better):\"\n        )\n        for kernel, aic in kernel_results:\n            print(f\"     - {kernel:15}: {aic:0.5}\")\n\n        print(f\"   Best kernel: {best_kernel} (AIC: {best_aic:0.5})\")\n\n    self.kernel_form = best_kernel\n    return best_model, best_likelihood\n</code></pre>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess.get_hyperparameters","title":"<code>get_hyperparameters()</code>","text":"<p>Return the learned GP hyperparameters (lengthscale, noise, weights, etc.).</p>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess.get_hyperparameters--returns","title":"Returns","text":"<p>hyper_dict : dict     Dictionary mapping parameter names to their (transformed) values.         Note: All rate-associated hyperparameters (e.g., not lengthscale)          are in units of the standardized data, not the original flux/time units.</p> Source code in <code>stela_toolkit/gaussian_process.py</code> <pre><code>def get_hyperparameters(self):\n    \"\"\"\n    Return the learned GP hyperparameters (lengthscale, noise, weights, etc.).\n\n    Returns\n    -------\n    hyper_dict : dict\n        Dictionary mapping parameter names to their (transformed) values.\n            Note: All rate-associated hyperparameters (e.g., not lengthscale) \n            are in units of the standardized data, not the original flux/time units.\n    \"\"\"\n\n    raw_hypers = self.model.named_parameters()\n    hypers = {}\n    for param_name, param in raw_hypers:\n        # Split the parameter name into hierarchy\n        parts = param_name.split('.')\n        module = self.model\n\n        # Traverse structure of the model to get the constraint\n        for part in parts[:-1]:  # last part is parameter\n            module = getattr(module, part, None)\n            if module is None:\n                raise AttributeError(\n                    f\"Module '{part}' not found while traversing '{param_name}'.\")\n\n        final_param_name = parts[-1]\n        constraint_name = f\"{final_param_name}_constraint\"\n        constraint = getattr(module, constraint_name, None)\n\n        if constraint is None:\n            raise AttributeError(\n                f\"Constraint '{constraint_name}' not found in module '{module}'.\")\n\n        # Transform the parameter using the constraint\n        transform_param = constraint.transform(param)\n\n        # Remove 'raw_' prefix from the parameter name for readability\n        param_name_withoutraw = param_name.replace('raw_', '')\n\n        if self.kernel_form == 'SpectralMixture':\n            transform_param = transform_param.detach().numpy().flatten()\n        else:\n            transform_param = transform_param.item()\n\n        hypers[param_name_withoutraw] = transform_param\n\n    return hypers\n</code></pre>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess.load","title":"<code>load(file_path)</code>  <code>staticmethod</code>","text":"<p>Load a saved GaussianProcess model from file.</p>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess.load--parameters","title":"Parameters","text":"<p>file_path : str     Path to the saved file.</p>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess.load--returns","title":"Returns","text":"<p>GaussianProcess     Restored instance of the model.</p> Source code in <code>stela_toolkit/gaussian_process.py</code> <pre><code>@staticmethod\ndef load(file_path):\n    \"\"\"\n    Load a saved GaussianProcess model from file.\n\n    Parameters\n    ----------\n    file_path : str\n        Path to the saved file.\n\n    Returns\n    -------\n    GaussianProcess\n        Restored instance of the model.\n    \"\"\"\n\n    with open(file_path, \"rb\") as f:\n        instance = pickle.load(f)\n    print(f\"GaussianProcess instance loaded from {file_path}.\")\n    return instance\n</code></pre>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess.plot","title":"<code>plot(pred_times=None)</code>","text":"<p>Plot the GP fit including mean, confidence intervals, one posterior sample, and data.</p>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess.plot--parameters","title":"Parameters","text":"<p>**kwargs : dict     Additional keyword arguments for plot customization.</p> Source code in <code>stela_toolkit/gaussian_process.py</code> <pre><code>def plot(self, pred_times=None):\n    \"\"\"\n    Plot the GP fit including mean, confidence intervals, one posterior sample, and data.\n\n    Parameters\n    ----------\n    **kwargs : dict\n        Additional keyword arguments for plot customization.\n    \"\"\"\n\n    if pred_times is None:\n        step = (self.train_times.max() - self.train_times.min()) / 1000\n        pred_times = np.arange(self.train_times.min(), self.train_times.max() + step, step)\n\n    predict_mean, predict_lower, predict_upper = self.predict(pred_times)\n\n    plt.figure(figsize=(8, 4.5))\n\n    plt.fill_between(pred_times, predict_lower, predict_upper,\n                     color='dodgerblue', alpha=0.2, label=r'Prediction 2$\\sigma$ CI')\n    plt.plot(pred_times, predict_mean, color='dodgerblue', label='Prediction Mean')\n\n    sample = self.sample(pred_times, num_samples=1, _save_to_state=False)\n    plt.plot(pred_times, sample[0], color='orange', lw=1, label='Sample')\n\n    if self.train_errors.size(dim=0) &gt; 0:\n        plt.errorbar(self.lc.times, self.lc.rates, yerr=self.lc.errors,\n                     fmt='o', color='black', lw=1.5, ms=3)\n    else:\n        plt.scatter(self.lc.times, self.lc.rates, color='black', s=6)\n\n    plt.xlabel('Time', fontsize=12)\n    plt.ylabel('Rate', fontsize=12)\n    plt.legend()\n    plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n    plt.tick_params(which='both', direction='in', length=6, width=1,\n                    top=True, right=True, labelsize=12)\n    plt.show()\n</code></pre>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess.predict","title":"<code>predict(pred_times)</code>","text":"<p>Compute the posterior mean and 2-sigma confidence intervals at specified times.</p>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess.predict--parameters","title":"Parameters","text":"<p>pred_times : array-like     Time values to predict.</p>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess.predict--returns","title":"Returns","text":"<p>mean, lower, upper : ndarray     Predicted mean and lower/upper bounds of the 95 percent confidence interval.</p> Source code in <code>stela_toolkit/gaussian_process.py</code> <pre><code>def predict(self, pred_times):\n    \"\"\"\n    Compute the posterior mean and 2-sigma confidence intervals at specified times.\n\n    Parameters\n    ----------\n    pred_times : array-like\n        Time values to predict.\n\n    Returns\n    -------\n    mean, lower, upper : ndarray\n        Predicted mean and lower/upper bounds of the 95 percent confidence interval.\n    \"\"\"\n\n    # Check if pred_times is a torch tensor\n    if not isinstance(pred_times, torch.Tensor):\n        try:\n            pred_times = torch.tensor(pred_times, dtype=torch.float32)\n        except TypeError:\n            raise TypeError(\"pred_times must be a torch tensor or convertible to one.\")\n\n    self.model.eval()\n    self.likelihood.eval()\n\n    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n        pred_dist = self.likelihood(self.model(pred_times))\n        mean = pred_dist.mean\n        lower, upper = pred_dist.confidence_region()\n\n    # Unstandardize/unboxcox\n    mean = self._undo_transforms(mean)\n    lower = self._undo_transforms(lower)\n    upper = self._undo_transforms(upper)\n    return mean.numpy(), lower.numpy(), upper.numpy()\n</code></pre>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess.sample","title":"<code>sample(pred_times, num_samples, save_path=None, _save_to_state=True)</code>","text":"<p>Generate posterior samples from the trained GP model.</p> <p>These samples represent plausible realizations of the light curve. These are what is used by the coherence, power spectrum, and lag modules when a GP model is passed in.</p>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess.sample--parameters","title":"Parameters","text":"<p>pred_times : array-like     Time points where samples should be drawn. num_samples : int     Number of realizations to generate. save_path : str, optional     File path to save the samples. _save_to_state : bool, optional     Whether to store results in the object (used by other classes).</p>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess.sample--returns","title":"Returns","text":"<p>samples : ndarray     Array of sampled light curves with shape (num_samples, len(pred_times)).</p> Source code in <code>stela_toolkit/gaussian_process.py</code> <pre><code>def sample(self, pred_times, num_samples, save_path=None, _save_to_state=True):\n    \"\"\"\n    Generate posterior samples from the trained GP model.\n\n    These samples represent plausible realizations of the light curve. These are what is used\n    by the coherence, power spectrum, and lag modules when a GP model is passed in.\n\n    Parameters\n    ----------\n    pred_times : array-like\n        Time points where samples should be drawn.\n    num_samples : int\n        Number of realizations to generate.\n    save_path : str, optional\n        File path to save the samples.\n    _save_to_state : bool, optional\n        Whether to store results in the object (used by other classes).\n\n    Returns\n    -------\n    samples : ndarray\n        Array of sampled light curves with shape (num_samples, len(pred_times)).\n    \"\"\"\n\n    pred_times_tensor = torch.tensor(pred_times, dtype=torch.float32)\n    self.model.eval()\n    self.likelihood.eval()\n\n    # Make predictions\n    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n        pred_dist = self.likelihood(self.model(pred_times_tensor))\n        post_samples = pred_dist.sample(sample_shape=torch.Size([num_samples]))\n\n    samples = post_samples.numpy()\n    samples = self._undo_transforms(samples)\n\n    if save_path:\n        samples_with_time = np.insert(pred_times, num_samples, 0)\n        file_ext = save_path.split(\".\")[-1]\n        if file_ext == \"npy\":\n            np.save(save_path, samples_with_time)\n        else:\n            np.savetxt(save_path, samples_with_time)\n\n    if _save_to_state:\n        self.pred_times = pred_times\n        self.samples = samples\n    return samples\n</code></pre>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess.save","title":"<code>save(file_path)</code>","text":"<p>Save the trained GP model to a file using pickle.</p>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess.save--parameters","title":"Parameters","text":"<p>file_path : str     Path to save the model.</p> Source code in <code>stela_toolkit/gaussian_process.py</code> <pre><code>def save(self, file_path):\n    \"\"\"\n    Save the trained GP model to a file using pickle.\n\n    Parameters\n    ----------\n    file_path : str\n        Path to save the model.\n    \"\"\"\n\n    with open(file_path, \"wb\") as f:\n        pickle.dump(self, f)\n    print(f\"GaussianProcess instance saved to {file_path}.\")\n</code></pre>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess.set_kernel","title":"<code>set_kernel(kernel_form)</code>","text":"<p>Set the GP kernel (covariance function) based on user input.</p> <p>Handles spectral mixture, Matern, RBF, and other kernel types supported by GPyTorch. Applies reasonable defaults for lengthscale initialization.</p>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess.set_kernel--parameters","title":"Parameters","text":"<p>kernel_form : str     Name of the kernel, or 'SpectralMixture, N' to set the number of components.</p>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess.set_kernel--returns","title":"Returns","text":"<p>covar_module : gpytorch.kernels.Kernel</p> Source code in <code>stela_toolkit/gaussian_process.py</code> <pre><code>def set_kernel(self, kernel_form):\n    \"\"\"\n    Set the GP kernel (covariance function) based on user input.\n\n    Handles spectral mixture, Matern, RBF, and other kernel types supported by GPyTorch.\n    Applies reasonable defaults for lengthscale initialization.\n\n    Parameters\n    ----------\n    kernel_form : str\n        Name of the kernel, or 'SpectralMixture, N' to set the number of components.\n\n    Returns\n    -------\n    covar_module : gpytorch.kernels.Kernel\n    \"\"\"\n\n    kernel_form = kernel_form.strip()\n    if 'SpectralMixture' in kernel_form:\n        if ',' not in kernel_form:\n            raise ValueError(\n                \"Invalid Spectral Mixture kernel format (use 'SpectralMixture, N').\\n\"\n                \"N=4 is a good starting point.\"\n            )\n        else:\n            kernel_form, num_mixtures_str = kernel_form.split(',')\n            num_mixtures = int(num_mixtures_str.strip())\n    else:\n        num_mixtures = 4  # set num_mixtures for kernel_mapping when Spectral Mixture kernel not used\n\n    kernel_mapping = {\n        'Matern12': gpytorch.kernels.MaternKernel(nu=0.5),\n        'Matern32': gpytorch.kernels.MaternKernel(nu=1.5),\n        'Matern52': gpytorch.kernels.MaternKernel(nu=2.5),\n        'RQ': gpytorch.kernels.RQKernel(),\n        'RBF': gpytorch.kernels.RBFKernel(),\n        'SpectralMixture': gpytorch.kernels.SpectralMixtureKernel(num_mixtures=num_mixtures),\n        'Periodic': gpytorch.kernels.PeriodicKernel()\n    }\n\n    # Assign kernel if type is valid\n    if kernel_form in kernel_mapping:\n        kernel = kernel_mapping[kernel_form]\n    else:\n        raise ValueError(\n            f\"Invalid kernel functional form '{kernel_form}'. Choose from {list(kernel_mapping.keys())}.\")\n\n    if kernel_form == 'SpectralMixture':\n        kernel.initialize_from_data(self.train_times, self.train_rates)\n    else:\n        init_lengthscale = (self.train_times[-1] - self.train_times[0]) / 10\n        kernel.lengthscale = init_lengthscale\n\n    # Scale the kernel by a constant\n    covar_module = gpytorch.kernels.ScaleKernel(kernel)\n    self.kernel_form = kernel_form\n\n    return covar_module\n</code></pre>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess.set_likelihood","title":"<code>set_likelihood(white_noise, train_errors=torch.tensor([]))</code>","text":"<p>Set up the GP likelihood model based on user input and data characteristics.</p> <p>If error bars are available, uses a FixedNoiseGaussianLikelihood. Otherwise, defaults to a GaussianLikelihood with optional white noise. If white noise is enabled, the noise level is initialized based on Poisson statistics or variance in the data.</p>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess.set_likelihood--parameters","title":"Parameters","text":"<p>white_noise : bool     Whether to include a learnable noise term in the model. train_errors : torch.Tensor, optional     Measurement errors from the light curve.</p>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess.set_likelihood--returns","title":"Returns","text":"<p>likelihood : gpytorch.likelihoods.Likelihood     GPyTorch subclass, also used for training.</p> Source code in <code>stela_toolkit/gaussian_process.py</code> <pre><code>def set_likelihood(self, white_noise, train_errors=torch.tensor([])):\n    \"\"\"\n    Set up the GP likelihood model based on user input and data characteristics.\n\n    If error bars are available, uses a FixedNoiseGaussianLikelihood. Otherwise, defaults to a\n    GaussianLikelihood with optional white noise. If white noise is enabled, the noise level\n    is initialized based on Poisson statistics or variance in the data.\n\n    Parameters\n    ----------\n    white_noise : bool\n        Whether to include a learnable noise term in the model.\n    train_errors : torch.Tensor, optional\n        Measurement errors from the light curve.\n\n    Returns\n    -------\n    likelihood : gpytorch.likelihoods.Likelihood\n        GPyTorch subclass, also used for training.\n    \"\"\"\n\n    if white_noise:\n        noise_constraint = gpytorch.constraints.Interval(1e-5, 1)\n    else:\n        noise_constraint = gpytorch.constraints.Interval(1e-40, 1e-39)\n\n    if train_errors.size(dim=0) &gt; 0:\n        likelihood = gpytorch.likelihoods.FixedNoiseGaussianLikelihood(\n            noise=self.train_errors ** 2,\n            learn_additional_noise=white_noise,\n            noise_constraint=noise_constraint\n        )\n\n    else:\n        likelihood = gpytorch.likelihoods.GaussianLikelihood(\n            noise_constraint=noise_constraint,\n        )\n\n        if white_noise:\n            counts = np.abs(self.train_rates[1:].numpy()) * np.diff(self.train_times.numpy())\n            # begin with a slight underestimation to prevent overfitting\n            norm_poisson_var = 1 / (2 * np.mean(counts))\n            likelihood.noise = norm_poisson_var\n\n    # initialize noise parameter at the variance of the data\n    return likelihood\n</code></pre>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess.train","title":"<code>train(num_iter=500, learn_rate=0.1, plot=False, verbose=False)</code>","text":"<p>Train the GP model using the Adam optimizer to minimize the negative log marginal likelihood (NLML).</p> <p>By default, prints progress periodically and optionally plots the NLML loss curve over training iterations. This function is typically called after initialization unless <code>run_training=True</code> was set earlier.</p>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess.train--parameters","title":"Parameters","text":"<p>num_iter : int, optional     Number of optimization steps to perform. Default is 500. learn_rate : float, optional     Learning rate for the Adam optimizer. Default is 0.1. plot : bool, optional     If True, display a plot of the NLML loss as training progresses. verbose : bool, optional     If True, print progress updates at regular intervals during training.</p> Source code in <code>stela_toolkit/gaussian_process.py</code> <pre><code>def train(self, num_iter=500, learn_rate=1e-1, plot=False, verbose=False):\n    \"\"\"\n    Train the GP model using the Adam optimizer to minimize the negative log marginal likelihood (NLML).\n\n    By default, prints progress periodically and optionally plots the NLML loss curve over training iterations.\n    This function is typically called after initialization unless `run_training=True` was set earlier.\n\n    Parameters\n    ----------\n    num_iter : int, optional\n        Number of optimization steps to perform. Default is 500.\n    learn_rate : float, optional\n        Learning rate for the Adam optimizer. Default is 0.1.\n    plot : bool, optional\n        If True, display a plot of the NLML loss as training progresses.\n    verbose : bool, optional\n        If True, print progress updates at regular intervals during training.\n    \"\"\"\n\n    self.model.train()\n    self.likelihood.train()\n\n    optimizer = torch.optim.Adam(self.model.parameters(), lr=learn_rate)\n    mll = gpytorch.mlls.ExactMarginalLogLikelihood(self.likelihood, self.model)\n\n    print_every = max(1, num_iter // 20)\n\n    if plot:\n        plt.figure(figsize=(8, 5))\n\n    for i in range(num_iter):\n        optimizer.zero_grad()\n        output = self.model(self.train_times)\n        loss = -mll(output, self.train_rates)\n        loss.backward()\n\n        if verbose and (i == num_iter - 1 or i % print_every == 0):\n            if self.white_noise:\n                if self.train_errors.size(dim=0) &gt; 0:\n                    noise_param = self.model.likelihood.second_noise.item()\n                else:\n                    noise_param = self.model.likelihood.noise.item()\n\n            if self.kernel_form == 'SpectralMixture':\n                mixture_scales = self.model.covar_module.base_kernel.mixture_scales\n                mixture_scales = mixture_scales.detach().numpy().flatten()\n                mixture_weights = self.model.covar_module.base_kernel.mixture_weights\n                mixture_weights = mixture_weights.detach().numpy().flatten()\n\n                if self.white_noise:\n                    print('Iter %d/%d - loss: %.3f   mixture_lengthscales: %s   mixture_weights: %s   noise: %.1e' % (\n                        i + 1, num_iter, loss.item(),\n                        mixture_scales.round(3),\n                        mixture_weights.round(3),\n                        noise_param\n                    ))\n                else:\n                    print('Iter %d/%d - loss: %.3f   mixture_lengthscales: %s   mixture_weights: %s' % (\n                        i + 1, num_iter, loss.item(),\n                        mixture_scales.round(3),\n                        mixture_weights.round(3)\n                    ))\n\n            elif self.kernel_form == 'Periodic':\n                if self.white_noise:\n                    print('Iter %d/%d - loss: %.3f   period length: %.3f   lengthscale: %.3f   noise: %.1e' % (\n                        i + 1, num_iter, loss.item(),\n                        self.model.covar_module.base_kernel.period_length.item(),\n                        self.model.covar_module.base_kernel.lengthscale.item(),\n                        noise_param\n                    ))\n                else:\n                    print('Iter %d/%d - loss: %.3f   lengthscale: %.1e' % (\n                        i + 1, num_iter, loss.item(),\n                        self.model.covar_module.base_kernel.lengthscale.item()\n                    ))\n\n            else:\n                if self.white_noise:\n                    print('Iter %d/%d - loss: %.3f   lengthscale: %.3f   noise: %.1e' % (\n                        i + 1, num_iter, loss.item(),\n                        self.model.covar_module.base_kernel.lengthscale.item(),\n                        noise_param\n                    ))\n                else:\n                    print('Iter %d/%d - loss: %.3f   lengthscale: %.1e' % (\n                        i + 1, num_iter, loss.item(),\n                        self.model.covar_module.base_kernel.lengthscale.item()\n                    ))\n\n        optimizer.step()\n\n        if plot:\n            plt.scatter(i, loss.item(), color='black', s=2)\n\n    if verbose:\n        final_hypers = self.get_hyperparameters()\n        print(\n            \"Training complete. \\n\"\n            f\"   - Final loss: {loss.item():0.5}\\n\"\n            f\"   - Final hyperparameters:\")\n        for key, value in final_hypers.items():\n            print(f\"      {key:42}: {np.round(value, 4)}\")\n\n    if plot:\n        plt.xlabel('Iteration')\n        plt.ylabel('Negative Marginal Log Likelihood')\n        plt.title('Training Progress')\n        plt.show()\n</code></pre>"},{"location":"reference/lag_energy_spectrum/","title":"lag_energy_spectrum","text":""},{"location":"reference/lag_energy_spectrum/#stela_toolkit.lag_energy_spectrum.LagEnergySpectrum","title":"<code>LagEnergySpectrum</code>","text":"<p>Compute the time lag as a function of energy between two sets of light curves or GP models.</p> <p>This class accepts lists of LightCurve objects or trained GaussianProcess models, one per energy bin. If the inputs are GP models, the most recently generated samples will be used automatically. If no samples are found, 1000 realizations will be generated on a 1000-point grid.</p> <p>Each light curve pair (one per energy bin) is used to compute a single lag by integrating the cross-spectrum over a specified frequency range. This yields one lag per energy bin, forming the lag-energy spectrum.</p> <p>A positive lag means that the time series in <code>lcs_or_models1</code> is lagging behind the corresponding series in <code>lcs_or_models2</code>.</p> <p>Coherence values are also computed for each energy bin to assess correlation strength, and noise bias correction can be applied to the coherence before estimating uncertainties.</p>"},{"location":"reference/lag_energy_spectrum/#stela_toolkit.lag_energy_spectrum.LagEnergySpectrum--parameters","title":"Parameters","text":"<p>lcs_or_models1 : list of LightCurve or GaussianProcess     First set of inputs, one per energy bin. lcs_or_models2 : list of LightCurve or GaussianProcess     Second set of inputs, matched to <code>lcs_or_models1</code>. fmin : float     Minimum frequency to include when integrating. fmax : float     Maximum frequency to include when integrating. bin_edges : array-like     Edges of the energy bins corresponding to the light curves. subtract_coh_bias : bool, optional     Whether to subtract the coherence noise bias before estimating lag uncertainties.</p>"},{"location":"reference/lag_energy_spectrum/#stela_toolkit.lag_energy_spectrum.LagEnergySpectrum--attributes","title":"Attributes","text":"<p>energies : array-like     Mean energy of each bin. energy_widths : array-like     Half-width of each energy bin. lags : array-like     Integrated time lag per energy bin. lag_errors : array-like     Uncertainties in each lag value. cohs : array-like     Coherence values per energy bin. coh_errors : array-like     Uncertainties in the coherence values.</p> Source code in <code>stela_toolkit/lag_energy_spectrum.py</code> <pre><code>class LagEnergySpectrum:\n    \"\"\"\n    Compute the time lag as a function of energy between two sets of light curves or GP models.\n\n    This class accepts lists of LightCurve objects or trained GaussianProcess models, one per energy bin.\n    If the inputs are GP models, the most recently generated samples will be used automatically.\n    If no samples are found, 1000 realizations will be generated on a 1000-point grid.\n\n    Each light curve pair (one per energy bin) is used to compute a single lag by integrating the\n    cross-spectrum over a specified frequency range. This yields one lag per energy bin, forming\n    the lag-energy spectrum.\n\n    A **positive lag** means that the time series in `lcs_or_models1` is **lagging behind**\n    the corresponding series in `lcs_or_models2`.\n\n    Coherence values are also computed for each energy bin to assess correlation strength,\n    and noise bias correction can be applied to the coherence before estimating uncertainties.\n\n    Parameters\n    ----------\n    lcs_or_models1 : list of LightCurve or GaussianProcess\n        First set of inputs, one per energy bin.\n    lcs_or_models2 : list of LightCurve or GaussianProcess\n        Second set of inputs, matched to `lcs_or_models1`.\n    fmin : float\n        Minimum frequency to include when integrating.\n    fmax : float\n        Maximum frequency to include when integrating.\n    bin_edges : array-like\n        Edges of the energy bins corresponding to the light curves.\n    subtract_coh_bias : bool, optional\n        Whether to subtract the coherence noise bias before estimating lag uncertainties.\n\n    Attributes\n    ----------\n    energies : array-like\n        Mean energy of each bin.\n    energy_widths : array-like\n        Half-width of each energy bin.\n    lags : array-like\n        Integrated time lag per energy bin.\n    lag_errors : array-like\n        Uncertainties in each lag value.\n    cohs : array-like\n        Coherence values per energy bin.\n    coh_errors : array-like\n        Uncertainties in the coherence values.\n    \"\"\"\n\n    def __init__(self,\n                 lcs_or_models1,\n                 lcs_or_models2,\n                 fmin,\n                 fmax,\n                 bin_edges=[],\n                 subtract_coh_bias=True):\n\n        # leave main input check to LagFrequencySpectrum, check same input dimensions for now.\n        if len(lcs_or_models1) != len(lcs_or_models2):\n            raise ValueError(\"The lightcurves_or_models arrays must contain the sane number of lightcurve/model objects.\")\n\n        self.data_models1 = lcs_or_models1\n        self.data_models2 = lcs_or_models2\n\n        self.energies = [np.mean(bin_edges[i], bin_edges[i+1]) for i in range(len(bin_edges[:-1]))]\n        self.energy_widths = np.diff(bin_edges) / 2\n\n        self.fmin, self.fmax = fmin, fmax\n        lag_spectrum = self.compute_lag_spectrum(subtract_coh_bias=subtract_coh_bias)\n        self.lags, self.lag_errors, self.cohs, self. coh_errors = lag_spectrum\n\n    def compute_lag_spectrum(self, subtract_coh_bias):\n        \"\"\"\n        Compute the lag and coherence for each energy bin.\n\n        Parameters\n        ----------\n        subtract_coh_bias : bool\n            Whether to subtract Poisson noise bias from the coherence.\n\n        Returns\n        -------\n        lags : list\n            List of integrated lags for each bin.\n        lag_errors : list\n            List of lag uncertainties.\n        cohs : list\n            List of mean coherence values.\n        coh_errors : list\n            List of coherence uncertainties.\n        \"\"\"\n\n        lags, lag_errors, cohs, coh_errors = [], [], [], []\n        for i in range(len(self.data_models1)):\n            lfs = LagFrequencySpectrum(self.data_models1[i],\n                                       self.data_models2[i],\n                                       fmin=self.fmin,\n                                       fmax=self.fmax,\n                                       num_bins=1,\n                                       subtract_coh_bias=subtract_coh_bias,\n                                       )\n            lags.append(lfs.lags)\n            lag_errors.append(lfs.lag_errors)\n            cohs.append(lfs.cohs)\n            coh_errors.append(lfs.coh_errors)\n\n        return lags, lag_errors, cohs, coh_errors\n\n    def plot(self, energies=None, energy_widths=None, lags=None, lag_errors=None, cohs=None, coh_errors=None, **kwargs):\n        \"\"\"\n        Plot the lag-energy spectrum and associated coherence values.\n\n        Parameters\n        ----------\n        **kwargs : dict\n            Keyword arguments for customizing the plot (e.g., xlabel, xscale, yscale).\n        \"\"\"\n        energies = self.energies if energies is None else energies\n        energy_widths = self.energy_widths if energy_widths is None else energy_widths\n        lags = self.lags if lags is None else lags\n        lag_errors = self.lag_errors if lag_errors is None else lag_errors\n        cohs = self.cohs if cohs is None else cohs\n        coh_errors = self.coh_errors if coh_errors is None else coh_errors\n\n        figsize = kwargs.get('figsize', (8, 6))\n        xlabel = kwargs.get('xlabel', 'Energy')\n        ylabel = kwargs.get('ylabel', 'Time Lag')\n        xscale = kwargs.get('xscale', 'log')\n        yscale = kwargs.get('yscale', 'linear')\n\n        fig, (ax1, ax2) = plt.subplots(2, 1, gridspec_kw={'height_ratios': [2, 1]}, figsize=figsize, sharex=True)\n        plt.subplots_adjust(hspace=0.05)\n\n        # Lag-energy spectrum\n        ax1.errorbar(energies, lags, xerr=energy_widths, yerr=lag_errors, fmt='o', color='black', ms=3, lw=1.5)\n        ax1.set_xscale(xscale)\n        ax1.set_yscale(yscale)\n        ax1.set_ylabel(ylabel, fontsize=12)\n        ax1.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n        ax1.tick_params(which='both', direction='in', length=6, width=1, top=True, right=True, labelsize=12)\n\n        # Coherence spectrum\n        if cohs is not None and coh_errors is not None:\n            ax2.errorbar(energies, cohs, xerr=energy_widths, yerr=coh_errors, fmt='o', color='black', ms=3, lw=1.5)\n            ax2.set_xscale(xscale)\n            ax2.set_ylabel('Coherence', fontsize=12)\n            ax2.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n            ax2.tick_params(which='both', direction='in', length=6, width=1, top=True, right=True, labelsize=12)\n\n        fig.text(0.5, 0.04, xlabel, ha='center', va='center', fontsize=12)\n        plt.tight_layout()\n        plt.show()\n\n    def count_frequencies_in_bins(self, fmin=None, fmax=None, num_bins=None, bin_type=None, bin_edges=[]):\n        \"\"\"\n        Counts the number of frequencies in each frequency bin.\n        Wrapper method to use FrequencyBinning.count_frequencies_in_bins with class attributes.\n        \"\"\"\n\n        return FrequencyBinning.count_frequencies_in_bins(\n            self, fmin=fmin, fmax=fmax, num_bins=num_bins, bin_type=bin_type, bin_edges=bin_edges\n        )\n</code></pre>"},{"location":"reference/lag_energy_spectrum/#stela_toolkit.lag_energy_spectrum.LagEnergySpectrum.compute_lag_spectrum","title":"<code>compute_lag_spectrum(subtract_coh_bias)</code>","text":"<p>Compute the lag and coherence for each energy bin.</p>"},{"location":"reference/lag_energy_spectrum/#stela_toolkit.lag_energy_spectrum.LagEnergySpectrum.compute_lag_spectrum--parameters","title":"Parameters","text":"<p>subtract_coh_bias : bool     Whether to subtract Poisson noise bias from the coherence.</p>"},{"location":"reference/lag_energy_spectrum/#stela_toolkit.lag_energy_spectrum.LagEnergySpectrum.compute_lag_spectrum--returns","title":"Returns","text":"<p>lags : list     List of integrated lags for each bin. lag_errors : list     List of lag uncertainties. cohs : list     List of mean coherence values. coh_errors : list     List of coherence uncertainties.</p> Source code in <code>stela_toolkit/lag_energy_spectrum.py</code> <pre><code>def compute_lag_spectrum(self, subtract_coh_bias):\n    \"\"\"\n    Compute the lag and coherence for each energy bin.\n\n    Parameters\n    ----------\n    subtract_coh_bias : bool\n        Whether to subtract Poisson noise bias from the coherence.\n\n    Returns\n    -------\n    lags : list\n        List of integrated lags for each bin.\n    lag_errors : list\n        List of lag uncertainties.\n    cohs : list\n        List of mean coherence values.\n    coh_errors : list\n        List of coherence uncertainties.\n    \"\"\"\n\n    lags, lag_errors, cohs, coh_errors = [], [], [], []\n    for i in range(len(self.data_models1)):\n        lfs = LagFrequencySpectrum(self.data_models1[i],\n                                   self.data_models2[i],\n                                   fmin=self.fmin,\n                                   fmax=self.fmax,\n                                   num_bins=1,\n                                   subtract_coh_bias=subtract_coh_bias,\n                                   )\n        lags.append(lfs.lags)\n        lag_errors.append(lfs.lag_errors)\n        cohs.append(lfs.cohs)\n        coh_errors.append(lfs.coh_errors)\n\n    return lags, lag_errors, cohs, coh_errors\n</code></pre>"},{"location":"reference/lag_energy_spectrum/#stela_toolkit.lag_energy_spectrum.LagEnergySpectrum.count_frequencies_in_bins","title":"<code>count_frequencies_in_bins(fmin=None, fmax=None, num_bins=None, bin_type=None, bin_edges=[])</code>","text":"<p>Counts the number of frequencies in each frequency bin. Wrapper method to use FrequencyBinning.count_frequencies_in_bins with class attributes.</p> Source code in <code>stela_toolkit/lag_energy_spectrum.py</code> <pre><code>def count_frequencies_in_bins(self, fmin=None, fmax=None, num_bins=None, bin_type=None, bin_edges=[]):\n    \"\"\"\n    Counts the number of frequencies in each frequency bin.\n    Wrapper method to use FrequencyBinning.count_frequencies_in_bins with class attributes.\n    \"\"\"\n\n    return FrequencyBinning.count_frequencies_in_bins(\n        self, fmin=fmin, fmax=fmax, num_bins=num_bins, bin_type=bin_type, bin_edges=bin_edges\n    )\n</code></pre>"},{"location":"reference/lag_energy_spectrum/#stela_toolkit.lag_energy_spectrum.LagEnergySpectrum.plot","title":"<code>plot(energies=None, energy_widths=None, lags=None, lag_errors=None, cohs=None, coh_errors=None, **kwargs)</code>","text":"<p>Plot the lag-energy spectrum and associated coherence values.</p>"},{"location":"reference/lag_energy_spectrum/#stela_toolkit.lag_energy_spectrum.LagEnergySpectrum.plot--parameters","title":"Parameters","text":"<p>**kwargs : dict     Keyword arguments for customizing the plot (e.g., xlabel, xscale, yscale).</p> Source code in <code>stela_toolkit/lag_energy_spectrum.py</code> <pre><code>def plot(self, energies=None, energy_widths=None, lags=None, lag_errors=None, cohs=None, coh_errors=None, **kwargs):\n    \"\"\"\n    Plot the lag-energy spectrum and associated coherence values.\n\n    Parameters\n    ----------\n    **kwargs : dict\n        Keyword arguments for customizing the plot (e.g., xlabel, xscale, yscale).\n    \"\"\"\n    energies = self.energies if energies is None else energies\n    energy_widths = self.energy_widths if energy_widths is None else energy_widths\n    lags = self.lags if lags is None else lags\n    lag_errors = self.lag_errors if lag_errors is None else lag_errors\n    cohs = self.cohs if cohs is None else cohs\n    coh_errors = self.coh_errors if coh_errors is None else coh_errors\n\n    figsize = kwargs.get('figsize', (8, 6))\n    xlabel = kwargs.get('xlabel', 'Energy')\n    ylabel = kwargs.get('ylabel', 'Time Lag')\n    xscale = kwargs.get('xscale', 'log')\n    yscale = kwargs.get('yscale', 'linear')\n\n    fig, (ax1, ax2) = plt.subplots(2, 1, gridspec_kw={'height_ratios': [2, 1]}, figsize=figsize, sharex=True)\n    plt.subplots_adjust(hspace=0.05)\n\n    # Lag-energy spectrum\n    ax1.errorbar(energies, lags, xerr=energy_widths, yerr=lag_errors, fmt='o', color='black', ms=3, lw=1.5)\n    ax1.set_xscale(xscale)\n    ax1.set_yscale(yscale)\n    ax1.set_ylabel(ylabel, fontsize=12)\n    ax1.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n    ax1.tick_params(which='both', direction='in', length=6, width=1, top=True, right=True, labelsize=12)\n\n    # Coherence spectrum\n    if cohs is not None and coh_errors is not None:\n        ax2.errorbar(energies, cohs, xerr=energy_widths, yerr=coh_errors, fmt='o', color='black', ms=3, lw=1.5)\n        ax2.set_xscale(xscale)\n        ax2.set_ylabel('Coherence', fontsize=12)\n        ax2.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n        ax2.tick_params(which='both', direction='in', length=6, width=1, top=True, right=True, labelsize=12)\n\n    fig.text(0.5, 0.04, xlabel, ha='center', va='center', fontsize=12)\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"reference/lag_frequency_spectrum/","title":"lag_frequency_spectrum","text":""},{"location":"reference/lag_frequency_spectrum/#stela_toolkit.lag_frequency_spectrum.LagFrequencySpectrum","title":"<code>LagFrequencySpectrum</code>","text":"<p>Compute the time lag as a function of frequency between two time series.</p> <p>This class accepts either LightCurve objects (with regular sampling) or trained GaussianProcess models from this package. If GP models are provided, the most recently generated samples are used. If no samples have been created yet, the toolkit will automatically generate 1000 samples on a 1000-point grid.</p> <p>The sign convention is such that a positive lag indicates that the input provided as <code>lc_or_model1</code> is lagging behind the input provided as <code>lc_or_model2</code>.</p> <p>There are two modes for computing uncertainties: - If the inputs are individual light curves, lag uncertainties are propagated from   the coherence spectrum using a theoretical error model. - If the inputs are GP models, the lag spectrum is computed for each sample and   uncertainties are reported as the standard deviation across all samples.</p>"},{"location":"reference/lag_frequency_spectrum/#stela_toolkit.lag_frequency_spectrum.LagFrequencySpectrum--parameters","title":"Parameters","text":"<p>lc_or_model1 : LightCurve or GaussianProcess     First input time series or trained model. lc_or_model2 : LightCurve or GaussianProcess     Second input time series or trained model. fmin : float or 'auto', optional     Minimum frequency to include. If 'auto', uses the lowest nonzero FFT frequency. fmax : float or 'auto', optional     Maximum frequency to include. If 'auto', uses the Nyquist frequency. num_bins : int, optional     Number of frequency bins. bin_type : str, optional     Type of binning: 'log' or 'linear'. bin_edges : array-like, optional     Custom bin edges (overrides <code>num_bins</code> and <code>bin_type</code>). subtract_coh_bias : bool, optional     Whether to subtract Poisson noise bias from the coherence. plot_lfs : bool, optional     Whether to generate the lag-frequency plot on initialization.</p>"},{"location":"reference/lag_frequency_spectrum/#stela_toolkit.lag_frequency_spectrum.LagFrequencySpectrum--attributes","title":"Attributes","text":"<p>freqs : array-like     Frequency bin centers. freq_widths : array-like     Bin widths for each frequency bin. lags : array-like     Computed time lags. lag_errors : array-like     Uncertainties in the lag estimates. cohs : array-like     Coherence values for each frequency bin. coh_errors : array-like     Uncertainties in the coherence values.</p> Source code in <code>stela_toolkit/lag_frequency_spectrum.py</code> <pre><code>class LagFrequencySpectrum:\n    \"\"\"\n    Compute the time lag as a function of frequency between two time series.\n\n    This class accepts either LightCurve objects (with regular sampling) or trained\n    GaussianProcess models from this package. If GP models are provided, the most\n    recently generated samples are used. If no samples have been created yet,\n    the toolkit will automatically generate 1000 samples on a 1000-point grid.\n\n    The sign convention is such that a **positive lag** indicates that the input provided as\n    `lc_or_model1` is **lagging behind** the input provided as `lc_or_model2`.\n\n    There are two modes for computing uncertainties:\n    - If the inputs are individual light curves, lag uncertainties are propagated from\n      the coherence spectrum using a theoretical error model.\n    - If the inputs are GP models, the lag spectrum is computed for each sample and\n      uncertainties are reported as the standard deviation across all samples.\n\n    Parameters\n    ----------\n    lc_or_model1 : LightCurve or GaussianProcess\n        First input time series or trained model.\n    lc_or_model2 : LightCurve or GaussianProcess\n        Second input time series or trained model.\n    fmin : float or 'auto', optional\n        Minimum frequency to include. If 'auto', uses the lowest nonzero FFT frequency.\n    fmax : float or 'auto', optional\n        Maximum frequency to include. If 'auto', uses the Nyquist frequency.\n    num_bins : int, optional\n        Number of frequency bins.\n    bin_type : str, optional\n        Type of binning: 'log' or 'linear'.\n    bin_edges : array-like, optional\n        Custom bin edges (overrides `num_bins` and `bin_type`).\n    subtract_coh_bias : bool, optional\n        Whether to subtract Poisson noise bias from the coherence.\n    plot_lfs : bool, optional\n        Whether to generate the lag-frequency plot on initialization.\n\n    Attributes\n    ----------\n    freqs : array-like\n        Frequency bin centers.\n    freq_widths : array-like\n        Bin widths for each frequency bin.\n    lags : array-like\n        Computed time lags.\n    lag_errors : array-like\n        Uncertainties in the lag estimates.\n    cohs : array-like\n        Coherence values for each frequency bin.\n    coh_errors : array-like\n        Uncertainties in the coherence values.\n    \"\"\"\n\n    def __init__(self,\n                 lc_or_model1,\n                 lc_or_model2,\n                 fmin='auto',\n                 fmax='auto',\n                 num_bins=None,\n                 bin_type=\"log\",\n                 bin_edges=[],\n                 subtract_coh_bias=True):\n\n        # To do: update main docstring for lag interpretation, add coherence in plotting !!\n        input_data = _CheckInputs._check_lightcurve_or_model(lc_or_model1)\n        if input_data['type'] == 'model':\n            self.times1, self.rates1 = input_data['data']\n        else:\n            self.times1, self.rates1, _ = input_data['data']\n\n        input_data = _CheckInputs._check_lightcurve_or_model(lc_or_model2)\n        if input_data['type'] == 'model':\n            self.times2, self.rates2 = input_data['data']\n        else:\n            self.times2, self.rates2, _ = input_data['data']\n\n        _CheckInputs._check_input_bins(num_bins, bin_type, bin_edges)\n\n        if not np.allclose(self.times1, self.times2):\n            raise ValueError(\"The time arrays of the two light curves must be identical.\")\n\n        # Use absolute min and max frequencies if set to 'auto'\n        self.dt = np.diff(self.times1)[0]\n        self.fmin = np.fft.rfftfreq(len(self.rates1), d=self.dt)[1] if fmin == 'auto' else fmin\n        self.fmax = np.fft.rfftfreq(len(self.rates1), d=self.dt)[-1] if fmax == 'auto' else fmax  # nyquist frequency\n\n        self.num_bins = num_bins\n        self.bin_type = bin_type\n        self.bin_edges = bin_edges\n\n        if len(self.rates1.shape) == 2 and len(self.rates2.shape) == 2:\n            lag_spectrum = self.compute_stacked_lag_spectrum()\n        else:\n            lag_spectrum = self.compute_lag_spectrum(subtract_coh_bias=subtract_coh_bias)\n\n        self.freqs, self.freq_widths, self.lags, self.lag_errors, self.cohs, self.coh_errors = lag_spectrum\n\n    def compute_lag_spectrum(self, \n                             times1=None, rates1=None,\n                             times2=None, rates2=None,\n                             subtract_coh_bias=True):\n        \"\"\"\n        Compute the lag spectrum for a single pair of light curves or model realizations.\n\n        The phase of the cross-spectrum is converted to time lags, and uncertainties are\n        computed either from coherence (light curves) or from GP sampling (if stacked mode).\n\n        Parameters\n        ----------\n        times1, rates1 : array-like, optional\n            Input time and rates for the first time series.\n        times2, rates2 : array-like, optional\n            Input time and rates for the second time series.\n        subtract_coh_bias : bool, optional\n            Whether to subtract noise bias from coherence.\n\n        Returns\n        -------\n        freqs : array-like\n            Frequency bin centers.\n        freq_widths : array-like\n            Frequency bin widths.\n        lags : array-like\n            Time lags at each frequency.\n        lag_errors : array-like\n            Uncertainty in the lag values.\n        cohs : array-like\n            Coherence values.\n        coh_errors : array-like\n            Uncertainties in the coherence values.\n        \"\"\"\n\n        times1 = times1 if times1 is not None else self.times1\n        times2 = times2 if times2 is not None else self.times2\n        rates1 = rates1 if rates1 is not None else self.rates1\n        rates2 = rates2 if rates2 is not None else self.rates2 \n\n        lc1 = LightCurve(times=times1, rates=rates1)\n        lc2 = LightCurve(times=times2, rates=rates2)\n\n        # Compute the cross spectrum\n        cross_spectrum = CrossSpectrum(lc1, lc2,\n                                       fmin=self.fmin, fmax=self.fmax,\n                                       num_bins=self.num_bins, bin_type=self.bin_type,\n                                       bin_edges=self.bin_edges,\n                                       norm=False\n                                    )\n\n        lags = np.angle(cross_spectrum.cs) / (2 * np.pi * cross_spectrum.freqs)\n\n        coherence = Coherence(lc1, lc2,\n                              fmin=self.fmin, fmax=self.fmax,\n                              num_bins=self.num_bins, bin_type=self.bin_type, bin_edges=self.bin_edges,\n                              subtract_noise_bias=subtract_coh_bias\n                            )    \n        cohs = coherence.cohs\n        coh_errors = coherence.coh_errors\n\n        num_freq = self.count_frequencies_in_bins()\n\n        phase_errors = _ClearWarnings.run(\n            lambda: np.sqrt((1 - coherence.cohs) / (2 * coherence.cohs * num_freq)),\n            explanation=\"Error from sqrt when computing (unbinned) phase errors here is common \"\n                        \"and typically due to &gt;1 coherence at the minimum frequency.\"\n        )\n\n        lag_errors = phase_errors / (2 * np.pi * cross_spectrum.freqs)\n\n        return cross_spectrum.freqs, cross_spectrum.freq_widths, lags, lag_errors, cohs, coh_errors\n\n    def compute_stacked_lag_spectrum(self):\n        \"\"\"\n        Compute lag-frequency spectrum for stacked GP samples.\n\n        This method assumes the input light curves are model-generated and include\n        multiple realizations. Returns mean and standard deviation of lag and coherence.\n\n        Returns\n        -------\n        freqs : array-like\n            Frequency bin centers.\n        freq_widths : array-like\n            Frequency bin widths.\n        lags : array-like\n            Mean time lags across samples.\n        lag_errors : array-like\n            Standard deviation of lags.\n        cohs : array-like\n            Mean coherence values.\n        coh_errors : array-like\n            Standard deviation of coherence values.\n        \"\"\"\n\n        # Compute lag spectrum for each pair of realizations\n        lag_spectra = []\n        coh_spectra = []\n        for i in range(self.rates1.shape[0]):\n            lag_spectrum = self.compute_lag_spectrum(times1=self.times1, rates1=self.rates1[i],\n                                                     times2=self.times2, rates2=self.rates2[i],\n                                                     subtract_coh_bias=False\n                                                    )\n            lag_spectra.append(lag_spectrum[2])\n            coh_spectra.append(lag_spectrum[4])\n\n        # Average lag spectra\n        lag_spectra_mean = np.mean(lag_spectra, axis=0)\n        lag_spectra_std = np.std(lag_spectra, axis=0)\n\n        # Average coherence spectra\n        coh_spectra_mean = np.mean(coh_spectra, axis=0)\n        coh_spectra_std = np.std(coh_spectra, axis=0)\n\n        freqs, freq_widths = lag_spectrum[0], lag_spectrum[1]\n\n        return freqs, freq_widths, lag_spectra_mean, lag_spectra_std, coh_spectra_mean, coh_spectra_std\n\n    def plot(self, freqs=None, freq_widths=None, lags=None, lag_errors=None, cohs=None, coh_errors=None, **kwargs):\n        \"\"\"\n        Plot the lag-frequency and coherence spectrum.\n\n        Parameters\n        ----------\n        **kwargs : dict\n            Custom plotting arguments (xlabel, xscale, yscale, etc.).\n        \"\"\"\n        freqs = self.freqs if freqs is None else freqs\n        freq_widths = self.freq_widths if freq_widths is None else freq_widths\n        lags = self.lags if lags is None else lags\n        lag_errors = self.lag_errors if lag_errors is None else lag_errors\n        cohs = self.cohs if cohs is None else cohs\n        coh_errors = self.coh_errors if coh_errors is None else coh_errors\n\n        figsize = kwargs.get('figsize', (8, 6))\n        xlabel = kwargs.get('xlabel', 'Frequency')\n        ylabel = kwargs.get('ylabel', 'Time Lag')\n        xscale = kwargs.get('xscale', 'log')\n        yscale = kwargs.get('yscale', 'linear')\n\n        fig, (ax1, ax2) = plt.subplots(2, 1, gridspec_kw={'height_ratios': [2, 1]}, figsize=figsize, sharex=True)\n        plt.subplots_adjust(hspace=0.05)\n\n        # Lag-frequency spectrum\n        ax1.errorbar(freqs, lags, xerr=freq_widths, yerr=lag_errors, fmt='o', color='black', ms=3, lw=1.5)\n        ax1.set_xscale(xscale)\n        ax1.set_yscale(yscale)\n        ax1.set_ylabel(ylabel, fontsize=12)\n        ax1.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n        ax1.tick_params(which='both', direction='in', length=6, width=1, top=True, right=True, labelsize=12)\n\n        # Coherence spectrum\n        if cohs is not None and coh_errors is not None:\n            ax2.errorbar(freqs, cohs, xerr=freq_widths, yerr=coh_errors, fmt='o', color='black', ms=3, lw=1.5)\n            ax2.set_xscale(xscale)\n            ax2.set_ylabel('Coherence', fontsize=12)\n            ax2.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n            ax2.tick_params(which='both', direction='in', length=6, width=1, top=True, right=True, labelsize=12)\n\n        fig.text(0.5, 0.04, xlabel, ha='center', va='center', fontsize=12)\n        plt.show()\n\n    def count_frequencies_in_bins(self, fmin=None, fmax=None, num_bins=None, bin_type=None, bin_edges=[]):\n        \"\"\"\n        Counts the number of frequencies in each frequency bin.\n        Wrapper method to use FrequencyBinning.count_frequencies_in_bins with class attributes.\n        \"\"\"\n\n        return FrequencyBinning.count_frequencies_in_bins(\n            self, fmin=fmin, fmax=fmax, num_bins=num_bins, bin_type=bin_type, bin_edges=bin_edges\n        )\n</code></pre>"},{"location":"reference/lag_frequency_spectrum/#stela_toolkit.lag_frequency_spectrum.LagFrequencySpectrum.compute_lag_spectrum","title":"<code>compute_lag_spectrum(times1=None, rates1=None, times2=None, rates2=None, subtract_coh_bias=True)</code>","text":"<p>Compute the lag spectrum for a single pair of light curves or model realizations.</p> <p>The phase of the cross-spectrum is converted to time lags, and uncertainties are computed either from coherence (light curves) or from GP sampling (if stacked mode).</p>"},{"location":"reference/lag_frequency_spectrum/#stela_toolkit.lag_frequency_spectrum.LagFrequencySpectrum.compute_lag_spectrum--parameters","title":"Parameters","text":"<p>times1, rates1 : array-like, optional     Input time and rates for the first time series. times2, rates2 : array-like, optional     Input time and rates for the second time series. subtract_coh_bias : bool, optional     Whether to subtract noise bias from coherence.</p>"},{"location":"reference/lag_frequency_spectrum/#stela_toolkit.lag_frequency_spectrum.LagFrequencySpectrum.compute_lag_spectrum--returns","title":"Returns","text":"<p>freqs : array-like     Frequency bin centers. freq_widths : array-like     Frequency bin widths. lags : array-like     Time lags at each frequency. lag_errors : array-like     Uncertainty in the lag values. cohs : array-like     Coherence values. coh_errors : array-like     Uncertainties in the coherence values.</p> Source code in <code>stela_toolkit/lag_frequency_spectrum.py</code> <pre><code>def compute_lag_spectrum(self, \n                         times1=None, rates1=None,\n                         times2=None, rates2=None,\n                         subtract_coh_bias=True):\n    \"\"\"\n    Compute the lag spectrum for a single pair of light curves or model realizations.\n\n    The phase of the cross-spectrum is converted to time lags, and uncertainties are\n    computed either from coherence (light curves) or from GP sampling (if stacked mode).\n\n    Parameters\n    ----------\n    times1, rates1 : array-like, optional\n        Input time and rates for the first time series.\n    times2, rates2 : array-like, optional\n        Input time and rates for the second time series.\n    subtract_coh_bias : bool, optional\n        Whether to subtract noise bias from coherence.\n\n    Returns\n    -------\n    freqs : array-like\n        Frequency bin centers.\n    freq_widths : array-like\n        Frequency bin widths.\n    lags : array-like\n        Time lags at each frequency.\n    lag_errors : array-like\n        Uncertainty in the lag values.\n    cohs : array-like\n        Coherence values.\n    coh_errors : array-like\n        Uncertainties in the coherence values.\n    \"\"\"\n\n    times1 = times1 if times1 is not None else self.times1\n    times2 = times2 if times2 is not None else self.times2\n    rates1 = rates1 if rates1 is not None else self.rates1\n    rates2 = rates2 if rates2 is not None else self.rates2 \n\n    lc1 = LightCurve(times=times1, rates=rates1)\n    lc2 = LightCurve(times=times2, rates=rates2)\n\n    # Compute the cross spectrum\n    cross_spectrum = CrossSpectrum(lc1, lc2,\n                                   fmin=self.fmin, fmax=self.fmax,\n                                   num_bins=self.num_bins, bin_type=self.bin_type,\n                                   bin_edges=self.bin_edges,\n                                   norm=False\n                                )\n\n    lags = np.angle(cross_spectrum.cs) / (2 * np.pi * cross_spectrum.freqs)\n\n    coherence = Coherence(lc1, lc2,\n                          fmin=self.fmin, fmax=self.fmax,\n                          num_bins=self.num_bins, bin_type=self.bin_type, bin_edges=self.bin_edges,\n                          subtract_noise_bias=subtract_coh_bias\n                        )    \n    cohs = coherence.cohs\n    coh_errors = coherence.coh_errors\n\n    num_freq = self.count_frequencies_in_bins()\n\n    phase_errors = _ClearWarnings.run(\n        lambda: np.sqrt((1 - coherence.cohs) / (2 * coherence.cohs * num_freq)),\n        explanation=\"Error from sqrt when computing (unbinned) phase errors here is common \"\n                    \"and typically due to &gt;1 coherence at the minimum frequency.\"\n    )\n\n    lag_errors = phase_errors / (2 * np.pi * cross_spectrum.freqs)\n\n    return cross_spectrum.freqs, cross_spectrum.freq_widths, lags, lag_errors, cohs, coh_errors\n</code></pre>"},{"location":"reference/lag_frequency_spectrum/#stela_toolkit.lag_frequency_spectrum.LagFrequencySpectrum.compute_stacked_lag_spectrum","title":"<code>compute_stacked_lag_spectrum()</code>","text":"<p>Compute lag-frequency spectrum for stacked GP samples.</p> <p>This method assumes the input light curves are model-generated and include multiple realizations. Returns mean and standard deviation of lag and coherence.</p>"},{"location":"reference/lag_frequency_spectrum/#stela_toolkit.lag_frequency_spectrum.LagFrequencySpectrum.compute_stacked_lag_spectrum--returns","title":"Returns","text":"<p>freqs : array-like     Frequency bin centers. freq_widths : array-like     Frequency bin widths. lags : array-like     Mean time lags across samples. lag_errors : array-like     Standard deviation of lags. cohs : array-like     Mean coherence values. coh_errors : array-like     Standard deviation of coherence values.</p> Source code in <code>stela_toolkit/lag_frequency_spectrum.py</code> <pre><code>def compute_stacked_lag_spectrum(self):\n    \"\"\"\n    Compute lag-frequency spectrum for stacked GP samples.\n\n    This method assumes the input light curves are model-generated and include\n    multiple realizations. Returns mean and standard deviation of lag and coherence.\n\n    Returns\n    -------\n    freqs : array-like\n        Frequency bin centers.\n    freq_widths : array-like\n        Frequency bin widths.\n    lags : array-like\n        Mean time lags across samples.\n    lag_errors : array-like\n        Standard deviation of lags.\n    cohs : array-like\n        Mean coherence values.\n    coh_errors : array-like\n        Standard deviation of coherence values.\n    \"\"\"\n\n    # Compute lag spectrum for each pair of realizations\n    lag_spectra = []\n    coh_spectra = []\n    for i in range(self.rates1.shape[0]):\n        lag_spectrum = self.compute_lag_spectrum(times1=self.times1, rates1=self.rates1[i],\n                                                 times2=self.times2, rates2=self.rates2[i],\n                                                 subtract_coh_bias=False\n                                                )\n        lag_spectra.append(lag_spectrum[2])\n        coh_spectra.append(lag_spectrum[4])\n\n    # Average lag spectra\n    lag_spectra_mean = np.mean(lag_spectra, axis=0)\n    lag_spectra_std = np.std(lag_spectra, axis=0)\n\n    # Average coherence spectra\n    coh_spectra_mean = np.mean(coh_spectra, axis=0)\n    coh_spectra_std = np.std(coh_spectra, axis=0)\n\n    freqs, freq_widths = lag_spectrum[0], lag_spectrum[1]\n\n    return freqs, freq_widths, lag_spectra_mean, lag_spectra_std, coh_spectra_mean, coh_spectra_std\n</code></pre>"},{"location":"reference/lag_frequency_spectrum/#stela_toolkit.lag_frequency_spectrum.LagFrequencySpectrum.count_frequencies_in_bins","title":"<code>count_frequencies_in_bins(fmin=None, fmax=None, num_bins=None, bin_type=None, bin_edges=[])</code>","text":"<p>Counts the number of frequencies in each frequency bin. Wrapper method to use FrequencyBinning.count_frequencies_in_bins with class attributes.</p> Source code in <code>stela_toolkit/lag_frequency_spectrum.py</code> <pre><code>def count_frequencies_in_bins(self, fmin=None, fmax=None, num_bins=None, bin_type=None, bin_edges=[]):\n    \"\"\"\n    Counts the number of frequencies in each frequency bin.\n    Wrapper method to use FrequencyBinning.count_frequencies_in_bins with class attributes.\n    \"\"\"\n\n    return FrequencyBinning.count_frequencies_in_bins(\n        self, fmin=fmin, fmax=fmax, num_bins=num_bins, bin_type=bin_type, bin_edges=bin_edges\n    )\n</code></pre>"},{"location":"reference/lag_frequency_spectrum/#stela_toolkit.lag_frequency_spectrum.LagFrequencySpectrum.plot","title":"<code>plot(freqs=None, freq_widths=None, lags=None, lag_errors=None, cohs=None, coh_errors=None, **kwargs)</code>","text":"<p>Plot the lag-frequency and coherence spectrum.</p>"},{"location":"reference/lag_frequency_spectrum/#stela_toolkit.lag_frequency_spectrum.LagFrequencySpectrum.plot--parameters","title":"Parameters","text":"<p>**kwargs : dict     Custom plotting arguments (xlabel, xscale, yscale, etc.).</p> Source code in <code>stela_toolkit/lag_frequency_spectrum.py</code> <pre><code>def plot(self, freqs=None, freq_widths=None, lags=None, lag_errors=None, cohs=None, coh_errors=None, **kwargs):\n    \"\"\"\n    Plot the lag-frequency and coherence spectrum.\n\n    Parameters\n    ----------\n    **kwargs : dict\n        Custom plotting arguments (xlabel, xscale, yscale, etc.).\n    \"\"\"\n    freqs = self.freqs if freqs is None else freqs\n    freq_widths = self.freq_widths if freq_widths is None else freq_widths\n    lags = self.lags if lags is None else lags\n    lag_errors = self.lag_errors if lag_errors is None else lag_errors\n    cohs = self.cohs if cohs is None else cohs\n    coh_errors = self.coh_errors if coh_errors is None else coh_errors\n\n    figsize = kwargs.get('figsize', (8, 6))\n    xlabel = kwargs.get('xlabel', 'Frequency')\n    ylabel = kwargs.get('ylabel', 'Time Lag')\n    xscale = kwargs.get('xscale', 'log')\n    yscale = kwargs.get('yscale', 'linear')\n\n    fig, (ax1, ax2) = plt.subplots(2, 1, gridspec_kw={'height_ratios': [2, 1]}, figsize=figsize, sharex=True)\n    plt.subplots_adjust(hspace=0.05)\n\n    # Lag-frequency spectrum\n    ax1.errorbar(freqs, lags, xerr=freq_widths, yerr=lag_errors, fmt='o', color='black', ms=3, lw=1.5)\n    ax1.set_xscale(xscale)\n    ax1.set_yscale(yscale)\n    ax1.set_ylabel(ylabel, fontsize=12)\n    ax1.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n    ax1.tick_params(which='both', direction='in', length=6, width=1, top=True, right=True, labelsize=12)\n\n    # Coherence spectrum\n    if cohs is not None and coh_errors is not None:\n        ax2.errorbar(freqs, cohs, xerr=freq_widths, yerr=coh_errors, fmt='o', color='black', ms=3, lw=1.5)\n        ax2.set_xscale(xscale)\n        ax2.set_ylabel('Coherence', fontsize=12)\n        ax2.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n        ax2.tick_params(which='both', direction='in', length=6, width=1, top=True, right=True, labelsize=12)\n\n    fig.text(0.5, 0.04, xlabel, ha='center', va='center', fontsize=12)\n    plt.show()\n</code></pre>"},{"location":"reference/plot/","title":"plot","text":""},{"location":"reference/plot/#stela_toolkit.plot.Plotter","title":"<code>Plotter</code>","text":"<p>Flexible wrapper around matplotlib for plotting binned or unbinned spectral results. Handles default formatting, error bars, labels, and saving.</p> Source code in <code>stela_toolkit/plot.py</code> <pre><code>class Plotter:\n    \"\"\"\n    Flexible wrapper around matplotlib for plotting binned or unbinned spectral results.\n    Handles default formatting, error bars, labels, and saving.\n    \"\"\"\n\n    @staticmethod\n    def plot(x=None, y=None, xerr=None, yerr=None, **kwargs):\n        \"\"\"\n        Generalized plotting method for spectrum-like data.\n\n        Parameters:\n        - x: Array of x-axis values (e.g., frequencies).\n        - y: Array of y-axis values (e.g., power or cross-power values).\n        - xerr: Uncertainties in the x-axis values (e.g., frequency widths).\n        - yerr: Uncertainties in the y-axis values (e.g., power uncertainties).\n        - **kwargs: Additional keyword arguments for customization.\n        \"\"\"\n\n        if xerr is not None:\n            xerr = xerr if len(list(xerr)) &gt; 0 else None\n        if yerr is not None:\n            yerr = yerr if len(list(yerr)) &gt; 0 else None\n\n        if x is None or y is None:\n            raise ValueError(\"Both 'x' and 'y' must be provided.\")\n\n        title = kwargs.get('title', None)\n\n        # Default plotting settings\n        if yerr is not None or xerr is not None:\n            default_plot_kwargs = {'color': 'black', 'fmt': 'o', 'ms': 3, 'lw': 1.5, 'label': None}\n        else:\n            default_plot_kwargs = {'color': 'black', 's': 3, 'label': None}\n\n        figsize = kwargs.get('figsize', (8, 4.5))\n        fig_kwargs = {'figsize': figsize, **kwargs.pop('fig_kwargs', {})}\n        plot_kwargs = {**default_plot_kwargs, **kwargs.pop('plot_kwargs', {})}\n        major_tick_kwargs = {'which': 'major', **kwargs.pop('major_tick_kwargs', {})}\n        minor_tick_kwargs = {'which': 'minor', **kwargs.pop('minor_tick_kwargs', {})}\n        savefig_kwargs = kwargs.pop('savefig_kwargs', {})\n        save = kwargs.pop('save', None)\n\n        plt.figure(**fig_kwargs)\n\n        if yerr is not None:\n            if xerr is not None:\n                plt.errorbar(x, y, xerr=xerr,yerr=yerr, **plot_kwargs)\n            else:\n                plt.errorbar(x, y, yerr=yerr, **plot_kwargs)\n        else:\n            if xerr is not None:\n                plt.errorbar(x, y, xerr=xerr, **plot_kwargs)\n            else:\n                plt.scatter(x, y, **plot_kwargs)\n\n        # Set labels if provided\n        xlabel = kwargs.get('xlabel', None)\n        ylabel = kwargs.get('ylabel', None)\n\n        if xlabel:\n            plt.xlabel(xlabel, fontsize=12)\n        if ylabel:\n            plt.ylabel(ylabel, fontsize=12)\n\n        plt.xscale(kwargs.get('xscale', 'linear'))\n        plt.yscale(kwargs.get('yscale', 'linear'))\n\n        # Show legend if label is provided\n        if plot_kwargs.get('label'):\n            plt.legend()\n\n        if title:\n            plt.title(title)\n\n        # Tick kwargs\n        major_tick_kwargs.setdefault('which', 'both')\n        major_tick_kwargs.setdefault('direction', 'in')\n        major_tick_kwargs.setdefault('length', 6)\n        major_tick_kwargs.setdefault('width', 1)\n        major_tick_kwargs.setdefault('labelsize', 12)\n        major_tick_kwargs.setdefault('top', True)\n        major_tick_kwargs.setdefault('right', True)\n\n        plt.tick_params(**major_tick_kwargs)\n        if len(minor_tick_kwargs) &gt; 1:\n            plt.minorticks_on()\n            plt.tick_params(**minor_tick_kwargs)\n\n        plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n\n        if save:\n            plt.savefig(save, **savefig_kwargs)\n\n        plt.show()\n</code></pre>"},{"location":"reference/plot/#stela_toolkit.plot.Plotter.plot","title":"<code>plot(x=None, y=None, xerr=None, yerr=None, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Generalized plotting method for spectrum-like data.</p> <p>Parameters: - x: Array of x-axis values (e.g., frequencies). - y: Array of y-axis values (e.g., power or cross-power values). - xerr: Uncertainties in the x-axis values (e.g., frequency widths). - yerr: Uncertainties in the y-axis values (e.g., power uncertainties). - **kwargs: Additional keyword arguments for customization.</p> Source code in <code>stela_toolkit/plot.py</code> <pre><code>@staticmethod\ndef plot(x=None, y=None, xerr=None, yerr=None, **kwargs):\n    \"\"\"\n    Generalized plotting method for spectrum-like data.\n\n    Parameters:\n    - x: Array of x-axis values (e.g., frequencies).\n    - y: Array of y-axis values (e.g., power or cross-power values).\n    - xerr: Uncertainties in the x-axis values (e.g., frequency widths).\n    - yerr: Uncertainties in the y-axis values (e.g., power uncertainties).\n    - **kwargs: Additional keyword arguments for customization.\n    \"\"\"\n\n    if xerr is not None:\n        xerr = xerr if len(list(xerr)) &gt; 0 else None\n    if yerr is not None:\n        yerr = yerr if len(list(yerr)) &gt; 0 else None\n\n    if x is None or y is None:\n        raise ValueError(\"Both 'x' and 'y' must be provided.\")\n\n    title = kwargs.get('title', None)\n\n    # Default plotting settings\n    if yerr is not None or xerr is not None:\n        default_plot_kwargs = {'color': 'black', 'fmt': 'o', 'ms': 3, 'lw': 1.5, 'label': None}\n    else:\n        default_plot_kwargs = {'color': 'black', 's': 3, 'label': None}\n\n    figsize = kwargs.get('figsize', (8, 4.5))\n    fig_kwargs = {'figsize': figsize, **kwargs.pop('fig_kwargs', {})}\n    plot_kwargs = {**default_plot_kwargs, **kwargs.pop('plot_kwargs', {})}\n    major_tick_kwargs = {'which': 'major', **kwargs.pop('major_tick_kwargs', {})}\n    minor_tick_kwargs = {'which': 'minor', **kwargs.pop('minor_tick_kwargs', {})}\n    savefig_kwargs = kwargs.pop('savefig_kwargs', {})\n    save = kwargs.pop('save', None)\n\n    plt.figure(**fig_kwargs)\n\n    if yerr is not None:\n        if xerr is not None:\n            plt.errorbar(x, y, xerr=xerr,yerr=yerr, **plot_kwargs)\n        else:\n            plt.errorbar(x, y, yerr=yerr, **plot_kwargs)\n    else:\n        if xerr is not None:\n            plt.errorbar(x, y, xerr=xerr, **plot_kwargs)\n        else:\n            plt.scatter(x, y, **plot_kwargs)\n\n    # Set labels if provided\n    xlabel = kwargs.get('xlabel', None)\n    ylabel = kwargs.get('ylabel', None)\n\n    if xlabel:\n        plt.xlabel(xlabel, fontsize=12)\n    if ylabel:\n        plt.ylabel(ylabel, fontsize=12)\n\n    plt.xscale(kwargs.get('xscale', 'linear'))\n    plt.yscale(kwargs.get('yscale', 'linear'))\n\n    # Show legend if label is provided\n    if plot_kwargs.get('label'):\n        plt.legend()\n\n    if title:\n        plt.title(title)\n\n    # Tick kwargs\n    major_tick_kwargs.setdefault('which', 'both')\n    major_tick_kwargs.setdefault('direction', 'in')\n    major_tick_kwargs.setdefault('length', 6)\n    major_tick_kwargs.setdefault('width', 1)\n    major_tick_kwargs.setdefault('labelsize', 12)\n    major_tick_kwargs.setdefault('top', True)\n    major_tick_kwargs.setdefault('right', True)\n\n    plt.tick_params(**major_tick_kwargs)\n    if len(minor_tick_kwargs) &gt; 1:\n        plt.minorticks_on()\n        plt.tick_params(**minor_tick_kwargs)\n\n    plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n\n    if save:\n        plt.savefig(save, **savefig_kwargs)\n\n    plt.show()\n</code></pre>"},{"location":"reference/power_spectrum/","title":"power_spectrum","text":""},{"location":"reference/power_spectrum/#stela_toolkit.power_spectrum.PowerSpectrum","title":"<code>PowerSpectrum</code>","text":"<p>Compute the power spectrum of a light curve using the FFT.</p> <p>This class accepts either a STELA LightCurve object or a trained GaussianProcess model. If a GaussianProcess is passed, the most recently generated samples are used.  If no samples exist, the toolkit will automatically generate 1000 posterior realizations on a 1000-point grid.</p> <p>For single light curves, the FFT is applied directly to the time series. For GP models, the power spectrum is computed for each sampled realization, and the mean and standard deviation across all samples are returned.</p> <p>Power spectra are computed in variance units by default (i.e., normalized to units of squared flux), allowing for direct interpretation in the context of variability amplitude and fractional RMS.</p> <p>Frequency binning is supported via linear, logarithmic, or user-defined bins.</p>"},{"location":"reference/power_spectrum/#stela_toolkit.power_spectrum.PowerSpectrum--parameters","title":"Parameters","text":"<p>lc_or_model : LightCurve or GaussianProcess     Input light curve or trained GP model. fmin : float or 'auto', optional     Minimum frequency to include. If 'auto', uses the lowest nonzero FFT frequency. fmax : float or 'auto', optional     Maximum frequency to include. If 'auto', uses the Nyquist frequency. num_bins : int, optional     Number of frequency bins. bin_type : str, optional     Binning type: 'log' or 'linear'. bin_edges : array-like, optional     Custom bin edges (overrides <code>num_bins</code> and <code>bin_type</code>). norm : bool, optional     Whether to normalize the power spectrum to variance units (i.e., PSD units).</p>"},{"location":"reference/power_spectrum/#stela_toolkit.power_spectrum.PowerSpectrum--attributes","title":"Attributes","text":"<p>freqs : array-like     Center frequencies of each bin. freq_widths : array-like     Bin widths for each frequency bin. powers : array-like     Power spectrum values (or mean if using GP samples). power_errors : array-like     Uncertainties in the power spectrum (std across GP samples if applicable).</p> Source code in <code>stela_toolkit/power_spectrum.py</code> <pre><code>class PowerSpectrum:\n    \"\"\"\n    Compute the power spectrum of a light curve using the FFT.\n\n    This class accepts either a STELA LightCurve object or a trained GaussianProcess model.\n    If a GaussianProcess is passed, the most recently generated samples are used. \n    If no samples exist, the toolkit will automatically generate 1000 posterior realizations\n    on a 1000-point grid.\n\n    For single light curves, the FFT is applied directly to the time series.\n    For GP models, the power spectrum is computed for each sampled realization,\n    and the mean and standard deviation across all samples are returned.\n\n    Power spectra are computed in variance units by default (i.e., normalized to units\n    of squared flux), allowing for direct interpretation in the context of variability\n    amplitude and fractional RMS.\n\n    Frequency binning is supported via linear, logarithmic, or user-defined bins.\n\n    Parameters\n    ----------\n    lc_or_model : LightCurve or GaussianProcess\n        Input light curve or trained GP model.\n    fmin : float or 'auto', optional\n        Minimum frequency to include. If 'auto', uses the lowest nonzero FFT frequency.\n    fmax : float or 'auto', optional\n        Maximum frequency to include. If 'auto', uses the Nyquist frequency.\n    num_bins : int, optional\n        Number of frequency bins.\n    bin_type : str, optional\n        Binning type: 'log' or 'linear'.\n    bin_edges : array-like, optional\n        Custom bin edges (overrides `num_bins` and `bin_type`).\n    norm : bool, optional\n        Whether to normalize the power spectrum to variance units (i.e., PSD units).\n\n    Attributes\n    ----------\n    freqs : array-like\n        Center frequencies of each bin.\n    freq_widths : array-like\n        Bin widths for each frequency bin.\n    powers : array-like\n        Power spectrum values (or mean if using GP samples).\n    power_errors : array-like\n        Uncertainties in the power spectrum (std across GP samples if applicable).\n    \"\"\"\n\n    def __init__(self,\n                 lc_or_model,\n                 fmin='auto',\n                 fmax='auto',\n                 num_bins=None,\n                 bin_type=\"log\",\n                 bin_edges=[],\n                 norm=True):\n\n        # To do: ValueError for norm=True acting on mean=0 (standardized data)\n        input_data = _CheckInputs._check_lightcurve_or_model(lc_or_model)\n        if input_data['type'] == 'model':\n            self.times, self.rates = input_data['data']\n        else:\n            self.times, self.rates, _ = input_data['data']\n        _CheckInputs._check_input_bins(num_bins, bin_type, bin_edges)\n\n        # Use absolute min and max frequencies if set to 'auto'\n        self.dt = np.diff(self.times)[0]\n        self.fmin = np.fft.rfftfreq(len(self.rates), d=self.dt)[1] if fmin == 'auto' else fmin\n        self.fmax = np.fft.rfftfreq(len(self.rates), d=self.dt)[-1] if fmax == 'auto' else fmax  # nyquist frequency\n\n        self.num_bins = num_bins\n        self.bin_type = bin_type\n        self.bin_edges = bin_edges\n\n        # if multiple light curve are provided, compute the stacked power spectrum\n        if len(self.rates.shape) == 2:\n            power_spectrum = self.compute_stacked_power_spectrum(norm=norm)\n        else:\n            power_spectrum = self.compute_power_spectrum(norm=norm)\n\n        self.freqs, self.freq_widths, self.powers, self.power_errors = power_spectrum\n\n    def compute_power_spectrum(self, times=None, rates=None, norm=True):\n        \"\"\"\n        Compute the power spectrum for a single light curve.\n\n        Applies the FFT to the light curve and optionally normalizes the result\n        to variance (PSD) units. If binning is enabled, returns binned power.\n\n        Parameters\n        ----------\n        times : array-like, optional\n            Time array to use (defaults to internal value).\n        rates : array-like, optional\n            Rate array to use (defaults to internal value).\n        norm : bool, optional\n            Whether to normalize to variance units.\n\n        Returns\n        -------\n        freqs : array-like\n            Frequencies of the power spectrum.\n        freq_widths : array-like or None\n            Bin widths (if binned).\n        powers : array-like\n            Power spectrum values.\n        power_errors : array-like or None\n            Power spectrum uncertainties (if binned).\n        \"\"\"\n\n        times = self.times if times is None else times\n        rates = self.rates if rates is None else rates\n        length = len(rates)\n\n        freqs, fft = LightCurve(times=times, rates=rates).fft()\n        powers = np.abs(fft) ** 2\n\n        # Filter frequencies within [fmin, fmax]\n        valid_mask = (freqs &gt;= self.fmin) &amp; (freqs &lt;= self.fmax)\n        freqs = freqs[valid_mask]\n        powers = powers[valid_mask]\n\n        if norm:\n            powers /= length * np.mean(rates) ** 2 / (2 * self.dt)\n\n        # Apply binning\n        if self.num_bins or self.bin_edges:\n\n            if self.bin_edges:\n                bin_edges = FrequencyBinning.define_bins(self.fmin, self.fmax, num_bins=self.num_bins, \n                                                         bin_type=self.bin_type, bin_edges=self.bin_edges\n                                                        )\n\n            elif self.num_bins:\n                bin_edges = FrequencyBinning.define_bins(self.fmin, self.fmax, num_bins=self.num_bins, bin_type=self.bin_type)\n\n            else:\n                raise ValueError(\"Either num_bins or bin_edges must be provided.\\n\"\n                                 \"In other words, you must specify the number of bins or the bin edges.\")\n\n            binned_power = FrequencyBinning.bin_data(freqs, powers, bin_edges)\n            freqs, freq_widths, powers, power_errors = binned_power\n        else:\n            freq_widths, power_errors = None, None\n\n        return freqs, freq_widths, powers, power_errors\n\n    def compute_stacked_power_spectrum(self, norm=True):\n        \"\"\"\n        Compute power spectrum for each GP sample and return the mean and std.\n        This method is used automatically when a GP model with samples is passed.\n\n        Parameters\n        ----------\n        norm : bool, optional\n            Whether to normalize to variance units.\n\n        Returns\n        -------\n        freqs : array-like\n            Frequencies of the power spectrum.\n        freq_widths : array-like\n            Widths of frequency bins.\n        power_mean : array-like\n            Mean power spectrum values.\n        power_std : array-like\n            Standard deviation of power values across realizations.\n        \"\"\"\n\n        powers = []\n        for i in range(self.rates.shape[0]):\n            power_spectrum = self.compute_power_spectrum(self.times, self.rates[i], norm=norm)\n            freqs, freq_widths, power, _ = power_spectrum\n            powers.append(power)\n\n        # Stack the collected powers and errors\n        powers = np.vstack(powers)\n        power_mean = np.mean(powers, axis=0)\n        power_std = np.std(powers, axis=0)\n\n        return freqs, freq_widths, power_mean, power_std\n\n    def plot(self, freqs=None, freq_widths=None, powers=None, power_errors=None, **kwargs):\n        \"\"\"\n        Plot the power spectrum.\n\n        Parameters\n        ----------\n        **kwargs : dict\n            Custom plotting options (xlabel, yscale, etc.).\n        \"\"\"\n\n        freqs = self.freqs if freqs is None else freqs\n        freq_widths = self.freq_widths if freq_widths is None else freq_widths\n        powers = self.powers if powers is None else powers\n        power_errors = self.power_errors if power_errors is None else power_errors\n\n        kwargs.setdefault('xlabel', 'Frequency')\n        kwargs.setdefault('ylabel', 'Power')\n        kwargs.setdefault('xscale', 'log')\n        kwargs.setdefault('yscale', 'log')\n        Plotter.plot(x=freqs, y=powers, xerr=freq_widths, yerr=power_errors, **kwargs)\n\n    def count_frequencies_in_bins(self, fmin=None, fmax=None, num_bins=None, bin_type=None, bin_edges=[]):\n        \"\"\"\n        Counts the number of frequencies in each frequency bin.\n        Wrapper method to use FrequencyBinning.count_frequencies_in_bins with class attributes.\n        \"\"\"\n\n        return FrequencyBinning.count_frequencies_in_bins(\n            self, fmin=fmin, fmax=fmax, num_bins=num_bins, bin_type=bin_type, bin_edges=bin_edges\n        )\n</code></pre>"},{"location":"reference/power_spectrum/#stela_toolkit.power_spectrum.PowerSpectrum.compute_power_spectrum","title":"<code>compute_power_spectrum(times=None, rates=None, norm=True)</code>","text":"<p>Compute the power spectrum for a single light curve.</p> <p>Applies the FFT to the light curve and optionally normalizes the result to variance (PSD) units. If binning is enabled, returns binned power.</p>"},{"location":"reference/power_spectrum/#stela_toolkit.power_spectrum.PowerSpectrum.compute_power_spectrum--parameters","title":"Parameters","text":"<p>times : array-like, optional     Time array to use (defaults to internal value). rates : array-like, optional     Rate array to use (defaults to internal value). norm : bool, optional     Whether to normalize to variance units.</p>"},{"location":"reference/power_spectrum/#stela_toolkit.power_spectrum.PowerSpectrum.compute_power_spectrum--returns","title":"Returns","text":"<p>freqs : array-like     Frequencies of the power spectrum. freq_widths : array-like or None     Bin widths (if binned). powers : array-like     Power spectrum values. power_errors : array-like or None     Power spectrum uncertainties (if binned).</p> Source code in <code>stela_toolkit/power_spectrum.py</code> <pre><code>def compute_power_spectrum(self, times=None, rates=None, norm=True):\n    \"\"\"\n    Compute the power spectrum for a single light curve.\n\n    Applies the FFT to the light curve and optionally normalizes the result\n    to variance (PSD) units. If binning is enabled, returns binned power.\n\n    Parameters\n    ----------\n    times : array-like, optional\n        Time array to use (defaults to internal value).\n    rates : array-like, optional\n        Rate array to use (defaults to internal value).\n    norm : bool, optional\n        Whether to normalize to variance units.\n\n    Returns\n    -------\n    freqs : array-like\n        Frequencies of the power spectrum.\n    freq_widths : array-like or None\n        Bin widths (if binned).\n    powers : array-like\n        Power spectrum values.\n    power_errors : array-like or None\n        Power spectrum uncertainties (if binned).\n    \"\"\"\n\n    times = self.times if times is None else times\n    rates = self.rates if rates is None else rates\n    length = len(rates)\n\n    freqs, fft = LightCurve(times=times, rates=rates).fft()\n    powers = np.abs(fft) ** 2\n\n    # Filter frequencies within [fmin, fmax]\n    valid_mask = (freqs &gt;= self.fmin) &amp; (freqs &lt;= self.fmax)\n    freqs = freqs[valid_mask]\n    powers = powers[valid_mask]\n\n    if norm:\n        powers /= length * np.mean(rates) ** 2 / (2 * self.dt)\n\n    # Apply binning\n    if self.num_bins or self.bin_edges:\n\n        if self.bin_edges:\n            bin_edges = FrequencyBinning.define_bins(self.fmin, self.fmax, num_bins=self.num_bins, \n                                                     bin_type=self.bin_type, bin_edges=self.bin_edges\n                                                    )\n\n        elif self.num_bins:\n            bin_edges = FrequencyBinning.define_bins(self.fmin, self.fmax, num_bins=self.num_bins, bin_type=self.bin_type)\n\n        else:\n            raise ValueError(\"Either num_bins or bin_edges must be provided.\\n\"\n                             \"In other words, you must specify the number of bins or the bin edges.\")\n\n        binned_power = FrequencyBinning.bin_data(freqs, powers, bin_edges)\n        freqs, freq_widths, powers, power_errors = binned_power\n    else:\n        freq_widths, power_errors = None, None\n\n    return freqs, freq_widths, powers, power_errors\n</code></pre>"},{"location":"reference/power_spectrum/#stela_toolkit.power_spectrum.PowerSpectrum.compute_stacked_power_spectrum","title":"<code>compute_stacked_power_spectrum(norm=True)</code>","text":"<p>Compute power spectrum for each GP sample and return the mean and std. This method is used automatically when a GP model with samples is passed.</p>"},{"location":"reference/power_spectrum/#stela_toolkit.power_spectrum.PowerSpectrum.compute_stacked_power_spectrum--parameters","title":"Parameters","text":"<p>norm : bool, optional     Whether to normalize to variance units.</p>"},{"location":"reference/power_spectrum/#stela_toolkit.power_spectrum.PowerSpectrum.compute_stacked_power_spectrum--returns","title":"Returns","text":"<p>freqs : array-like     Frequencies of the power spectrum. freq_widths : array-like     Widths of frequency bins. power_mean : array-like     Mean power spectrum values. power_std : array-like     Standard deviation of power values across realizations.</p> Source code in <code>stela_toolkit/power_spectrum.py</code> <pre><code>def compute_stacked_power_spectrum(self, norm=True):\n    \"\"\"\n    Compute power spectrum for each GP sample and return the mean and std.\n    This method is used automatically when a GP model with samples is passed.\n\n    Parameters\n    ----------\n    norm : bool, optional\n        Whether to normalize to variance units.\n\n    Returns\n    -------\n    freqs : array-like\n        Frequencies of the power spectrum.\n    freq_widths : array-like\n        Widths of frequency bins.\n    power_mean : array-like\n        Mean power spectrum values.\n    power_std : array-like\n        Standard deviation of power values across realizations.\n    \"\"\"\n\n    powers = []\n    for i in range(self.rates.shape[0]):\n        power_spectrum = self.compute_power_spectrum(self.times, self.rates[i], norm=norm)\n        freqs, freq_widths, power, _ = power_spectrum\n        powers.append(power)\n\n    # Stack the collected powers and errors\n    powers = np.vstack(powers)\n    power_mean = np.mean(powers, axis=0)\n    power_std = np.std(powers, axis=0)\n\n    return freqs, freq_widths, power_mean, power_std\n</code></pre>"},{"location":"reference/power_spectrum/#stela_toolkit.power_spectrum.PowerSpectrum.count_frequencies_in_bins","title":"<code>count_frequencies_in_bins(fmin=None, fmax=None, num_bins=None, bin_type=None, bin_edges=[])</code>","text":"<p>Counts the number of frequencies in each frequency bin. Wrapper method to use FrequencyBinning.count_frequencies_in_bins with class attributes.</p> Source code in <code>stela_toolkit/power_spectrum.py</code> <pre><code>def count_frequencies_in_bins(self, fmin=None, fmax=None, num_bins=None, bin_type=None, bin_edges=[]):\n    \"\"\"\n    Counts the number of frequencies in each frequency bin.\n    Wrapper method to use FrequencyBinning.count_frequencies_in_bins with class attributes.\n    \"\"\"\n\n    return FrequencyBinning.count_frequencies_in_bins(\n        self, fmin=fmin, fmax=fmax, num_bins=num_bins, bin_type=bin_type, bin_edges=bin_edges\n    )\n</code></pre>"},{"location":"reference/power_spectrum/#stela_toolkit.power_spectrum.PowerSpectrum.plot","title":"<code>plot(freqs=None, freq_widths=None, powers=None, power_errors=None, **kwargs)</code>","text":"<p>Plot the power spectrum.</p>"},{"location":"reference/power_spectrum/#stela_toolkit.power_spectrum.PowerSpectrum.plot--parameters","title":"Parameters","text":"<p>**kwargs : dict     Custom plotting options (xlabel, yscale, etc.).</p> Source code in <code>stela_toolkit/power_spectrum.py</code> <pre><code>def plot(self, freqs=None, freq_widths=None, powers=None, power_errors=None, **kwargs):\n    \"\"\"\n    Plot the power spectrum.\n\n    Parameters\n    ----------\n    **kwargs : dict\n        Custom plotting options (xlabel, yscale, etc.).\n    \"\"\"\n\n    freqs = self.freqs if freqs is None else freqs\n    freq_widths = self.freq_widths if freq_widths is None else freq_widths\n    powers = self.powers if powers is None else powers\n    power_errors = self.power_errors if power_errors is None else power_errors\n\n    kwargs.setdefault('xlabel', 'Frequency')\n    kwargs.setdefault('ylabel', 'Power')\n    kwargs.setdefault('xscale', 'log')\n    kwargs.setdefault('yscale', 'log')\n    Plotter.plot(x=freqs, y=powers, xerr=freq_widths, yerr=power_errors, **kwargs)\n</code></pre>"},{"location":"reference/preprocessing/","title":"preprocessing","text":""},{"location":"reference/preprocessing/#stela_toolkit.preprocessing.Preprocessing","title":"<code>Preprocessing</code>","text":"<p>Utility functions for cleaning and transforming light curves.</p> <p>The static methods in this class operate on LightCurve objects directly, modifying them in place unless otherwise specified.</p> <p>These methods are used throughout the STELA Toolkit to prepare light curves for Gaussian process modeling and spectral analysis. This includes:</p> <ul> <li>Standardizing light curve data (zero mean, unit variance)</li> <li>Applying and reversing a Box-Cox transformation to normalize flux distributions</li> <li>Checking for Gaussianity using the Shapiro-Wilk test and Q-Q plots</li> <li>Trimming light curves by time range</li> <li>Removing outliers using global or local IQR</li> <li>Polynomial detrending</li> <li>Handling NaNs or missing data</li> </ul> <p>Most methods automatically store relevant metadata (e.g., original mean, std, Box-Cox lambda) on the LightCurve object for later reversal.</p> <p>All methods are static and do not require instantiating this class.</p> Source code in <code>stela_toolkit/preprocessing.py</code> <pre><code>class Preprocessing:\n    \"\"\"\n    Utility functions for cleaning and transforming light curves.\n\n    The static methods in this class operate on LightCurve objects directly,\n    modifying them in place unless otherwise specified.\n\n    These methods are used throughout the STELA Toolkit to prepare light curves\n    for Gaussian process modeling and spectral analysis. This includes:\n\n    - Standardizing light curve data (zero mean, unit variance)\n    - Applying and reversing a Box-Cox transformation to normalize flux distributions\n    - Checking for Gaussianity using the Shapiro-Wilk test and Q-Q plots\n    - Trimming light curves by time range\n    - Removing outliers using global or local IQR\n    - Polynomial detrending\n    - Handling NaNs or missing data\n\n    Most methods automatically store relevant metadata (e.g., original mean, std, Box-Cox lambda)\n    on the LightCurve object for later reversal.\n\n    All methods are static and do not require instantiating this class.\n    \"\"\"\n\n    @staticmethod\n    def standardize(lightcurve):\n        \"\"\"\n        Standardize the light curve by subtracting its mean and dividing by its std.\n\n        Saves the original mean and std as attributes for future unstandardization.\n        \"\"\"\n        lc = lightcurve\n\n        # check for standardization\n        if np.isclose(lc.mean, 0, atol=1e-10) and np.isclose(lc.std, 1, atol=1e-10) or getattr(lc, \"is_standard\", False):\n            if not hasattr(lc, \"unstandard_mean\") and not hasattr(lc, \"unstandard_std\"):\n                lc.unstandard_mean = 0\n                lc.unstandard_std = 1\n            print(\"The data is already standardized.\")\n\n        # apply standardization\n        else:\n            lc.unstandard_mean = lc.mean\n            lc.unstandard_std = lc.std\n            lc.rates = (lc.rates - lc.unstandard_mean) / lc.unstandard_std\n            if lc.errors.size &gt; 0:\n                lc.errors = lc.errors / lc.unstandard_std\n\n        lc.is_standard = True # flag for detecting transformation without computation\n\n    @staticmethod\n    def unstandardize(lightcurve):\n        \"\"\"\n        Restore the light curve to its original units using stored mean and std.\n\n        This reverses a previous call to `standardize`.\n        \"\"\"\n        lc = lightcurve\n        # check that data has been standardized\n        if getattr(lc, \"is_standard\", False):\n            lc.rates = (lc.rates * lc.unstandard_std) + lc.unstandard_mean\n        else:\n            if np.isclose(lc.mean, 0, atol=1e-10) and np.isclose(lc.std, 1, atol=1e-10):\n                raise AttributeError(\n                    \"The data has not been standardized by STELA.\\n\"\n                    \"Please call the 'standardize' method first.\"\n                )\n            else:\n                raise AttributeError(\n                    \"The data is not standardized, and needs to be standardized first by STELA.\\n\"\n                    \"Please call the 'standardize' method first (e.g., Preprocessing.standardize(lightcurve)).\"\n                )\n\n        if lc.errors.size &gt; 0:\n            lc.errors = lc.errors * lc.unstandard_std\n\n        lc.is_standard = False  # reset the standardization flag\n\n    @staticmethod\n    def generate_qq_plot(lightcurve=None, rates=[]):\n        \"\"\"\n        Generate a Q-Q plot to visually assess normality.\n\n        Parameters\n        ----------\n        lightcurve : LightCurve, optional\n            Light curve to extract rates from.\n        rates : array-like, optional\n            Direct rate values if not using a LightCurve.\n        \"\"\"\n        if lightcurve:\n            rates = lightcurve.rates.copy()\n        elif np.array(rates).size != 0:\n            pass\n        else: \n            raise ValueError(\"Either 'lightcurve' or 'rates' must be provided.\")\n\n        rates_std = (rates - np.mean(rates)) / np.std(rates)\n        (osm, osr), _ = probplot(rates_std, dist=\"norm\")\n\n        plt.figure(figsize=(8, 4.5))\n        plt.plot(osm, osr, 'o', color='black', markersize=4)\n        plt.plot(osm, osm, 'g--', lw=1, label='Ideal Normal')\n\n        plt.title(\"Q-Q Plot\", fontsize=12)\n        plt.xlabel(\"Theoretical Quantiles\", fontsize=12)\n        plt.ylabel(\"Sample Quantiles\", fontsize=12)\n        plt.legend(loc='upper left')\n        plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n        plt.tick_params(which='both', direction='in', length=6, width=1, top=True, right=True, labelsize=12)\n        plt.show()\n\n    @staticmethod\n    def check_normal(lightcurve=None, rates=[], plot=True, _boxcox=False, verbose=True):\n        \"\"\"\n        Test for normality using an appropriate statistical test based on sample size.\n\n        For small samples (n &lt; 50), this uses the Shapiro-Wilk test. For larger samples,\n        it uses the Lilliefors version of the Kolmogorov-Smirnov test. Results are printed\n        with an interpretation of the strength of evidence against normality.\n\n        If `plot=True`, a Q-Q plot of the distribution is shown. This function supports either\n        a full LightCurve object or a raw array of flux values.\n\n        Parameters\n        ----------\n        lightcurve : LightCurve, optional\n            The light curve object containing the rates to test.\n        rates : array-like, optional\n            Direct rate values if not using a LightCurve.\n        plot : bool, optional\n            Whether to display a Q-Q plot.\n        _boxcox : bool, optional\n            Whether this check is being called internally after Box-Cox (affects messaging only).\n        verbose : bool, optional\n            Whether to generate print statements.\n\n        Returns\n        -------\n        is_normal : bool\n            True if the data appears normally distributed (p &gt; 0.05).\n        pvalue : float\n            The p-value from the chosen normality test.\n        \"\"\"\n\n        if lightcurve:\n            rates = lightcurve.rates.copy()\n        elif np.array(rates).size != 0:\n            rates = np.array(rates)\n        else:\n            raise ValueError(\"Either 'lightcurve' or 'rates' must be provided.\")\n\n        n = len(rates)\n        if n &lt; 50:\n            if verbose:\n                print(\"Using Shapiro-Wilk test (recommended for n &lt; 50)\")\n            test_name = \"Shapiro-Wilk\"\n            pvalue = shapiro(rates).pvalue\n        else:\n            if verbose:\n                print(\"Using Lilliefors test (for n &gt;= 50)\")\n            test_name = \"Lilliefors (modified KS)\"\n            _, pvalue = lilliefors(rates, dist='norm')\n\n        if verbose:\n            print(f\"{test_name} test p-value: {pvalue:.3g}\")\n            if pvalue &lt;= 0.001:\n                strength = \"very strong\"\n            elif pvalue &lt;= 0.01:\n                strength = \"strong\"\n            elif pvalue &lt;= 0.05:\n                strength = \"weak\"\n            else:\n                strength = \"little to no\"\n\n            print(f\"  -&gt; {strength.capitalize()} evidence against normality (p = {pvalue:.3g})\")\n\n            if pvalue &lt;= 0.05 and not _boxcox:\n                print(\"     - Consider running `check_boxcox_normal()` to see if a Box-Cox transformation can help.\")\n                print(\"     - Often checking normality via a Q-Q plot (run `generate_qq_plot(lightcurve)`) is sufficient.\")\n            print(\"===================\")\n\n        if plot:\n            Preprocessing.generate_qq_plot(rates=rates)\n\n        return pvalue &gt; 0.05, pvalue\n\n\n    @staticmethod\n    def boxcox_transform(lightcurve, save=True):\n        \"\"\"\n        Apply a Box-Cox transformation to normalize the flux distribution.\n\n        Also adjusts errors using the delta method. Stores the transformation\n        parameter lambda and sets a flag for reversal.\n\n        Parameters\n        ----------\n        lightcurve : LightCurve\n            The input light curve.\n        save : bool\n            Whether to modify the light curve in place.\n        \"\"\"\n\n        lc = lightcurve\n        rates_boxcox, lambda_opt = boxcox(lc.rates)\n\n        # transform errors using delta method (derivative-based propagation)\n        if lc.errors.size != 0:\n            if lambda_opt == 0:  # log transformation (lambda = 0)\n                errors_boxcox = lc.errors / lc.rates\n            else:\n                errors_boxcox = (lc.rates ** (lambda_opt - 1)) * lc.errors\n        else:\n            errors_boxcox = None\n\n        if save:\n            lc.rates = rates_boxcox\n            lc.errors = errors_boxcox\n            lc.lambda_boxcox = lambda_opt  # save lambda for inverse transformation\n            lc.is_boxcox_transformed = True  # flag to indicate transformation\n        else:\n            return rates_boxcox, errors_boxcox\n\n    @staticmethod\n    def reverse_boxcox_transform(lightcurve):\n        \"\"\"\n        Reverse a previously applied Box-Cox transformation.\n\n        Parameters\n        ----------\n        lightcurve : LightCurve\n            The transformed light curve.\n        \"\"\"\n\n        lc = lightcurve\n\n        if not getattr(lc, \"is_boxcox_transformed\", False):\n            raise ValueError(\"Light curve data has not been transformed with Box-Cox.\")\n\n        lambda_opt = lc.lambda_boxcox\n        if lambda_opt == 0:  # inverse log transformation\n            rates_original = np.exp(lc.rates)\n        else:\n            rates_original = (lc.rates * lambda_opt + 1) ** (1 / lambda_opt)\n\n        if lc.errors.size != 0:\n            if lambda_opt == 0:  # inverse log transformation (lambda = 0)\n                errors_original = lc.errors * rates_original\n            else:\n                errors_original = lc.errors / (rates_original ** (lambda_opt - 1))\n        else:\n            errors_original = None\n\n        lc.rates = rates_original\n        lc.errors = errors_original\n        lc.is_boxcox_transformed = False\n        del lc.lambda_boxcox\n\n    @staticmethod\n    def check_boxcox_normal(lightcurve, plot=True):\n        \"\"\"\n        Apply a Box-Cox transformation and re-test for normality using the appropriate statistical test.\n\n        This method compares the normality of the original flux distribution to its Box-Cox transformed version,\n        using either the Shapiro-Wilk or Lilliefors test depending on sample size. If `plot=True`, a Q-Q plot\n        is generated showing both the original and transformed data.\n\n        Parameters\n        ----------\n        lightcurve : LightCurve\n            The input light curve containing flux values.\n        plot : bool, optional\n            Whether to show a Q-Q plot comparing original and Box-Cox transformed distributions.\n\n        Returns\n        -------\n        is_normal : bool\n            True if the Box-Cox transformed data appears normally distributed (p &gt; 0.05).\n        pvalue : float\n            The p-value from the normality test applied to the transformed data.\n        \"\"\"\n\n        rates_original = lightcurve.rates.copy()\n        rates_boxcox, _ = Preprocessing.boxcox_transform(lightcurve, save=False)\n\n        print(\"Before Box-Cox:\")\n        print(\"----------------\")\n        Preprocessing.check_normal(lightcurve=lightcurve, plot=False)\n\n        print(\"After Box-Cox:\")\n        print(\"----------------\")\n        is_normal, pvalue = Preprocessing.check_normal(rates=rates_boxcox, plot=False, _boxcox=True)\n\n        if plot:\n            rates_original_std = (rates_original - np.mean(rates_original)) / np.std(rates_original)\n            rates_boxcox_std = (rates_boxcox - np.mean(rates_boxcox)) / np.std(rates_boxcox)\n\n            (osm1, osr1), _ = probplot(rates_original_std, dist=\"norm\")\n            (osm2, osr2), _ = probplot(rates_boxcox_std, dist=\"norm\")\n\n            plt.figure(figsize=(8, 4.5))\n            plt.plot(osm1, osr1, 'o', label='Original', color='black', alpha=0.6, markersize=4)\n            plt.plot(osm2, osr2, 'o', label='Transformed', color='dodgerblue', alpha=0.6, markersize=4)\n            plt.plot(osm1, osm1, 'g--', label='Ideal Normal', alpha=0.5, lw=1.5)\n\n            plt.xlabel(\"Theoretical Quantiles\", fontsize=12)\n            plt.ylabel(\"Sample Quantiles\", fontsize=12)\n            plt.title(\"Q-Q Plot Before and After Box-Cox\", fontsize=12)\n            plt.legend(loc='upper left')\n            plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n            plt.tick_params(which='both', direction='in', length=6, width=1, top=True, right=True, labelsize=12)\n            plt.show()\n\n        return is_normal, pvalue\n\n\n    @staticmethod\n    def trim_time_segment(lightcurve, start_time=None, end_time=None, plot=False, save=True):\n        \"\"\"\n        Trim the light curve to a given time range.\n\n        Parameters\n        ----------\n        start_time : float, optional\n            Lower time bound.\n        end_time : float, optional\n            Upper time bound.\n        plot : bool\n            Whether to plot before/after trimming.\n        save : bool\n            Whether to modify the light curve in place.\n        \"\"\"\n\n        lc = lightcurve\n\n        if start_time is None:\n            start_time = lc.times[0]\n        if end_time is None:\n            end_time = lc.times[-1]\n        if start_time and end_time is None:\n            raise ValueError(\"Please specify a start and/or end time.\")\n\n        # Apply mask to trim data\n        mask = (lc.times &gt;= start_time) &amp; (lc.times &lt;= end_time)\n        if plot:\n            plt.figure(figsize=(8, 4.5))\n            if lc.errors is not None and len(lc.errors) &gt; 0:\n                plt.errorbar(lc.times[mask], lc.rates[mask], yerr=lc.errors[mask], \n                             fmt='o', color='black', ms=3, label='Kept')\n                plt.errorbar(lc.times[~mask], lc.rates[~mask], yerr=lc.errors[~mask], \n                             fmt='o', color='orange', ms=3, label='Trimmed')\n            else:\n                plt.scatter(lc.times[mask], lc.rates[mask], s=6, color=\"black\", label=\"Kept\")\n                plt.scatter(lc.times[~mask], lc.rates[~mask], s=6, color=\"red\", label=\"Trimmed\")\n            plt.xlabel(\"Time\", fontsize=12)\n            plt.ylabel(\"Rates\", fontsize=12)\n            plt.title(\"Trimming\")\n            plt.legend()\n            plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n            plt.tick_params(which='both', direction='in', length=6, width=1, top=True, right=True, labelsize=12)\n            plt.show()\n\n        if save:\n            lc.times = lc.times[mask]\n            lc.rates = lc.rates[mask]\n            if lc.errors.size &gt; 0:\n                lc.errors = lc.errors[mask]\n\n    @staticmethod\n    def remove_nans(lightcurve, verbose=True):\n        \"\"\"\n        Remove time, rate, or error entries that are NaN.\n\n        Parameters\n        ----------\n        lightcurve : LightCurve\n            Light curve to clean.\n        verbose : bool\n            Whether to print how many NaNs were removed.\n        \"\"\"\n\n        lc = lightcurve\n        if lc.errors.size &gt; 0:\n            nonnan_mask = ~np.isnan(lc.rates) &amp; ~np.isnan(lc.times) &amp; ~np.isnan(lc.errors)\n        else:\n            nonnan_mask = ~np.isnan(lc.rates) &amp; ~np.isnan(lc.times)\n\n        if verbose:\n            print(f\"Removed {np.sum(~nonnan_mask)} NaN points.\\n\"\n                  f\"({np.sum(np.isnan(lc.rates))} NaN rates, \"\n                  f\"{np.sum(np.isnan(lc.errors))} NaN errors)\")\n            print(\"===================\")\n        lc.times = lc.times[nonnan_mask]\n        lc.rates = lc.rates[nonnan_mask]\n        if lc.errors.size &gt; 0:\n            lc.errors = lc.errors[nonnan_mask]\n\n    @staticmethod\n    def remove_outliers(lightcurve, threshold=1.5, rolling_window=None, plot=True, save=True, verbose=True):\n        \"\"\"\n        Remove outliers using the IQR method, globally or locally.\n\n        Parameters\n        ----------\n        lightcurve : LightCurve\n            The input light curve.\n        threshold : float\n            IQR multiplier.\n        rolling_window : int, optional\n            Size of local window (if local filtering is desired).\n        plot : bool\n            Whether to visualize removed points.\n        save : bool\n            Whether to modify the light curve in place.\n        verbose : bool\n            Whether to print how many points were removed.\n        \"\"\"\n\n        def plot_outliers(outlier_mask):\n            \"\"\"Plots the data flagged as outliers.\"\"\"\n            plt.figure(figsize=(8, 4.5))\n            if errors is not None:\n                plt.errorbar(times[~outlier_mask], rates[~outlier_mask], yerr=errors[~outlier_mask], \n                             fmt='o', color='black', ms=3, label='Kept')\n                plt.errorbar(times[outlier_mask], rates[outlier_mask], yerr=errors[outlier_mask], \n                             fmt='o', color='orange', ms=3, label='Outliers')\n            else:\n                plt.scatter(times[~outlier_mask], rates[~outlier_mask], \n                            s=6, color='black', label='Kept')\n                plt.scatter(times[outlier_mask], rates[outlier_mask], \n                            s=6, color='orange', label='Outliers')\n            plt.xlabel(\"Time\", fontsize=12)\n            plt.ylabel(\"Rates\", fontsize=12)\n            plt.title(\"Outlier Detection\")\n            plt.legend()\n            plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n            plt.tick_params(which='both', direction='in', length=6, width=1, top=True, right=True, labelsize=12)\n            plt.show()\n\n        def detect_outliers(rates, threshold, rolling_window):\n            if rolling_window:\n                outlier_mask = np.zeros_like(rates, dtype=bool)\n                half_window = rolling_window // 2\n                for i in range(len(rates)):\n                    start = max(0, i - half_window)\n                    end = min(len(rates), i + half_window + 1)\n                    local_data = rates[start:end]\n                    q1, q3 = np.percentile(local_data, [25, 75])\n                    iqr = q3 - q1\n                    lower_bound = q1 - threshold * iqr\n                    upper_bound = q3 + threshold * iqr\n                    if rates[i] &lt; lower_bound or rates[i] &gt; upper_bound:\n                        outlier_mask[i] = True\n            else:\n                q1, q3 = np.percentile(rates, [25, 75])\n                iqr = q3 - q1\n                lower_bound = q1 - threshold * iqr\n                upper_bound = q3 + threshold * iqr\n                outlier_mask = (rates &lt; lower_bound) | (rates &gt; upper_bound)\n            return outlier_mask\n\n        lc = deepcopy(lightcurve)\n        times = lc.times\n        rates = lc.rates\n        errors = lc.errors\n\n        outlier_mask = detect_outliers(rates, threshold=threshold, rolling_window=rolling_window)\n\n        if verbose:\n            print(f\"Removed {np.sum(outlier_mask)} outliers \"\n                  f\"({np.sum(outlier_mask) / len(rates) * 100:.2f}% of data).\")\n            print(\"===================\")\n\n        if plot:\n            plot_outliers(outlier_mask)\n\n        # Save results back to the original lightcurve if save=True\n        if save:\n            lc.times = times[~outlier_mask]\n            lc.rates = rates[~outlier_mask]\n            if errors.size &gt; 0:\n                lc.errors = errors[~outlier_mask]\n\n    @staticmethod\n    def polynomial_detrend(lightcurve, degree=1, plot=False, save=True):\n        \"\"\"\n        Remove a polynomial trend from the light curve.\n\n        Fits and subtracts a polynomial. Optionally modifies in place.\n\n        Parameters\n        ----------\n        lightcurve : LightCurve\n            The input light curve.\n        degree : int\n            Degree of the polynomial (default is 1).\n        plot : bool\n            Whether to show the trend removal visually.\n        save : bool\n            Whether to apply the change to the light curve.\n\n        Returns\n        -------\n        detrended_rates : ndarray, optional\n            Only returned if `save=False`.\n        \"\"\"\n\n        lc = deepcopy(lightcurve)\n\n        # Fit polynomial to the data\n        if lc.errors.size &gt; 0:\n            coefficients = np.polyfit(lc.times, lc.rates, degree, w=1/lc.errors)\n        else:\n            coefficients = np.polyfit(lc.times, lc.rates, degree)\n        polynomial = np.poly1d(coefficients)\n        trend = polynomial(lc.times)\n\n        detrended_rates = lc.rates - trend\n        if plot:\n            plt.figure(figsize=(8, 4.5))\n            if lc.errors is not None and len(lc.errors) &gt; 0:\n                plt.errorbar(lc.times, lc.rates, yerr=lc.errors, \n                             fmt='o', color='black', label=\"Original\", ms=3, lw=1.5, alpha=0.6)\n                plt.errorbar(lc.times, detrended_rates, yerr=lc.errors, \n                             fmt='o', color='dodgerblue', label=\"Detrended\", ms=3, lw=1.5)\n            else:\n                plt.plot(lc.times, lc.rates, label=\"Original\", color=\"black\", alpha=0.6, ms=3, lw=1.5)\n                plt.plot(lc.times, detrended_rates, label=\"Detrended\", color=\"dodgerblue\", ms=3, lw=1.5)\n            plt.plot(lc.times, trend, color='orange', linestyle='--', label='Fitted Trend')\n            plt.xlabel(\"Time\", fontsize=12)\n            plt.ylabel(\"Rates\", fontsize=12)\n            plt.title(\"Polynomial Detrending\")\n            plt.legend()\n            plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n            plt.tick_params(which='both', direction='in', length=6, width=1, top=True, right=True, labelsize=12)\n            plt.show()\n\n        if save:\n            lc.rates = detrended_rates\n</code></pre>"},{"location":"reference/preprocessing/#stela_toolkit.preprocessing.Preprocessing.boxcox_transform","title":"<code>boxcox_transform(lightcurve, save=True)</code>  <code>staticmethod</code>","text":"<p>Apply a Box-Cox transformation to normalize the flux distribution.</p> <p>Also adjusts errors using the delta method. Stores the transformation parameter lambda and sets a flag for reversal.</p>"},{"location":"reference/preprocessing/#stela_toolkit.preprocessing.Preprocessing.boxcox_transform--parameters","title":"Parameters","text":"<p>lightcurve : LightCurve     The input light curve. save : bool     Whether to modify the light curve in place.</p> Source code in <code>stela_toolkit/preprocessing.py</code> <pre><code>@staticmethod\ndef boxcox_transform(lightcurve, save=True):\n    \"\"\"\n    Apply a Box-Cox transformation to normalize the flux distribution.\n\n    Also adjusts errors using the delta method. Stores the transformation\n    parameter lambda and sets a flag for reversal.\n\n    Parameters\n    ----------\n    lightcurve : LightCurve\n        The input light curve.\n    save : bool\n        Whether to modify the light curve in place.\n    \"\"\"\n\n    lc = lightcurve\n    rates_boxcox, lambda_opt = boxcox(lc.rates)\n\n    # transform errors using delta method (derivative-based propagation)\n    if lc.errors.size != 0:\n        if lambda_opt == 0:  # log transformation (lambda = 0)\n            errors_boxcox = lc.errors / lc.rates\n        else:\n            errors_boxcox = (lc.rates ** (lambda_opt - 1)) * lc.errors\n    else:\n        errors_boxcox = None\n\n    if save:\n        lc.rates = rates_boxcox\n        lc.errors = errors_boxcox\n        lc.lambda_boxcox = lambda_opt  # save lambda for inverse transformation\n        lc.is_boxcox_transformed = True  # flag to indicate transformation\n    else:\n        return rates_boxcox, errors_boxcox\n</code></pre>"},{"location":"reference/preprocessing/#stela_toolkit.preprocessing.Preprocessing.check_boxcox_normal","title":"<code>check_boxcox_normal(lightcurve, plot=True)</code>  <code>staticmethod</code>","text":"<p>Apply a Box-Cox transformation and re-test for normality using the appropriate statistical test.</p> <p>This method compares the normality of the original flux distribution to its Box-Cox transformed version, using either the Shapiro-Wilk or Lilliefors test depending on sample size. If <code>plot=True</code>, a Q-Q plot is generated showing both the original and transformed data.</p>"},{"location":"reference/preprocessing/#stela_toolkit.preprocessing.Preprocessing.check_boxcox_normal--parameters","title":"Parameters","text":"<p>lightcurve : LightCurve     The input light curve containing flux values. plot : bool, optional     Whether to show a Q-Q plot comparing original and Box-Cox transformed distributions.</p>"},{"location":"reference/preprocessing/#stela_toolkit.preprocessing.Preprocessing.check_boxcox_normal--returns","title":"Returns","text":"<p>is_normal : bool     True if the Box-Cox transformed data appears normally distributed (p &gt; 0.05). pvalue : float     The p-value from the normality test applied to the transformed data.</p> Source code in <code>stela_toolkit/preprocessing.py</code> <pre><code>@staticmethod\ndef check_boxcox_normal(lightcurve, plot=True):\n    \"\"\"\n    Apply a Box-Cox transformation and re-test for normality using the appropriate statistical test.\n\n    This method compares the normality of the original flux distribution to its Box-Cox transformed version,\n    using either the Shapiro-Wilk or Lilliefors test depending on sample size. If `plot=True`, a Q-Q plot\n    is generated showing both the original and transformed data.\n\n    Parameters\n    ----------\n    lightcurve : LightCurve\n        The input light curve containing flux values.\n    plot : bool, optional\n        Whether to show a Q-Q plot comparing original and Box-Cox transformed distributions.\n\n    Returns\n    -------\n    is_normal : bool\n        True if the Box-Cox transformed data appears normally distributed (p &gt; 0.05).\n    pvalue : float\n        The p-value from the normality test applied to the transformed data.\n    \"\"\"\n\n    rates_original = lightcurve.rates.copy()\n    rates_boxcox, _ = Preprocessing.boxcox_transform(lightcurve, save=False)\n\n    print(\"Before Box-Cox:\")\n    print(\"----------------\")\n    Preprocessing.check_normal(lightcurve=lightcurve, plot=False)\n\n    print(\"After Box-Cox:\")\n    print(\"----------------\")\n    is_normal, pvalue = Preprocessing.check_normal(rates=rates_boxcox, plot=False, _boxcox=True)\n\n    if plot:\n        rates_original_std = (rates_original - np.mean(rates_original)) / np.std(rates_original)\n        rates_boxcox_std = (rates_boxcox - np.mean(rates_boxcox)) / np.std(rates_boxcox)\n\n        (osm1, osr1), _ = probplot(rates_original_std, dist=\"norm\")\n        (osm2, osr2), _ = probplot(rates_boxcox_std, dist=\"norm\")\n\n        plt.figure(figsize=(8, 4.5))\n        plt.plot(osm1, osr1, 'o', label='Original', color='black', alpha=0.6, markersize=4)\n        plt.plot(osm2, osr2, 'o', label='Transformed', color='dodgerblue', alpha=0.6, markersize=4)\n        plt.plot(osm1, osm1, 'g--', label='Ideal Normal', alpha=0.5, lw=1.5)\n\n        plt.xlabel(\"Theoretical Quantiles\", fontsize=12)\n        plt.ylabel(\"Sample Quantiles\", fontsize=12)\n        plt.title(\"Q-Q Plot Before and After Box-Cox\", fontsize=12)\n        plt.legend(loc='upper left')\n        plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n        plt.tick_params(which='both', direction='in', length=6, width=1, top=True, right=True, labelsize=12)\n        plt.show()\n\n    return is_normal, pvalue\n</code></pre>"},{"location":"reference/preprocessing/#stela_toolkit.preprocessing.Preprocessing.check_normal","title":"<code>check_normal(lightcurve=None, rates=[], plot=True, _boxcox=False, verbose=True)</code>  <code>staticmethod</code>","text":"<p>Test for normality using an appropriate statistical test based on sample size.</p> <p>For small samples (n &lt; 50), this uses the Shapiro-Wilk test. For larger samples, it uses the Lilliefors version of the Kolmogorov-Smirnov test. Results are printed with an interpretation of the strength of evidence against normality.</p> <p>If <code>plot=True</code>, a Q-Q plot of the distribution is shown. This function supports either a full LightCurve object or a raw array of flux values.</p>"},{"location":"reference/preprocessing/#stela_toolkit.preprocessing.Preprocessing.check_normal--parameters","title":"Parameters","text":"<p>lightcurve : LightCurve, optional     The light curve object containing the rates to test. rates : array-like, optional     Direct rate values if not using a LightCurve. plot : bool, optional     Whether to display a Q-Q plot. _boxcox : bool, optional     Whether this check is being called internally after Box-Cox (affects messaging only). verbose : bool, optional     Whether to generate print statements.</p>"},{"location":"reference/preprocessing/#stela_toolkit.preprocessing.Preprocessing.check_normal--returns","title":"Returns","text":"<p>is_normal : bool     True if the data appears normally distributed (p &gt; 0.05). pvalue : float     The p-value from the chosen normality test.</p> Source code in <code>stela_toolkit/preprocessing.py</code> <pre><code>@staticmethod\ndef check_normal(lightcurve=None, rates=[], plot=True, _boxcox=False, verbose=True):\n    \"\"\"\n    Test for normality using an appropriate statistical test based on sample size.\n\n    For small samples (n &lt; 50), this uses the Shapiro-Wilk test. For larger samples,\n    it uses the Lilliefors version of the Kolmogorov-Smirnov test. Results are printed\n    with an interpretation of the strength of evidence against normality.\n\n    If `plot=True`, a Q-Q plot of the distribution is shown. This function supports either\n    a full LightCurve object or a raw array of flux values.\n\n    Parameters\n    ----------\n    lightcurve : LightCurve, optional\n        The light curve object containing the rates to test.\n    rates : array-like, optional\n        Direct rate values if not using a LightCurve.\n    plot : bool, optional\n        Whether to display a Q-Q plot.\n    _boxcox : bool, optional\n        Whether this check is being called internally after Box-Cox (affects messaging only).\n    verbose : bool, optional\n        Whether to generate print statements.\n\n    Returns\n    -------\n    is_normal : bool\n        True if the data appears normally distributed (p &gt; 0.05).\n    pvalue : float\n        The p-value from the chosen normality test.\n    \"\"\"\n\n    if lightcurve:\n        rates = lightcurve.rates.copy()\n    elif np.array(rates).size != 0:\n        rates = np.array(rates)\n    else:\n        raise ValueError(\"Either 'lightcurve' or 'rates' must be provided.\")\n\n    n = len(rates)\n    if n &lt; 50:\n        if verbose:\n            print(\"Using Shapiro-Wilk test (recommended for n &lt; 50)\")\n        test_name = \"Shapiro-Wilk\"\n        pvalue = shapiro(rates).pvalue\n    else:\n        if verbose:\n            print(\"Using Lilliefors test (for n &gt;= 50)\")\n        test_name = \"Lilliefors (modified KS)\"\n        _, pvalue = lilliefors(rates, dist='norm')\n\n    if verbose:\n        print(f\"{test_name} test p-value: {pvalue:.3g}\")\n        if pvalue &lt;= 0.001:\n            strength = \"very strong\"\n        elif pvalue &lt;= 0.01:\n            strength = \"strong\"\n        elif pvalue &lt;= 0.05:\n            strength = \"weak\"\n        else:\n            strength = \"little to no\"\n\n        print(f\"  -&gt; {strength.capitalize()} evidence against normality (p = {pvalue:.3g})\")\n\n        if pvalue &lt;= 0.05 and not _boxcox:\n            print(\"     - Consider running `check_boxcox_normal()` to see if a Box-Cox transformation can help.\")\n            print(\"     - Often checking normality via a Q-Q plot (run `generate_qq_plot(lightcurve)`) is sufficient.\")\n        print(\"===================\")\n\n    if plot:\n        Preprocessing.generate_qq_plot(rates=rates)\n\n    return pvalue &gt; 0.05, pvalue\n</code></pre>"},{"location":"reference/preprocessing/#stela_toolkit.preprocessing.Preprocessing.generate_qq_plot","title":"<code>generate_qq_plot(lightcurve=None, rates=[])</code>  <code>staticmethod</code>","text":"<p>Generate a Q-Q plot to visually assess normality.</p>"},{"location":"reference/preprocessing/#stela_toolkit.preprocessing.Preprocessing.generate_qq_plot--parameters","title":"Parameters","text":"<p>lightcurve : LightCurve, optional     Light curve to extract rates from. rates : array-like, optional     Direct rate values if not using a LightCurve.</p> Source code in <code>stela_toolkit/preprocessing.py</code> <pre><code>@staticmethod\ndef generate_qq_plot(lightcurve=None, rates=[]):\n    \"\"\"\n    Generate a Q-Q plot to visually assess normality.\n\n    Parameters\n    ----------\n    lightcurve : LightCurve, optional\n        Light curve to extract rates from.\n    rates : array-like, optional\n        Direct rate values if not using a LightCurve.\n    \"\"\"\n    if lightcurve:\n        rates = lightcurve.rates.copy()\n    elif np.array(rates).size != 0:\n        pass\n    else: \n        raise ValueError(\"Either 'lightcurve' or 'rates' must be provided.\")\n\n    rates_std = (rates - np.mean(rates)) / np.std(rates)\n    (osm, osr), _ = probplot(rates_std, dist=\"norm\")\n\n    plt.figure(figsize=(8, 4.5))\n    plt.plot(osm, osr, 'o', color='black', markersize=4)\n    plt.plot(osm, osm, 'g--', lw=1, label='Ideal Normal')\n\n    plt.title(\"Q-Q Plot\", fontsize=12)\n    plt.xlabel(\"Theoretical Quantiles\", fontsize=12)\n    plt.ylabel(\"Sample Quantiles\", fontsize=12)\n    plt.legend(loc='upper left')\n    plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n    plt.tick_params(which='both', direction='in', length=6, width=1, top=True, right=True, labelsize=12)\n    plt.show()\n</code></pre>"},{"location":"reference/preprocessing/#stela_toolkit.preprocessing.Preprocessing.polynomial_detrend","title":"<code>polynomial_detrend(lightcurve, degree=1, plot=False, save=True)</code>  <code>staticmethod</code>","text":"<p>Remove a polynomial trend from the light curve.</p> <p>Fits and subtracts a polynomial. Optionally modifies in place.</p>"},{"location":"reference/preprocessing/#stela_toolkit.preprocessing.Preprocessing.polynomial_detrend--parameters","title":"Parameters","text":"<p>lightcurve : LightCurve     The input light curve. degree : int     Degree of the polynomial (default is 1). plot : bool     Whether to show the trend removal visually. save : bool     Whether to apply the change to the light curve.</p>"},{"location":"reference/preprocessing/#stela_toolkit.preprocessing.Preprocessing.polynomial_detrend--returns","title":"Returns","text":"<p>detrended_rates : ndarray, optional     Only returned if <code>save=False</code>.</p> Source code in <code>stela_toolkit/preprocessing.py</code> <pre><code>@staticmethod\ndef polynomial_detrend(lightcurve, degree=1, plot=False, save=True):\n    \"\"\"\n    Remove a polynomial trend from the light curve.\n\n    Fits and subtracts a polynomial. Optionally modifies in place.\n\n    Parameters\n    ----------\n    lightcurve : LightCurve\n        The input light curve.\n    degree : int\n        Degree of the polynomial (default is 1).\n    plot : bool\n        Whether to show the trend removal visually.\n    save : bool\n        Whether to apply the change to the light curve.\n\n    Returns\n    -------\n    detrended_rates : ndarray, optional\n        Only returned if `save=False`.\n    \"\"\"\n\n    lc = deepcopy(lightcurve)\n\n    # Fit polynomial to the data\n    if lc.errors.size &gt; 0:\n        coefficients = np.polyfit(lc.times, lc.rates, degree, w=1/lc.errors)\n    else:\n        coefficients = np.polyfit(lc.times, lc.rates, degree)\n    polynomial = np.poly1d(coefficients)\n    trend = polynomial(lc.times)\n\n    detrended_rates = lc.rates - trend\n    if plot:\n        plt.figure(figsize=(8, 4.5))\n        if lc.errors is not None and len(lc.errors) &gt; 0:\n            plt.errorbar(lc.times, lc.rates, yerr=lc.errors, \n                         fmt='o', color='black', label=\"Original\", ms=3, lw=1.5, alpha=0.6)\n            plt.errorbar(lc.times, detrended_rates, yerr=lc.errors, \n                         fmt='o', color='dodgerblue', label=\"Detrended\", ms=3, lw=1.5)\n        else:\n            plt.plot(lc.times, lc.rates, label=\"Original\", color=\"black\", alpha=0.6, ms=3, lw=1.5)\n            plt.plot(lc.times, detrended_rates, label=\"Detrended\", color=\"dodgerblue\", ms=3, lw=1.5)\n        plt.plot(lc.times, trend, color='orange', linestyle='--', label='Fitted Trend')\n        plt.xlabel(\"Time\", fontsize=12)\n        plt.ylabel(\"Rates\", fontsize=12)\n        plt.title(\"Polynomial Detrending\")\n        plt.legend()\n        plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n        plt.tick_params(which='both', direction='in', length=6, width=1, top=True, right=True, labelsize=12)\n        plt.show()\n\n    if save:\n        lc.rates = detrended_rates\n</code></pre>"},{"location":"reference/preprocessing/#stela_toolkit.preprocessing.Preprocessing.remove_nans","title":"<code>remove_nans(lightcurve, verbose=True)</code>  <code>staticmethod</code>","text":"<p>Remove time, rate, or error entries that are NaN.</p>"},{"location":"reference/preprocessing/#stela_toolkit.preprocessing.Preprocessing.remove_nans--parameters","title":"Parameters","text":"<p>lightcurve : LightCurve     Light curve to clean. verbose : bool     Whether to print how many NaNs were removed.</p> Source code in <code>stela_toolkit/preprocessing.py</code> <pre><code>@staticmethod\ndef remove_nans(lightcurve, verbose=True):\n    \"\"\"\n    Remove time, rate, or error entries that are NaN.\n\n    Parameters\n    ----------\n    lightcurve : LightCurve\n        Light curve to clean.\n    verbose : bool\n        Whether to print how many NaNs were removed.\n    \"\"\"\n\n    lc = lightcurve\n    if lc.errors.size &gt; 0:\n        nonnan_mask = ~np.isnan(lc.rates) &amp; ~np.isnan(lc.times) &amp; ~np.isnan(lc.errors)\n    else:\n        nonnan_mask = ~np.isnan(lc.rates) &amp; ~np.isnan(lc.times)\n\n    if verbose:\n        print(f\"Removed {np.sum(~nonnan_mask)} NaN points.\\n\"\n              f\"({np.sum(np.isnan(lc.rates))} NaN rates, \"\n              f\"{np.sum(np.isnan(lc.errors))} NaN errors)\")\n        print(\"===================\")\n    lc.times = lc.times[nonnan_mask]\n    lc.rates = lc.rates[nonnan_mask]\n    if lc.errors.size &gt; 0:\n        lc.errors = lc.errors[nonnan_mask]\n</code></pre>"},{"location":"reference/preprocessing/#stela_toolkit.preprocessing.Preprocessing.remove_outliers","title":"<code>remove_outliers(lightcurve, threshold=1.5, rolling_window=None, plot=True, save=True, verbose=True)</code>  <code>staticmethod</code>","text":"<p>Remove outliers using the IQR method, globally or locally.</p>"},{"location":"reference/preprocessing/#stela_toolkit.preprocessing.Preprocessing.remove_outliers--parameters","title":"Parameters","text":"<p>lightcurve : LightCurve     The input light curve. threshold : float     IQR multiplier. rolling_window : int, optional     Size of local window (if local filtering is desired). plot : bool     Whether to visualize removed points. save : bool     Whether to modify the light curve in place. verbose : bool     Whether to print how many points were removed.</p> Source code in <code>stela_toolkit/preprocessing.py</code> <pre><code>@staticmethod\ndef remove_outliers(lightcurve, threshold=1.5, rolling_window=None, plot=True, save=True, verbose=True):\n    \"\"\"\n    Remove outliers using the IQR method, globally or locally.\n\n    Parameters\n    ----------\n    lightcurve : LightCurve\n        The input light curve.\n    threshold : float\n        IQR multiplier.\n    rolling_window : int, optional\n        Size of local window (if local filtering is desired).\n    plot : bool\n        Whether to visualize removed points.\n    save : bool\n        Whether to modify the light curve in place.\n    verbose : bool\n        Whether to print how many points were removed.\n    \"\"\"\n\n    def plot_outliers(outlier_mask):\n        \"\"\"Plots the data flagged as outliers.\"\"\"\n        plt.figure(figsize=(8, 4.5))\n        if errors is not None:\n            plt.errorbar(times[~outlier_mask], rates[~outlier_mask], yerr=errors[~outlier_mask], \n                         fmt='o', color='black', ms=3, label='Kept')\n            plt.errorbar(times[outlier_mask], rates[outlier_mask], yerr=errors[outlier_mask], \n                         fmt='o', color='orange', ms=3, label='Outliers')\n        else:\n            plt.scatter(times[~outlier_mask], rates[~outlier_mask], \n                        s=6, color='black', label='Kept')\n            plt.scatter(times[outlier_mask], rates[outlier_mask], \n                        s=6, color='orange', label='Outliers')\n        plt.xlabel(\"Time\", fontsize=12)\n        plt.ylabel(\"Rates\", fontsize=12)\n        plt.title(\"Outlier Detection\")\n        plt.legend()\n        plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n        plt.tick_params(which='both', direction='in', length=6, width=1, top=True, right=True, labelsize=12)\n        plt.show()\n\n    def detect_outliers(rates, threshold, rolling_window):\n        if rolling_window:\n            outlier_mask = np.zeros_like(rates, dtype=bool)\n            half_window = rolling_window // 2\n            for i in range(len(rates)):\n                start = max(0, i - half_window)\n                end = min(len(rates), i + half_window + 1)\n                local_data = rates[start:end]\n                q1, q3 = np.percentile(local_data, [25, 75])\n                iqr = q3 - q1\n                lower_bound = q1 - threshold * iqr\n                upper_bound = q3 + threshold * iqr\n                if rates[i] &lt; lower_bound or rates[i] &gt; upper_bound:\n                    outlier_mask[i] = True\n        else:\n            q1, q3 = np.percentile(rates, [25, 75])\n            iqr = q3 - q1\n            lower_bound = q1 - threshold * iqr\n            upper_bound = q3 + threshold * iqr\n            outlier_mask = (rates &lt; lower_bound) | (rates &gt; upper_bound)\n        return outlier_mask\n\n    lc = deepcopy(lightcurve)\n    times = lc.times\n    rates = lc.rates\n    errors = lc.errors\n\n    outlier_mask = detect_outliers(rates, threshold=threshold, rolling_window=rolling_window)\n\n    if verbose:\n        print(f\"Removed {np.sum(outlier_mask)} outliers \"\n              f\"({np.sum(outlier_mask) / len(rates) * 100:.2f}% of data).\")\n        print(\"===================\")\n\n    if plot:\n        plot_outliers(outlier_mask)\n\n    # Save results back to the original lightcurve if save=True\n    if save:\n        lc.times = times[~outlier_mask]\n        lc.rates = rates[~outlier_mask]\n        if errors.size &gt; 0:\n            lc.errors = errors[~outlier_mask]\n</code></pre>"},{"location":"reference/preprocessing/#stela_toolkit.preprocessing.Preprocessing.reverse_boxcox_transform","title":"<code>reverse_boxcox_transform(lightcurve)</code>  <code>staticmethod</code>","text":"<p>Reverse a previously applied Box-Cox transformation.</p>"},{"location":"reference/preprocessing/#stela_toolkit.preprocessing.Preprocessing.reverse_boxcox_transform--parameters","title":"Parameters","text":"<p>lightcurve : LightCurve     The transformed light curve.</p> Source code in <code>stela_toolkit/preprocessing.py</code> <pre><code>@staticmethod\ndef reverse_boxcox_transform(lightcurve):\n    \"\"\"\n    Reverse a previously applied Box-Cox transformation.\n\n    Parameters\n    ----------\n    lightcurve : LightCurve\n        The transformed light curve.\n    \"\"\"\n\n    lc = lightcurve\n\n    if not getattr(lc, \"is_boxcox_transformed\", False):\n        raise ValueError(\"Light curve data has not been transformed with Box-Cox.\")\n\n    lambda_opt = lc.lambda_boxcox\n    if lambda_opt == 0:  # inverse log transformation\n        rates_original = np.exp(lc.rates)\n    else:\n        rates_original = (lc.rates * lambda_opt + 1) ** (1 / lambda_opt)\n\n    if lc.errors.size != 0:\n        if lambda_opt == 0:  # inverse log transformation (lambda = 0)\n            errors_original = lc.errors * rates_original\n        else:\n            errors_original = lc.errors / (rates_original ** (lambda_opt - 1))\n    else:\n        errors_original = None\n\n    lc.rates = rates_original\n    lc.errors = errors_original\n    lc.is_boxcox_transformed = False\n    del lc.lambda_boxcox\n</code></pre>"},{"location":"reference/preprocessing/#stela_toolkit.preprocessing.Preprocessing.standardize","title":"<code>standardize(lightcurve)</code>  <code>staticmethod</code>","text":"<p>Standardize the light curve by subtracting its mean and dividing by its std.</p> <p>Saves the original mean and std as attributes for future unstandardization.</p> Source code in <code>stela_toolkit/preprocessing.py</code> <pre><code>@staticmethod\ndef standardize(lightcurve):\n    \"\"\"\n    Standardize the light curve by subtracting its mean and dividing by its std.\n\n    Saves the original mean and std as attributes for future unstandardization.\n    \"\"\"\n    lc = lightcurve\n\n    # check for standardization\n    if np.isclose(lc.mean, 0, atol=1e-10) and np.isclose(lc.std, 1, atol=1e-10) or getattr(lc, \"is_standard\", False):\n        if not hasattr(lc, \"unstandard_mean\") and not hasattr(lc, \"unstandard_std\"):\n            lc.unstandard_mean = 0\n            lc.unstandard_std = 1\n        print(\"The data is already standardized.\")\n\n    # apply standardization\n    else:\n        lc.unstandard_mean = lc.mean\n        lc.unstandard_std = lc.std\n        lc.rates = (lc.rates - lc.unstandard_mean) / lc.unstandard_std\n        if lc.errors.size &gt; 0:\n            lc.errors = lc.errors / lc.unstandard_std\n\n    lc.is_standard = True # flag for detecting transformation without computation\n</code></pre>"},{"location":"reference/preprocessing/#stela_toolkit.preprocessing.Preprocessing.trim_time_segment","title":"<code>trim_time_segment(lightcurve, start_time=None, end_time=None, plot=False, save=True)</code>  <code>staticmethod</code>","text":"<p>Trim the light curve to a given time range.</p>"},{"location":"reference/preprocessing/#stela_toolkit.preprocessing.Preprocessing.trim_time_segment--parameters","title":"Parameters","text":"<p>start_time : float, optional     Lower time bound. end_time : float, optional     Upper time bound. plot : bool     Whether to plot before/after trimming. save : bool     Whether to modify the light curve in place.</p> Source code in <code>stela_toolkit/preprocessing.py</code> <pre><code>@staticmethod\ndef trim_time_segment(lightcurve, start_time=None, end_time=None, plot=False, save=True):\n    \"\"\"\n    Trim the light curve to a given time range.\n\n    Parameters\n    ----------\n    start_time : float, optional\n        Lower time bound.\n    end_time : float, optional\n        Upper time bound.\n    plot : bool\n        Whether to plot before/after trimming.\n    save : bool\n        Whether to modify the light curve in place.\n    \"\"\"\n\n    lc = lightcurve\n\n    if start_time is None:\n        start_time = lc.times[0]\n    if end_time is None:\n        end_time = lc.times[-1]\n    if start_time and end_time is None:\n        raise ValueError(\"Please specify a start and/or end time.\")\n\n    # Apply mask to trim data\n    mask = (lc.times &gt;= start_time) &amp; (lc.times &lt;= end_time)\n    if plot:\n        plt.figure(figsize=(8, 4.5))\n        if lc.errors is not None and len(lc.errors) &gt; 0:\n            plt.errorbar(lc.times[mask], lc.rates[mask], yerr=lc.errors[mask], \n                         fmt='o', color='black', ms=3, label='Kept')\n            plt.errorbar(lc.times[~mask], lc.rates[~mask], yerr=lc.errors[~mask], \n                         fmt='o', color='orange', ms=3, label='Trimmed')\n        else:\n            plt.scatter(lc.times[mask], lc.rates[mask], s=6, color=\"black\", label=\"Kept\")\n            plt.scatter(lc.times[~mask], lc.rates[~mask], s=6, color=\"red\", label=\"Trimmed\")\n        plt.xlabel(\"Time\", fontsize=12)\n        plt.ylabel(\"Rates\", fontsize=12)\n        plt.title(\"Trimming\")\n        plt.legend()\n        plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n        plt.tick_params(which='both', direction='in', length=6, width=1, top=True, right=True, labelsize=12)\n        plt.show()\n\n    if save:\n        lc.times = lc.times[mask]\n        lc.rates = lc.rates[mask]\n        if lc.errors.size &gt; 0:\n            lc.errors = lc.errors[mask]\n</code></pre>"},{"location":"reference/preprocessing/#stela_toolkit.preprocessing.Preprocessing.unstandardize","title":"<code>unstandardize(lightcurve)</code>  <code>staticmethod</code>","text":"<p>Restore the light curve to its original units using stored mean and std.</p> <p>This reverses a previous call to <code>standardize</code>.</p> Source code in <code>stela_toolkit/preprocessing.py</code> <pre><code>@staticmethod\ndef unstandardize(lightcurve):\n    \"\"\"\n    Restore the light curve to its original units using stored mean and std.\n\n    This reverses a previous call to `standardize`.\n    \"\"\"\n    lc = lightcurve\n    # check that data has been standardized\n    if getattr(lc, \"is_standard\", False):\n        lc.rates = (lc.rates * lc.unstandard_std) + lc.unstandard_mean\n    else:\n        if np.isclose(lc.mean, 0, atol=1e-10) and np.isclose(lc.std, 1, atol=1e-10):\n            raise AttributeError(\n                \"The data has not been standardized by STELA.\\n\"\n                \"Please call the 'standardize' method first.\"\n            )\n        else:\n            raise AttributeError(\n                \"The data is not standardized, and needs to be standardized first by STELA.\\n\"\n                \"Please call the 'standardize' method first (e.g., Preprocessing.standardize(lightcurve)).\"\n            )\n\n    if lc.errors.size &gt; 0:\n        lc.errors = lc.errors * lc.unstandard_std\n\n    lc.is_standard = False  # reset the standardization flag\n</code></pre>"}]}