{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p> Logo design by Elizabeth Jarboe </p>"},{"location":"#welcome-to-the-stela-toolkit","title":"Welcome to the STELA Toolkit","text":"<p>The STELA Toolkit is a Python package for interpolating astrophysical light curves using Gaussian Processes (more ML models to come!) in order to compute dta products in the frequency domain (power spectra, lag spectra, coherence, etc.) and in the standard time domain, like lags from the cross correlation function. </p> <p>STELA is designed for:</p> <ul> <li>Astronomers dealing with irregular, noisy light curve data</li> <li>Users who want to generate Fourier-based data products (lag spectra, coherence spectra, power spectra, cross spectra)</li> <li>... or standard time-domain products, like lags with CCF and linear ICCF.</li> </ul> <p>Get started:</p> <ul> <li>Overview \u2014 What STELA can do and how it works</li> <li>Tutorial \u2014 Hands-on with GP modeling, interpolation, and lag analysis</li> <li>Gaussian Processes \u2014 Background, intuition, and implementation</li> <li>Module Reference \u2014 Technical docs and API</li> </ul> <p>If you're new to GP modeling, the overview, tutorial, and intro to GPs are the best places to begin.</p>"},{"location":"gaussian_process_intro/","title":"Introduction to Gaussian Processes","text":""},{"location":"gaussian_process_intro/#a-gaussian-process-gp-is-one-of-the-most-powerful-tools-in-bayesian-statistics-for-modeling-functions-in-stela-we-use-gps-to-interpolate-noisy-irregularly-sampled-light-curves-in-order-to-generate-frequency-resolved-data-products-using-fourier-techniques","title":"A Gaussian Process (GP) is one of the most powerful tools in Bayesian statistics for modeling functions. In STELA, we use GPs to interpolate noisy, irregularly sampled light curves in order to generate frequency-resolved data products using Fourier techniques.","text":""},{"location":"gaussian_process_intro/#1-from-parametric-to-nonparametric-models","title":"1. From Parametric to Nonparametric Models","text":"<p>In classical modeling approaches\u2014like linear regression\u2014we assume a specific functional form:</p> \\[ y = X \\beta + \\epsilon \\] <p>This is a parametric model, where the function is fully described by a finite set of parameters such as \\( \\beta \\). But what if we don\u2019t know the correct form of the function? What if the variability is too complex to capture with some formula of y = f(X)?</p>"},{"location":"gaussian_process_intro/#gaussian-processes-a-nonparametric-prior","title":"Gaussian Processes: A Nonparametric Prior","text":"<p>A Gaussian Process (GP) is a nonparametric Bayesian model: instead of defining a formula with a small number of parameters, it places a prior directly over functions themselves, instead letting the data inform the shape of the function.</p> <p>A GP defines a distribution such that...</p> <p>Any finite collection of function values should follow a multivariate normal distribution.</p> <p>This means:</p> <ul> <li>Each time point \\( t \\) corresponds to a random variable \\( f(t) \\)</li> <li>Any set \\( [f(t_1), \\dots, f(t_n)] \\) is jointly Gaussian</li> <li>The function's behavior is therefore, like any Gaussian, entirely determined by its mean and covariance</li> </ul> <p>This flexibility allows the GP to capture complex trends and variability directly from the data.</p> <p>Because the GP framework assumes normally distributed data, it's important that the observed fluxes approximate Gaussianity. STELA includes both hypothesis testing to check this assumption, as well as a Box-Cox transformation to help normalize the data before modeling. After inference, we invert the transformation so that predictions are returned in the original flux units.</p>"},{"location":"gaussian_process_intro/#2-mean-and-covariancekernel-functions","title":"2. Mean and Covariance/Kernel Functions","text":"<p>A GP is fully defined by:</p> <ul> <li>A mean function: \\( m(t) = \\mathbb{E}[f(t)] \\)</li> <li>A covariance function (or kernel): \\( k(t, t') = \\text{Cov}(f(t), f(t')) \\)</li> </ul> <p>This is written as:</p> \\[ f(t) \\sim \\mathcal{GP}(m(t), k(t, t')) \\]"},{"location":"gaussian_process_intro/#in-stela","title":"In STELA:","text":"<ul> <li>We assume \\( m(t) = 0 \\) by standardizing the light curve data before modeling.</li> <li>The covariance structure is encoded via a kernel, which determines the shape, smoothness, and periodicity of the model. Different kernels encode different assumptions:</li> <li>Smooth trends (e.g. RBF)</li> <li>Quasi-periodic variability (e.g. Spectral Mixture)</li> <li>Long- and short-term variability (e.g. Rational Quadratic)</li> </ul> <p>This combination of nonparametric flexibility and probabilistic structure is what enables STELA to robustly interpolate, sample, and propagate uncertainty across all downstream Fourier analyses.</p>"},{"location":"gaussian_process_intro/#3-kernel-functions","title":"3. Kernel Functions","text":"<p>The kernel defines the relationship between any two inputs. Common kernels include:</p> <ul> <li>RBF (Squared Exponential)</li> </ul> <p>$$   k(t, t') = \\sigma^2 \\exp\\left(-\\frac{(t - t')^2}{2\\ell^2}\\right)   $$</p> <ul> <li>Matern (\u00bd, 3/2, 5/2)</li> <li>Rational Quadratic</li> <li>Spectral Mixture</li> </ul> <p>Each kernel has hyperparameters:</p> <ul> <li>\\( \\ell \\): length scale</li> <li>\\( \\sigma \\): output variance</li> <li>For spectral: mixture weights, frequencies</li> </ul>"},{"location":"gaussian_process_intro/#4-noise-handling","title":"4. Noise Handling","text":"<p>STELA handles noise in two ways:</p> <ul> <li>Explicit error bars on light curve points (heteroscedastic noise).</li> <li>White noise model:</li> </ul> <p>Adds a diagonal term \\( \\sigma_w^2 I \\) to the kernel for unaccounted stochastic noise on top of the uncertainties (homoscedastic noise).</p>"},{"location":"gaussian_process_intro/#5-training-the-gp","title":"5. Training the GP","text":"<p>We don\u2019t sample hyperparameters, we optimize (\"train\") them by maximizing the marginal likelihood:</p> \\[ p(\\mathbf{y} \\mid \\theta) = \\mathcal{N}(0, K_\\theta + \\sigma_n^2 I) \\] <p>Its log form:</p> \\[ \\log p(\\mathbf{y}) = -\\frac{1}{2} \\mathbf{y}^\\top K^{-1} \\mathbf{y} - \\frac{1}{2} \\log |K| - \\frac{n}{2} \\log(2\\pi) \\] <p>The three terms correspond to different aspects of model performance, respectively:</p> <ul> <li>Data fit: how well the model explains the data</li> <li>Complexity penalty: penalizes overfitting</li> <li>Normalization of the Gaussian</li> </ul> <p>STELA minimizes the Negative Log Marginal Likelihood (NLML) using the Adam optimizer. The step size and number of steps to take (number of iterations) can be varied to ensure convergence of the final kernel hyperparameters. Use <code>plot_training=True</code> and/or <code>verbose=True</code> to check for convergence.</p>"},{"location":"gaussian_process_intro/#6-bayesian-inference-and-gp-predictions","title":"6. Bayesian Inference and GP Predictions","text":""},{"location":"gaussian_process_intro/#goal","title":"Goal:","text":"<p>Given noisy data \\( \\mathbf{y} \\) at times \\( \\mathbf{t} \\), predict \\( f_* \\) at new times \\( \\mathbf{t}_* \\).</p> <p>We start with the prior:</p> \\[ \\begin{bmatrix} \\mathbf{y} \\\\ f_* \\end{bmatrix} \\sim \\mathcal{N}\\left(0,  \\begin{bmatrix} K + \\sigma_n^2 I &amp; K_*^\\top \\\\ K_* &amp; K_{**} \\end{bmatrix} \\right) \\] <p>Where:</p> <ul> <li>\\( K \\): covariance of training points</li> <li>\\( K_* \\): covariance between training and test</li> <li>\\( K_{**} \\): covariance of test points</li> <li>\\( \\sigma_n^2 \\): noise variance</li> </ul>"},{"location":"gaussian_process_intro/#posterior-prediction-vs-samples","title":"Posterior Prediction vs. Samples","text":"<p>In our GP framework, we distinguish between the posterior prediction (via <code>.predict</code>) and posterior samples (via <code>.sample</code>). Both are derived from the GP posterior but serve different purposes:</p> <p>Prediction (<code>predict</code>) computes the posterior mean and covariance of the function at new time points:</p> \\[ \\mathbb{E}[f_*] = K_*^\\top (K + \\sigma_n^2 I)^{-1} \\mathbf{y} \\] \\[ \\text{Cov}[f_*] = K_{**} - K_*^\\top (K + \\sigma_n^2 I)^{-1} K_* \\] <p>This provides a Gaussian distribution over function values at the desired points, with uncertainty encoded in the posterior covariance.</p> <p>Samples (<code>sample</code>) draw individual function realizations from this posterior distribution. These are full, self-consistent samples of the latent function, incorporating both the posterior mean and covariance structure.</p> <p>In our use case, we use samples to compute the Fourier-domain products such as power spectra, cross-spectra, and frequency-resolved time lags. This allows for propagating the uncertainty in the modeling through to the downstream quantities of interest in a fully Bayesian manner.</p>"},{"location":"installation/","title":"Installation","text":"<p>Not ready yet.</p>"},{"location":"overview/","title":"STELA Toolkit Overview","text":""},{"location":"overview/#stela-sampling-time-for-even-lightcurve-analysis-is-a-python-package-for-interpolating-astrophysical-light-curves-using-gaussian-processes-more-ml-models-to-come-in-order-to-compute-frequency-domain-and-standard-time-domain-data-products","title":"STELA (Sampling Time for Even Lightcurve Analysis) is a Python package for interpolating astrophysical light curves using Gaussian Processes (more ML models to come!) in order to compute frequency-domain and standard time domain data products.","text":""},{"location":"overview/#core-workflow","title":"Core Workflow","text":""},{"location":"overview/#1-load-and-inspect-your-light-curve","title":"1. Load and Inspect Your Light Curve","text":"<p>STELA begins with the <code>LightCurve</code> class, which allows users to load irregularly or regularly sampled light curves from formats including FITS, CSV, and plain text. These objects are used directly in downstream analyses or as input to Gaussian Process models.</p>"},{"location":"overview/#2-preprocess-and-clean-the-data","title":"2. Preprocess and Clean the Data","text":"<p>STELA includes robust preprocessing tools that:</p> <ul> <li>Check for normality using Shapiro-Wilk or Lilliefors tests, chosen automatically based on sample size</li> <li>Apply Box-Cox transformations to approximate Gaussianity</li> <li>Standardize the light curve to zero mean and unit variance</li> <li>Remove NaNs and outliers</li> <li>Detrend via polynomial fitting and trimming</li> <li>Visualize Gaussianity using Q-Q plots</li> </ul> <p>All transformations can either overwrite the original light curve (<code>save=True</code>).</p>"},{"location":"overview/#3-gaussian-process-gp-modeling","title":"3. Gaussian Process (GP) Modeling","text":"<p>STELA uses Gaussian Processes to model AGN light curves in a Bayesian framework:</p> <ul> <li> <p>Functions for testing normality assumption via Shapiro-Wilk or Lilliefors tests, which is chosen automatically based on sample size</p> <ul> <li>Box-Cox transformation transforms the data to reach normality by optimizing a parameter from the data</li> </ul> </li> <li> <p>Fits kernel hyperparameters by minimizing the negative log marginal likelihood (NLML)</p> </li> <li> <p>Allows easy selection among multiple kernel types, including automatic kernel selection by comparing post-trained kernel Akaike information criterion (AIC):</p> <ul> <li>Radial Basis Function (RBF)</li> <li>Rational Quadratic (RQ)</li> <li>Matern (\u00bd, 3/2, 5/2)</li> <li>Spectral Mixture (for advanced periodic behavior)</li> </ul> </li> <li> <p>Supports white noise fitting alongside observational error</p> </li> </ul> <p>After training, you can generate posterior samples or predictions at any times of interest, enabling accurate interpolation as well as uncertainty propagation by drawing more and more evenly sampled realizations/samples to compute the data product of interest.</p>"},{"location":"overview/#2-frequency-domain-tools","title":"2. Frequency-Domain Tools","text":"<p>Taking inputs of either a trained GP model, or evenly-sampled data defined using the <code>LightCurve</code> class, STELA can compute:</p> <ul> <li>Power Spectrum: Measures the amount of variability/power at different frequencies (normalized periodogram).</li> <li>Cross Spectrum: Measures the relationship between two light curves via both real and imaginary parts</li> <li>Lag-Frequency Spectrum: Time delay as a function of frequency, from the phase of the cross spectrum</li> <li>Lag-Energy Spectrum: Time lag across energy bands</li> <li>Coherence Spectrum: Quantifies how correlated the signal is at each frequency. Bias due to noise can be optionally accounted for.</li> </ul> <p>When Gaussian Process realizations are used, STELA performs Monte Carlo sampling to derive uncertainties: Uncertainty is propagated by computing the data product of interest for each pair of realizations, resulting in a distribution of the final quantity in each frequency bin.</p>"},{"location":"overview/#5-time-domain-lag-analysis","title":"5. Time-Domain Lag Analysis","text":"<p>STELA provides two approaches for time-domain lag estimation:</p> <ul> <li>Interpolated Cross-Correlation (ICCF): </li> <li>Linearly interpolates one light curve onto another</li> <li>Computes peak and centroid lag</li> <li> <p>Uses Monte Carlo simulations for uncertainty estimation via redrawing flux values</p> </li> <li> <p>GP-Based Cross-Correlation:</p> </li> <li>Computes lag distributions across many GP realizations</li> <li>Outputs mean and standard deviation of peak and centroid lags</li> </ul>"},{"location":"overview/#6-light-curve-simulation","title":"6. Light Curve Simulation","text":"<p>STELA allows for simulating synthetic light curves using the method of Timmer and Konig:</p> <ul> <li>Specified power spectral properties (e.g., power-law slopes)</li> <li>Injected time lags with configurable impulse response functions</li> <li>Control over sampling, noise level, and structure</li> </ul> <p>These simulations are useful for benchmarking recovery of lags and variability structure.</p>"},{"location":"overview/#unified-api-design","title":"Unified API Design","text":"<p>Every major class in STELA (e.g., <code>PowerSpectrum</code>, <code>LagFrequencySpectrum</code>, <code>GaussianProcess</code>) includes:</p> <ul> <li><code>.plot()</code> method with easily visualizing results</li> <li> <p>Accepts inputs of either raw <code>LightCurve</code> objects or trained <code>GaussianProcess</code> models.</p> <ul> <li>If samples have not been previously generated for a Gaussian process model, STELA will do so itself, generating 1000 samples on an evenly sampled time grid of 1000 points.</li> </ul> </li> </ul>"},{"location":"overview/#next-steps","title":"Next Steps","text":"<ul> <li>Install STELA</li> <li>Understand Gaussian Processes</li> <li>Run the tutorial notebook</li> <li>Explore the module reference</li> </ul>"},{"location":"tutorial/","title":"Tutorial","text":"In\u00a0[\u00a0]: Copied! <pre># Import STELA\nfrom stela_toolkit import *\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n%load_ext autoreload\n%autoreload 2\n</pre> # Import STELA from stela_toolkit import *  import numpy as np import matplotlib.pyplot as plt %load_ext autoreload %autoreload 2 In\u00a0[\u00a0]: Copied! <pre># Option 1: Load light curve directly from a text file\nlightcurve = LightCurve(file_path='../data/NGC5548_U_swift.dat')\n\n# Option 2: Or from arrays\ndata = np.genfromtxt(f'../data/NGC5548_X_swift.dat')\nlightcurve = LightCurve(times=data[:, 0], rates=data[:, 1], errors=data[:, 2])\n</pre> # Option 1: Load light curve directly from a text file lightcurve = LightCurve(file_path='../data/NGC5548_U_swift.dat')  # Option 2: Or from arrays data = np.genfromtxt(f'../data/NGC5548_X_swift.dat') lightcurve = LightCurve(times=data[:, 0], rates=data[:, 1], errors=data[:, 2]) In\u00a0[\u00a0]: Copied! <pre># Basic plot\nlightcurve.plot()\n\n# Customized plot\nlightcurve.plot(\n    figsize=(8, 4),\n    xlabel='Modified Heliocentric Julian Date (Days)',\n    ylabel='Flux',\n    xlim=(lightcurve.times[0], lightcurve.times[-1]),\n    title='NGC 5548 U-band Light Curve',\n    plot_kwargs={'color': 'purple', 'fmt': 'o', 'lw': 1, 'ms': 3},\n    major_tick_kwargs={'direction': 'in', 'top': True, 'right': True, 'length': 6, 'width': 1},\n    minor_tick_kwargs={'direction': 'in', 'top': True, 'right': True, 'length': 3, 'width': 0.5}\n    # save='my_plot.png', savefig_kwargs={'dpi': 300}\n)\n</pre> # Basic plot lightcurve.plot()  # Customized plot lightcurve.plot(     figsize=(8, 4),     xlabel='Modified Heliocentric Julian Date (Days)',     ylabel='Flux',     xlim=(lightcurve.times[0], lightcurve.times[-1]),     title='NGC 5548 U-band Light Curve',     plot_kwargs={'color': 'purple', 'fmt': 'o', 'lw': 1, 'ms': 3},     major_tick_kwargs={'direction': 'in', 'top': True, 'right': True, 'length': 6, 'width': 1},     minor_tick_kwargs={'direction': 'in', 'top': True, 'right': True, 'length': 3, 'width': 0.5}     # save='my_plot.png', savefig_kwargs={'dpi': 300} ) In\u00a0[\u00a0]: Copied! <pre># Remove any NaNs in the time, flux, or error arrays\nPreprocessing.remove_nans(lightcurve)\n\n# Explore outlier removal using a rolling IQR window, or a global IQR (set rolling_window = None)\nPreprocessing.remove_outliers(lightcurve, threshold=1.5, rolling_window=10, plot=True, verbose=True, save=False)\nPreprocessing.remove_outliers(lightcurve, threshold=1.5, rolling_window=50, plot=True, verbose=True, save=False)\n\n# Trim the light curve to a specific observing window\nPreprocessing.trim_time_segment(lightcurve, end_time=56815, plot=True, save=False)\n\n# Detrend the light curve using a polynomial fit (here, degree 3)\nPreprocessing.polynomial_detrend(lightcurve, degree=3, plot=True, save=False)\n\n# Standardize the light curve (zero mean, unit variance)\nPreprocessing.standardize(lightcurve)\nPreprocessing.unstandardize(lightcurve)\n</pre> # Remove any NaNs in the time, flux, or error arrays Preprocessing.remove_nans(lightcurve)  # Explore outlier removal using a rolling IQR window, or a global IQR (set rolling_window = None) Preprocessing.remove_outliers(lightcurve, threshold=1.5, rolling_window=10, plot=True, verbose=True, save=False) Preprocessing.remove_outliers(lightcurve, threshold=1.5, rolling_window=50, plot=True, verbose=True, save=False)  # Trim the light curve to a specific observing window Preprocessing.trim_time_segment(lightcurve, end_time=56815, plot=True, save=False)  # Detrend the light curve using a polynomial fit (here, degree 3) Preprocessing.polynomial_detrend(lightcurve, degree=3, plot=True, save=False)  # Standardize the light curve (zero mean, unit variance) Preprocessing.standardize(lightcurve) Preprocessing.unstandardize(lightcurve) In\u00a0[\u00a0]: Copied! <pre># the Q-Q plot already appears reasonable, although the tails here seem nonnormal, or our normal is skewed\nPreprocessing.generate_qq_plot(lightcurve)\n\n# test indicates that the data is not normal\nPreprocessing.check_normal(lightcurve, plot=False)\n\n# our boxcox transformation helps!\nPreprocessing.check_boxcox_normal(lightcurve, plot=True)\n</pre> # the Q-Q plot already appears reasonable, although the tails here seem nonnormal, or our normal is skewed Preprocessing.generate_qq_plot(lightcurve)  # test indicates that the data is not normal Preprocessing.check_normal(lightcurve, plot=False)  # our boxcox transformation helps! Preprocessing.check_boxcox_normal(lightcurve, plot=True) In\u00a0[\u00a0]: Copied! <pre>gp_model = GaussianProcess(lightcurve, \n                           kernel_form=\"Matern32\", \n                           white_noise=True, \n                           enforce_normality=True, \n                           run_training=False\n                        )\n</pre> gp_model = GaussianProcess(lightcurve,                             kernel_form=\"Matern32\",                             white_noise=True,                             enforce_normality=True,                             run_training=False                         ) In\u00a0[\u00a0]: Copied! <pre># Train the model manually after initialization\ngp_model.train(\n    num_iter=500,        # Number of training steps\n    learn_rate=0.1,      # Step size for optimizer\n    plot=True,           # Show NLML evolution\n    verbose=True         # Print progress info\n)\n\n# Alternatively: train automatically during initialization\ngp_model = GaussianProcess(lightcurve, \n                           kernel_form=\"Matern32\", \n                           white_noise=True, \n                           enforce_normality=True, \n                           run_training=True,\n                           plot_training=False,\n                           num_iter=500,        \n                           learn_rate=0.1, \n                           verbose=False)\n\n# To view the hyperparameters of a model at a later time, use the get_hyperparameters method\ngp_model.get_hyperparameters()\n\n# It is good practice to save the model, so that you can load it later for consistency\n# Both can be done with the package's save_model and load_model methods\n# gp_model.save_model('gp_model.pkl')\n# gp_model.load_model('gp_model.pkl')\n</pre> # Train the model manually after initialization gp_model.train(     num_iter=500,        # Number of training steps     learn_rate=0.1,      # Step size for optimizer     plot=True,           # Show NLML evolution     verbose=True         # Print progress info )  # Alternatively: train automatically during initialization gp_model = GaussianProcess(lightcurve,                             kernel_form=\"Matern32\",                             white_noise=True,                             enforce_normality=True,                             run_training=True,                            plot_training=False,                            num_iter=500,                                    learn_rate=0.1,                             verbose=False)  # To view the hyperparameters of a model at a later time, use the get_hyperparameters method gp_model.get_hyperparameters()  # It is good practice to save the model, so that you can load it later for consistency # Both can be done with the package's save_model and load_model methods # gp_model.save_model('gp_model.pkl') # gp_model.load_model('gp_model.pkl') In\u00a0[\u00a0]: Copied! <pre># Option 1: Let STELA select automatically\n# Will take a bit longer than normal to train, assess all the models\ngp_model = GaussianProcess(lightcurve,\n                           kernel_form=\"auto\", # consider all kernels available\n                         # kernel_form=[\"RBF\", \"Matern32\", \"RQ\", \"Matern52\"],\n                           white_noise=True, \n                           enforce_normality=False, \n                        )  \n\n# Option 2: Compare kernels manually\ngp1 = GaussianProcess(lightcurve, kernel_form=\"RBF\", run_training=True)\ngp2 = GaussianProcess(lightcurve, kernel_form=\"Matern32\", run_training=True)\n\nprint(\"Kernel Statistics (lower is better for AIC/BIC):\")\nprint(\"===============================================\")\n\nprint(f\"{'Kernel':&lt;12} {'AIC':&gt;10} {'BIC':&gt;10}\")\nprint(\"-\" * 34)\nprint(f\"{'RBF':&lt;12} {gp1.aic():&gt;10.3f} {gp1.bic():&gt;10.3f}\")\nprint(f\"{'Matern32':&lt;12} {gp2.aic():&gt;10.3f} {gp2.bic():&gt;10.3f}\")\n</pre> # Option 1: Let STELA select automatically # Will take a bit longer than normal to train, assess all the models gp_model = GaussianProcess(lightcurve,                            kernel_form=\"auto\", # consider all kernels available                          # kernel_form=[\"RBF\", \"Matern32\", \"RQ\", \"Matern52\"],                            white_noise=True,                             enforce_normality=False,                          )    # Option 2: Compare kernels manually gp1 = GaussianProcess(lightcurve, kernel_form=\"RBF\", run_training=True) gp2 = GaussianProcess(lightcurve, kernel_form=\"Matern32\", run_training=True)  print(\"Kernel Statistics (lower is better for AIC/BIC):\") print(\"===============================================\")  print(f\"{'Kernel':&lt;12} {'AIC':&gt;10} {'BIC':&gt;10}\") print(\"-\" * 34) print(f\"{'RBF':&lt;12} {gp1.aic():&gt;10.3f} {gp1.bic():&gt;10.3f}\") print(f\"{'Matern32':&lt;12} {gp2.aic():&gt;10.3f} {gp2.bic():&gt;10.3f}\") In\u00a0[\u00a0]: Copied! <pre># Visualize the model's prediction, uncertainty, and a sample realization\ngp_model.plot()\n\n# Define a regular time grid for prediction and sampling\nnew_times = np.linspace(lightcurve.times[0], lightcurve.times[-1], 1000, dtype=np.float64)\n\n# Draw 1000 posterior samples from the GP on this grid\n# Each realization is a row in the resulting array\nsamples = gp_model.sample(new_times, num_samples=1000)\n\n# Compute the posterior mean and 2-sigma confidence bounds\npredict_mean, lower, upper = gp_model.predict(new_times)\n</pre> # Visualize the model's prediction, uncertainty, and a sample realization gp_model.plot()  # Define a regular time grid for prediction and sampling new_times = np.linspace(lightcurve.times[0], lightcurve.times[-1], 1000, dtype=np.float64)  # Draw 1000 posterior samples from the GP on this grid # Each realization is a row in the resulting array samples = gp_model.sample(new_times, num_samples=1000)  # Compute the posterior mean and 2-sigma confidence bounds predict_mean, lower, upper = gp_model.predict(new_times) In\u00a0[\u00a0]: Copied! <pre># Compute the power spectrum of the light curve\n# norm=True (default) for normalization consistent with PSD, otherwise periodogram\nps = PowerSpectrum(gp_model, fmin='auto', fmax='auto', num_bins=8, bin_type='log', norm=True)\nps.plot()\n\n# Access the frequency and power arrays for your own use\nfreqs = ps.freqs\nfreq_widths = ps.freq_widths\npowers = ps.powers\npower_errors = ps.power_errors\n\n# Show how many frequencies are in each bin\nprint(\"Number of frequencies per bin:\")\nprint(\"===============================\")\nprint(ps.count_frequencies_in_bins())\n</pre> # Compute the power spectrum of the light curve # norm=True (default) for normalization consistent with PSD, otherwise periodogram ps = PowerSpectrum(gp_model, fmin='auto', fmax='auto', num_bins=8, bin_type='log', norm=True) ps.plot()  # Access the frequency and power arrays for your own use freqs = ps.freqs freq_widths = ps.freq_widths powers = ps.powers power_errors = ps.power_errors  # Show how many frequencies are in each bin print(\"Number of frequencies per bin:\") print(\"===============================\") print(ps.count_frequencies_in_bins()) In\u00a0[\u00a0]: Copied! <pre>lightcurve_uvw2 = LightCurve(file_path=\"../data/NGC5548_UVW2_swift.dat\")\ngp_model_uvw2 = GaussianProcess(lightcurve_uvw2,\n                           kernel_form=\"auto\", # consider all kernels available\n                           white_noise=True, \n                           enforce_normality=False, \n                        )\n</pre> lightcurve_uvw2 = LightCurve(file_path=\"../data/NGC5548_UVW2_swift.dat\") gp_model_uvw2 = GaussianProcess(lightcurve_uvw2,                            kernel_form=\"auto\", # consider all kernels available                            white_noise=True,                             enforce_normality=False,                          ) In\u00a0[\u00a0]: Copied! <pre># Generate samples using the same time grid as the other band (important!!)\ngp_model_uvw2.sample(new_times, num_samples=1000)\n\n# Compute the cross spectrum between two light curves or GP models\ncs = CrossSpectrum(gp_model, gp_model_uvw2, \n                   fmin='auto', fmax='auto',\n                   num_bins=8, bin_type='log')\ncs.plot()\n\n# Compute the lag-frequency spectrum between two bands\n# Positive lag indicates that the first light curve LAGS the second\nlag_freq = LagFrequencySpectrum(gp_model, gp_model_uvw2,\n                                fmin='auto', fmax='auto',\n                                num_bins=8, bin_type='log')\nlag_freq.plot()\n\n# Compute the coherence spectrum\ncoh = Coherence(gp_model, gp_model_uvw2, \n                fmin='auto', fmax='auto',\n                num_bins=8, bin_type='log')\ncoh.plot()\n</pre> # Generate samples using the same time grid as the other band (important!!) gp_model_uvw2.sample(new_times, num_samples=1000)  # Compute the cross spectrum between two light curves or GP models cs = CrossSpectrum(gp_model, gp_model_uvw2,                     fmin='auto', fmax='auto',                    num_bins=8, bin_type='log') cs.plot()  # Compute the lag-frequency spectrum between two bands # Positive lag indicates that the first light curve LAGS the second lag_freq = LagFrequencySpectrum(gp_model, gp_model_uvw2,                                 fmin='auto', fmax='auto',                                 num_bins=8, bin_type='log') lag_freq.plot()  # Compute the coherence spectrum coh = Coherence(gp_model, gp_model_uvw2,                  fmin='auto', fmax='auto',                 num_bins=8, bin_type='log') coh.plot()"},{"location":"tutorial/#welcome-to-the-stela-toolkit","title":"Welcome to the STELA Toolkit!\u00b6","text":"<p>The STELA Toolkit is designed for studying variability in light curves, mainly through powerful frequency-domain data products, including power and cross spectra, time lags, and coherences.</p> <p>By using Gaussian process (GP) modeling to interpolate uneven light curves onto a regular time grid, STELA allows you to carry out Fourier-based analyses as if your data were perfectly sampled. That said, if your data is already regularly sampled, you can use STELA just as easily\u2014GPs are completely optional.</p> <p>In addition to frequency-domain tools, STELA also supports time-domain lag analysis using the cross-correlation function (CCF). For irregularly sampled data, you can choose between modeling the light curves with GPs or using the widely adopted interpolated cross-correlation function (ICCF) method, which linearly interpolates one time series onto the other\u2019s grid.</p> <p>STELA was designed to be convenient and intuitive, regardless of your background in Python or statistics. If you ever run into trouble\u2014whether it\u2019s a bug, something confusing, or a suggestion for how STELA could serve you better\u2014I\u2019d love to hear from you. Please open an issue on the GitHub Issues Page, or feel free to email me directly: Collin Lewin (clewin@mit.edu).</p>"},{"location":"tutorial/#in-this-tutorial-youll-learn-how-to","title":"In this tutorial, you\u2019ll learn how to...\u00b6","text":"<ol> <li>Import, clean, and plot time series data</li> <li>Model variability using Gaussian processes (GPs)</li> <li>Predict new values of the time series at previously unobserved times</li> <li>Generate powerful frequency-domain and lag-based data products</li> <li>Simulate light curves with custom variability and response/lag properties</li> </ol>"},{"location":"tutorial/#importing-your-data","title":"Importing Your Data\u00b6","text":"<p>All core functionality in the STELA Toolkit begins with time series data, which we represent using the <code>LightCurve</code> class. You can create a <code>LightCurve</code> object in one of two ways:</p> <ol> <li><p>From a file STELA supports text-based formats like <code>.dat</code>, <code>.txt</code>, and <code>.csv</code>, as well as FITS files. By default, it assumes the first three columns in the file contain:</p> <ul> <li>Time</li> <li>Measured values (e.g., flux or count rate)</li> <li>Measurement uncertainties</li> </ul> <p>If your file uses different columns, you can specify which ones to use with the <code>file_columns</code> argument.</p> </li> <li><p>From NumPy arrays You can also construct a light curve directly by passing arrays for time, values, and uncertainties.</p> </li> </ol> <p>Tip: Once you\u2019ve created a <code>LightCurve</code>, you can pass it to any STELA analysis tool: modeling, frequency-domain transforms, or lag computations.</p> <p>Below, we\u2019ll load and plot data from the AGN NGC 5548, observed by the Swift Observatory as part of the AGN STORM campaign.</p>"},{"location":"tutorial/#plotting-data-and-results","title":"Plotting Data and Results\u00b6","text":"<p>STELA makes it easy to preview and customize plots of your light curve\u2014and not just light curves. Every main class in the toolkit (from power spectra to lag measurements) includes a <code>.plot()</code> method with a consistent structure, so once you learn it, you can apply it everywhere.</p> <p>Almost every visual element of the plot can be customized using keyword arguments:</p> <ul> <li>Basic plot settings like <code>xlabel</code>, <code>ylabel</code>, <code>title</code>, <code>figsize</code>, <code>xscale</code>, and <code>yscale</code></li> <li>Advanced customization via <code>plot_kwargs</code>, <code>fig_kwargs</code>, <code>tick_kwargs</code>, etc.</li> <li>Save options using <code>save</code> and <code>save_kwargs</code></li> </ul> <p>This makes it easy to generate quick diagnostic plots or high-quality figures for publications.</p> <p>Let\u2019s start with a simple plot, followed by a fully customized one.</p>"},{"location":"tutorial/#data-preprocessing-and-cleaning","title":"Data Preprocessing and Cleaning\u00b6","text":"<p>Before modeling or computing spectral products, it\u2019s often helpful to inspect and clean your light curve. STELA provides a number of tools to help you do this safely and flexibly.</p> <p>All preprocessing tools live in the <code>Preprocessing</code> class. You don\u2019t need to create an instance\u2014just call the methods directly.</p> <p>Most of these functions let you:</p> <ul> <li>Visualize changes with <code>plot=True</code></li> <li>Avoid committing changes by setting <code>save=False</code> (recommended for first passes)</li> <li>Clean data without modifying the original file or LightCurve object</li> </ul> <p>Common tasks include:</p> <ul> <li><code>remove_nans()</code> \u2014 drop entries with missing data</li> <li><code>remove_outliers()</code> \u2014 exclude extreme points using the interquartile range (IQR)</li> <li><code>trim_time_segment()</code> \u2014 keep only part of the light curve (e.g., for campaign segmentation)</li> <li><code>polynomial_detrend()</code> \u2014 subtract long-term trends before analysis</li> <li><code>standardize()</code> and <code>unstandardize()</code> \u2014 mean-center and rescale your data</li> </ul> <p>Here\u2019s how to explore these features in practice:</p>"},{"location":"tutorial/#checking-for-normality","title":"Checking for Normality\u00b6","text":"<p>Before fitting a Gaussian Process model, it's important to check whether your light curve's flux distribution is approximately normal. Gaussian processes assume the data is drawn from a Gaussian distribution, so significant departures from normality can hurt performance or lead to unstable fits.</p> <p>STELA gives you a few tools in the <code>Preprocessing</code> class to assess and correct this via transformations:</p> <ul> <li><code>generate_qq_plot()</code>: Plots a Q-Q (quantile-quantile) comparison of your data against a normal distribution. If the points fall roughly on a straight 1:1 line, the data is reasonably normal.</li> <li><code>check_normal()</code>: Performs a formal statistical test for normality at significance level of 0.05. STELA automatically chooses the most appropriate test depending on your sample size, using Shapiro-Wilk for small sample (n&lt;50) and Lilliefors for larger ones (n&gt;50). A Q-Q plot can also be produced here using <code>plot=True</code>.</li> <li><code>check_boxcox_normal()</code>: Applies a Box-Cox transformation and re-runs the normality test to see the degree to which the transformation improves normality. A Q-Q plot with both the original and transformed data overlaid can also be produced here using <code>plot=True</code>.</li> </ul> <p>Note: The Box-Cox transform is a power-law transformation that reshapes the data to better resemble a normal distribution, and STELA automatically optimizes the transformation parameter (\u03bb) for your dataset.</p> <p>Don't want to do all this beforehand? Use the <code>enforce_normality=True</code> in the <code>GaussianProcess</code> class (see below).</p> <p>Let\u2019s try it out and see how our light curve looks.</p>"},{"location":"tutorial/#gaussian-process-models-in-stela","title":"Gaussian Process Models in STELA\u00b6","text":"<p>To model light curve variability in STELA, we use the <code>GaussianProcess</code> class. In summary, a Gaussian Process (GP) is a flexible, non-parametric model that treats your data as samples drawn from a multivariate normal distribution, with covariance determined by a kernel function. This allows us to infer what the light curve may have looked like between or beyond the observed times, while also accounting for uncertainty in a principled way.</p> <p>If you\u2019d rather not check for normality yourself (as in the previous section), simply set <code>enforce_normality=True</code>. STELA will automatically assess whether your light curve\u2019s flux distribution is sufficiently Gaussian, and if not, apply a Box-Cox transformation. It will then recheck the transformed data to confirm whether normality has improved.</p> <p>Want to learn more on GPs? Read the Introduction to Gaussian Processes part of the documentation (under construction, this won't work yet) to understand the theory behind what we\u2019re doing here.</p> <p>In STELA, you pass your <code>LightCurve</code> object to the <code>GaussianProcess</code> class to create the model.</p>"},{"location":"tutorial/#kernel-functions","title":"Kernel Functions\u00b6","text":"<p>The covariance function\u2014or kernel\u2014controls how the model expects different points in time to relate to one another. Later in the tutorial we introduce method for how to go about choosing a functional form. STELA includes several kernel options, specified via the <code>kernel_form</code> argument:</p> <ol> <li>Radial Basis Function (RBF) \u2013 very smooth</li> <li>Rational Quadratic (RQ) \u2013 similar to RBF, but with variable smoothness and an additional hyperparameter</li> <li>Matern kernels \u2013 less smooth; controlled by \u03bd:<ul> <li><code>Matern12</code> (\u03bd = 1/2)</li> <li><code>Matern32</code> (\u03bd = 3/2)</li> <li><code>Matern52</code> (\u03bd = 5/2)</li> </ul> <p>(Note: \u03bd is a fixed kernel parameter, not a hyperparameter to be optimized)</p> </li> <li>Spectral Mixture kernel \u2013 captures periodic or quasi-periodic behavior<ul> <li>Syntax: <code>\"SpectralMixture, N\"</code> sets the number of mixtures to <code>N</code> (e.g., <code>\"SpectralMixture, 4\"</code>)</li> </ul> </li> </ol>"},{"location":"tutorial/#error-and-noise-handling","title":"Error and Noise Handling\u00b6","text":"<p>STELA\u2019s GP models automatically incorporate your observational errors if you\u2019ve provided them in the <code>LightCurve</code>. These act as fixed noise levels for each data point in the likelihood.</p> <p>In addition, you can optionally fit an extra white noise component to account for unmodeled variability. This is enabled via <code>white_noise=True</code>.</p>"},{"location":"tutorial/#setting-up-a-gp-model","title":"Setting Up a GP Model\u00b6","text":"<p>You can create a <code>GaussianProcess</code> model by passing in your <code>LightCurve</code> object, along with options for kernel choice, noise modeling, and normality checking. This step only sets up the model\u2014it doesn\u2019t train it yet.</p> <p>Let\u2019s initialize a model and prepare it for training, although we could just set <code>run_training=True</code> for future reference.</p>"},{"location":"tutorial/#training-the-model","title":"Training the Model\u00b6","text":"<p>Once your Gaussian Process model is initialized, you can train it to fit your data. This step adjusts the kernel\u2019s hyperparameters (e.g., length scale, amplitude, white noise) so the model best represents the variability in your light curve.</p> <p>Training is performed by minimizing the Negative Log Marginal Likelihood (NLML) \u2014 a standard loss function for GPs. It measures how likely your observed data is under the current model.</p> <p>More precisely, the marginal likelihood is the probability of observing your data given the kernel hyperparameters (e.g., length scale, amplitude), after integrating over all possible functions the GP could represent. Lower NLML means the model is assigning higher probability to your observed data.</p>"},{"location":"tutorial/#running-training","title":"Running Training\u00b6","text":"<p>To train your model, call the <code>.train()</code> method on your <code>GaussianProcess</code> object. This adjusts the kernel\u2019s hyperparameters so the model best fits your light curve.</p> <p>If you don\u2019t know much about optimization, don\u2019t worry! While the default training settings (<code>learn_rate=0.1</code>, <code>num_iter=500</code>) are usually sufficient for most light curves and kernels, it's good practice to plot the NLML:</p> <p>Recommended: Set <code>plot_training=True</code>. This will show you how the training loss (NLML) evolves. You want to see the curve decrease and then flatten out by the end \u2014 that means the model has reached a stable fit. Otherwise see the subsection below.</p>"},{"location":"tutorial/#training-parameters","title":"Training Parameters\u00b6","text":"<ul> <li><code>num_iter</code>: how many training steps to take (optional, default 500)</li> <li><code>learn_rate</code>: how fast the optimizer updates the parameters (optional, default 0.1)</li> <li><code>plot</code>: show a plot of the loss improving over time (optional, default False)</li> <li><code>verbose</code>: print optimization details (optional, default False)</li> </ul>"},{"location":"tutorial/#what-to-try-if-things-dont-look-right","title":"What to Try if Things Don\u2019t Look Right\u00b6","text":"<ul> <li>If the loss curve is very slowly decreasing, try increasing <code>learn_rate</code> (e.g., to <code>0.3</code> or <code>0.5</code>)</li> <li>If the curve is bouncy or erratic, lower <code>learn_rate</code> (e.g., to <code>0.01</code>)</li> <li>If the curve is still going down at the end, increase <code>num_iter</code> (e.g., to <code>1000</code>)</li> </ul>"},{"location":"tutorial/#selecting-a-kernel-form","title":"Selecting a Kernel Form\u00b6","text":"<p>When you create a Gaussian Process model, a key decision is which kernel to use. The kernel affects how the model captures the observed variability, including how smooth, erratic, or periodic the signal is.</p> <p>This choice directly affects how well the GP can capture the underlying structure of your light curve and thus should be assessed on each light curve independently.</p>"},{"location":"tutorial/#option-1-let-stela-select-automatically","title":"Option 1: Let STELA Select Automatically\u00b6","text":"<p>If you're unsure which kernel is best, you can pass <code>'auto'</code> or a list of kernel names to <code>kernel_form</code>. STELA will try each one, train a model for each (even if <code>run_training=False</code>), and select the best based on Akaike Information Criterion (AIC). AIC rewards higher likelihood, and punishes model complexity based on the number of kernel hyperparameters.</p>"},{"location":"tutorial/#option-2-compare-kernels-manually","title":"Option 2: Compare Kernels Manually\u00b6","text":"<p>If you'd like to explore different kernels yourself, you can train them one at a time and compare their AIC/BIC scores manually.</p>"},{"location":"tutorial/#predicting-missing-data-in-the-gaps","title":"Predicting Missing Data in the Gaps\u00b6","text":"<p>Once your Gaussian Process model is trained, there are two main ways to evaluate it:</p>"},{"location":"tutorial/#1-predict-posterior-mean-and-standard-deviation","title":"1. <code>.predict()</code> : Posterior Mean and Standard Deviation\u00b6","text":"<p>The <code>.predict()</code> method gives you the posterior mean and standard deviation of the GP at each time point. These represent the best guess of the true underlying light curve and the uncertainty in that guess.</p> <p>This is useful for:</p> <ul> <li>Visualizing the smoothed light curve</li> <li>Understanding where the model is confident vs. uncertain</li> </ul> <p>But: we don't typically use <code>.predict()</code> results directly in downstream analyses like power spectrum or lag estimation, because those calculations require full realizations of plausible light curves \u2014 not just the mean prediction.</p>"},{"location":"tutorial/#2-sample-draw-posterior-realizations","title":"2. <code>.sample()</code> : Draw Posterior Realizations\u00b6","text":"<p>The <code>.sample()</code> method draws full realizations from the trained GP\u2019s posterior distribution \u2014 that is, plausible versions of what the true light curve could have looked like, given your data and the learned model.</p> <p>These samples allow STELA to intuitively propagate uncertainties from the GP method onto the data products below:</p> <p>Specifically, instead of computing these data products just once, STELA:</p> <ol> <li>Computes the result for each individual realization</li> <li>Aggregates the results across all samples</li> <li>Reports the mean and spread (standard deviation) of the final measurement</li> </ol>"},{"location":"tutorial/#how-samples-are-used-internally","title":"How Samples Are Used Internally\u00b6","text":"<p>Many STELA classes \u2014 including:</p> <ul> <li><code>PowerSpectrum</code></li> <li><code>LagFrequencySpectrum</code></li> <li><code>CrossSpectrum</code></li> <li><code>Coherence</code></li> <li><code>CrossCorrelation</code> (for GP mode)</li> </ul> <p>will automatically use the most recently generated samples from the model when you use the model as an input in these classes. You do not need to pass the samples in manually.</p> <p>If you don\u2019t generate samples yourself, STELA will do it for you upon input of the model into any of the classes above (default: 1000 samples).</p>"},{"location":"tutorial/#frequency-resolved-analysis-tools","title":"Frequency-Resolved Analysis Tools\u00b6","text":"<p>STELA includes several classes for computing main frequency-resolved data products. These tools help quantify variability, correlation, and time delays between light curves across different timescales/temporal frequencies.</p> <p>All of the classes listed below follow a similar interface and workflow:</p>"},{"location":"tutorial/#shared-structure-and-workflow","title":"Shared Structure and Workflow\u00b6","text":"<p>Each class takes as input:</p> <ul> <li>Either a pair of light curves (<code>LightCurve</code> objects), or</li> <li>A pair of trained GP models (<code>GaussianProcess</code> objects)</li> </ul> <p>If you pass GP models, STELA will automatically use the most recently generated GP samples. If you haven\u2019t generated samples yet, the class will generate 1000 by default.</p> <p>All classes support (via user-defined inputs):</p> <ul> <li>Frequency binning (log or linear)</li> <li>Custom bin edges</li> <li>Frequency range controls (<code>fmin</code>, <code>fmax</code>)<ul> <li>Use <code>fmin=\"auto\"</code>, <code>fmax=\"auto\"</code> to use the minimum and maximum frequency possible given the lightcurve duration, sampling rate.</li> </ul> </li> <li>A <code>.plot()</code> method to quickly visualize the result, including the coherence spectrum for lags.</li> <li>An optional <code>.count_frequencies_in_bins()</code> method (for diagnostics)</li> </ul>"},{"location":"tutorial/#what-each-class-computes","title":"What Each Class Computes\u00b6","text":"<ul> <li><p>PowerSpectrum Quantifies the variability amplitude in a single light curve as a function of frequency.</p> </li> <li><p>CrossSpectrum Measures the complex correlation between two light curves in the frequency domain.</p> </li> <li><p>LagFrequencySpectrum Extracts the phase lag between two light curves as a function of frequency. Coherence spectrum also computed.</p> </li> <li><p>LagEnergySpectrum Computes the time lag between a broad reference band and several comparison bands (e.g., energy bins), averaged over a specified frequency range. Coherence spectrum also computed.</p> </li> <li><p>Coherence Measures how strongly two light curves are linearly related at each frequency (ranges from 0 to 1). Additional coherence from correlated noise can be subtracted using <code>subtract_bias=True</code>. This is not needed for GP realizations, which effectively removes the coherence from noise.</p> </li> </ul> <p>In the following examples, we\u2019ll show how to use each of these classes:</p>"},{"location":"reference/check_inputs/","title":"_check_inputs","text":""},{"location":"reference/clarify_warnings/","title":"_clarify_warnings","text":""},{"location":"reference/coherence/","title":"coherence","text":""},{"location":"reference/coherence/#stela_toolkit.coherence.Coherence","title":"<code>Coherence</code>","text":"<p>Compute the frequency-dependent coherence between two light curves or GP models.</p> <p>This class estimates the coherence spectrum, which quantifies the degree of linear correlation between two time series as a function of frequency. Coherence values range from 0 to 1, with values near 1 indicating a strong linear relationship at that frequency.</p> <p>Inputs can be either LightCurve objects or trained GaussianProcess models from this package. If GP models are provided and posterior samples already exist, those are used. If no samples exist, 1000 GP realizations will be generated automatically on a 1000-point grid.</p> <p>If both inputs are GP models, the coherence is computed for each sample pair and the mean and standard deviation across samples are returned. Otherwise, coherence is computed on the raw input light curves.</p> <p>Poisson noise bias correction is supported and may be enabled to correct for uncorrelated noise.</p> <p>Parameters:</p> Name Type Description Default <code>lc_or_model1</code> <code>LightCurve or GaussianProcess</code> <p>First input light curve or trained GP model.</p> required <code>lc_or_model2</code> <code>LightCurve or GaussianProcess</code> <p>Second input light curve or trained GP model.</p> required <code>fmin</code> <code>float or auto</code> <p>Minimum frequency for the coherence spectrum. If 'auto', uses the lowest nonzero FFT frequency.</p> <code>'auto'</code> <code>fmax</code> <code>float or auto</code> <p>Maximum frequency. If 'auto', uses the Nyquist frequency.</p> <code>'auto'</code> <code>num_bins</code> <code>int</code> <p>Number of frequency bins.</p> <code>None</code> <code>bin_type</code> <code>str</code> <p>Type of frequency binning ('log' or 'linear').</p> <code>'log'</code> <code>bin_edges</code> <code>array - like</code> <p>Custom frequency bin edges.</p> <code>[]</code> <code>subtract_noise_bias</code> <code>bool</code> <p>Whether to subtract Poisson noise bias from the coherence spectrum.</p> <code>True</code> <code>bkg1</code> <code>float</code> <p>Background count rate for lightcurve 1 (used in noise bias correction).</p> <code>0</code> <code>bkg2</code> <code>float</code> <p>Background count rate for lightcurve 2.</p> <code>0</code> <p>Attributes:</p> Name Type Description <code>freqs</code> <code>array - like</code> <p>Frequency bin centers.</p> <code>freq_widths</code> <code>array - like</code> <p>Widths of each frequency bin.</p> <code>cohs</code> <code>array - like</code> <p>Coherence values.</p> <code>coh_errors</code> <code>array - like</code> <p>Uncertainties in the coherence values.</p> Source code in <code>stela_toolkit/coherence.py</code> <pre><code>class Coherence:\n    \"\"\"\n    Compute the frequency-dependent coherence between two light curves or GP models.\n\n    This class estimates the coherence spectrum, which quantifies the degree of linear correlation\n    between two time series as a function of frequency. Coherence values range from 0 to 1,\n    with values near 1 indicating a strong linear relationship at that frequency.\n\n    Inputs can be either LightCurve objects or trained GaussianProcess models from this package.\n    If GP models are provided and posterior samples already exist, those are used.\n    If no samples exist, 1000 GP realizations will be generated automatically on a 1000-point grid.\n\n    If both inputs are GP models, the coherence is computed for each sample pair and the\n    mean and standard deviation across samples are returned. Otherwise, coherence is computed\n    on the raw input light curves.\n\n    Poisson noise bias correction is supported and may be enabled to correct for uncorrelated noise.\n\n    Parameters\n    ----------\n    lc_or_model1 : LightCurve or GaussianProcess\n        First input light curve or trained GP model.\n\n    lc_or_model2 : LightCurve or GaussianProcess\n        Second input light curve or trained GP model.\n\n    fmin : float or 'auto', optional\n        Minimum frequency for the coherence spectrum. If 'auto', uses the lowest nonzero FFT frequency.\n\n    fmax : float or 'auto', optional\n        Maximum frequency. If 'auto', uses the Nyquist frequency.\n\n    num_bins : int, optional\n        Number of frequency bins.\n\n    bin_type : str, optional\n        Type of frequency binning ('log' or 'linear').\n\n    bin_edges : array-like, optional\n        Custom frequency bin edges.\n\n    subtract_noise_bias : bool, optional\n        Whether to subtract Poisson noise bias from the coherence spectrum.\n\n    bkg1 : float, optional\n        Background count rate for lightcurve 1 (used in noise bias correction).\n\n    bkg2 : float, optional\n        Background count rate for lightcurve 2.\n\n    Attributes\n    ----------\n    freqs : array-like\n        Frequency bin centers.\n\n    freq_widths : array-like\n        Widths of each frequency bin.\n\n    cohs : array-like\n        Coherence values.\n\n    coh_errors : array-like\n        Uncertainties in the coherence values.\n    \"\"\"\n\n    def __init__(self,\n                 lc_or_model1,\n                 lc_or_model2,\n                 fmin='auto',\n                 fmax='auto',\n                 num_bins=None,\n                 bin_type=\"log\",\n                 bin_edges=[],\n                 subtract_noise_bias=True,\n                 bkg1=0,\n                 bkg2=0):\n\n        input_data = _CheckInputs._check_lightcurve_or_model(lc_or_model1)\n        if input_data['type'] == 'model':\n            self.times1, self.rates1 = input_data['data']\n        else:\n            self.times1, self.rates1, _ = input_data['data']\n\n        input_data = _CheckInputs._check_lightcurve_or_model(lc_or_model2)\n        if input_data['type'] == 'model':\n            self.times2, self.rates2 = input_data['data']\n        else:\n            self.times2, self.rates2, _ = input_data['data']\n        _CheckInputs._check_input_bins(num_bins, bin_type, bin_edges)\n\n        if not np.allclose(self.times1, self.times2):\n            raise ValueError(\"The time arrays of the two light curves must be identical.\")\n\n        # Use absolute min and max frequencies if set to 'auto'\n        self.dt = np.diff(self.times1)[0]\n        self.fmin = np.fft.rfftfreq(len(self.rates1), d=self.dt)[1] if fmin == 'auto' else fmin\n        self.fmax = np.fft.rfftfreq(len(self.rates1), d=self.dt)[-1] if fmax == 'auto' else fmax  # nyquist frequency\n\n        self.num_bins = num_bins\n        self.bin_type = bin_type\n        self.bin_edges = bin_edges\n\n        self.bkg1 = bkg1\n        self.bkg2 = bkg2\n\n        # Check if the input rates are for multiple realizations\n        # this needs to be corrected for handling different shapes and dim val1 != dim val2\n        # namely for multiple observations\n        if len(self.rates1.shape) == 2 and len(self.rates2.shape) == 2:\n            coherence_spectrum = self.compute_stacked_coherence()\n        else:\n            coherence_spectrum = self.compute_coherence(subtract_noise_bias=subtract_noise_bias)\n\n        self.freqs, self.freq_widths, self.cohs, self.coh_errors = coherence_spectrum\n\n    def compute_coherence(self, times1=None, rates1=None, times2=None, rates2=None, subtract_noise_bias=True):\n        \"\"\"\n        Compute the coherence spectrum between two light curves.\n\n        Parameters\n        ----------\n        times1, rates1 : array-like, optional\n            Time and rate values for the first light curve. Defaults to object attributes.\n        times2, rates2 : array-like, optional\n            Time and rate values for the second light curve. Defaults to object attributes.\n        subtract_noise_bias : bool, optional\n            Whether to subtract the estimated noise bias.\n\n        Returns\n        -------\n        freqs : array-like\n            Frequency bin centers.\n        freq_widths : array-like\n            Frequency bin widths.\n        coherence : array-like\n            Coherence spectrum.\n        None\n            Reserved for compatibility (with the stacked method).\n        \"\"\"\n\n        times1 = self.times1 if times1 is None else times1\n        rates1 = self.rates1 if rates1 is None else rates1\n        times2 = self.times2 if times2 is None else times2\n        rates2 = self.rates2 if rates2 is None else rates2\n\n        lc1 = LightCurve(times=times1, rates=rates1)\n        lc2 = LightCurve(times=times2, rates=rates2)\n        cross_spectrum = CrossSpectrum(\n            lc1, lc2,\n            fmin=self.fmin, fmax=self.fmax,\n            num_bins=self.num_bins, bin_type=self.bin_type, bin_edges=self.bin_edges\n        )\n\n        power_spectrum1 = PowerSpectrum(\n            lc1,\n            fmin=self.fmin, fmax=self.fmax,\n            num_bins=self.num_bins, bin_type=self.bin_type, bin_edges=self.bin_edges\n        )\n        power_spectrum2 = PowerSpectrum(\n            lc2,\n            fmin=self.fmin, fmax=self.fmax,\n            num_bins=self.num_bins, bin_type=self.bin_type, bin_edges=self.bin_edges\n        )\n\n        ps1 = power_spectrum1.powers\n        ps2 = power_spectrum2.powers\n        cs = cross_spectrum.cs\n\n        if subtract_noise_bias:\n            bias = self.compute_bias(ps1, ps2)\n        else:\n            bias = 0\n\n        coherence = (np.abs(cs) ** 2 - bias) / (ps1 * ps2)\n        return power_spectrum1.freqs, power_spectrum1.freq_widths, coherence, None\n\n    def compute_stacked_coherence(self):\n        \"\"\"\n        Compute the coherence from stacked realizations of the light curves.\n\n        For multiple realizations (GP samples), this method computes the\n        coherence for each pair of realizations and returns the mean and standard deviation.\n\n        Returns\n        -------\n        freqs : array-like\n            Frequency bin centers.\n        freq_widths : array-like\n            Frequency bin widths.\n        coherence_mean : array-like\n            Mean coherence spectrum across realizations.\n        coherence_std : array-like\n            Standard deviation of the coherence across realizations.\n        \"\"\"\n\n        coherences = []\n        for i in range(self.rates1.shape[0]):\n            coherence_spectrum = self.compute_coherence(times1=self.times1, rates1=self.rates1[i],\n                                                        times2=self.times2, rates2=self.rates2[i],\n                                                        subtract_noise_bias=False\n                                                    )\n            freqs, freq_widths, coherence, _ = coherence_spectrum\n            coherences.append(coherence)\n\n        coherences = np.vstack(coherences)\n        coherences_mean = np.mean(coherences, axis=0)\n        coherences_std = np.std(coherences, axis=0)\n\n        return freqs, freq_widths, coherences_mean, coherences_std\n\n    def compute_bias(self, power_spectrum1, power_spectrum2):\n        \"\"\"\n        Estimate the Poisson noise bias for the coherence calculation. \n\n        Parameters\n        ----------\n        power_spectrum1 : array-like\n            Power spectrum of the first light curve.\n        power_spectrum2 : array-like\n            Power spectrum of the second light curve.\n\n        Returns\n        -------\n        bias : array-like\n            Estimated noise bias per frequency bin.\n        \"\"\"\n\n        mean1 = np.mean(self.rates1)\n        mean2 = np.mean(self.rates2)\n\n        pnoise1 = 2 * (mean1 + self.bkg1) / mean1 ** 2\n        pnoise2 = 2 * (mean2 + self.bkg2) / mean2 ** 2\n\n        bias = (\n            pnoise2 * (power_spectrum1 - pnoise1)\n            + pnoise1 * (power_spectrum2 - pnoise2)\n            + pnoise1 * pnoise2\n        )\n        num_freq = self.count_frequencies_in_bins()\n        bias /= num_freq\n        return bias\n\n    def plot(self, freqs=None, freq_widths=None, cohs=None, coh_errors=None, **kwargs):\n        \"\"\"\n        Plot the coherence spectrum.\n\n        Parameters\n        ----------\n        **kwargs : dict\n            Additional keyword arguments for plot customization (e.g., xlabel, xscale).\n        \"\"\"\n\n        freqs = self.freqs if freqs is None else freqs\n        freq_widths = self.freq_widths if freq_widths is None else freq_widths\n        cohs = self.cohs if cohs is None else cohs\n        coh_errors = self.coh_errors if coh_errors is None else coh_errors\n\n        kwargs.setdefault('xlabel', 'Frequency')\n        kwargs.setdefault('ylabel', 'Coherence')\n        kwargs.setdefault('xscale', 'log')\n        Plotter.plot(\n            x=freqs, y=cohs, xerr=freq_widths, yerr=coh_errors, **kwargs\n        )\n\n    def count_frequencies_in_bins(self, fmin=None, fmax=None, num_bins=None, bin_type=None, bin_edges=[]):\n        \"\"\"\n        Counts the number of frequencies in each frequency bin.\n        Wrapper method to use FrequencyBinning.count_frequencies_in_bins with class attributes.\n        \"\"\"\n\n        return FrequencyBinning.count_frequencies_in_bins(\n            self, fmin=fmin, fmax=fmax, num_bins=num_bins, bin_type=bin_type, bin_edges=bin_edges\n        )\n</code></pre>"},{"location":"reference/coherence/#stela_toolkit.coherence.Coherence.compute_bias","title":"<code>compute_bias(power_spectrum1, power_spectrum2)</code>","text":"<p>Estimate the Poisson noise bias for the coherence calculation. </p> <p>Parameters:</p> Name Type Description Default <code>power_spectrum1</code> <code>array - like</code> <p>Power spectrum of the first light curve.</p> required <code>power_spectrum2</code> <code>array - like</code> <p>Power spectrum of the second light curve.</p> required <p>Returns:</p> Name Type Description <code>bias</code> <code>array - like</code> <p>Estimated noise bias per frequency bin.</p> Source code in <code>stela_toolkit/coherence.py</code> <pre><code>def compute_bias(self, power_spectrum1, power_spectrum2):\n    \"\"\"\n    Estimate the Poisson noise bias for the coherence calculation. \n\n    Parameters\n    ----------\n    power_spectrum1 : array-like\n        Power spectrum of the first light curve.\n    power_spectrum2 : array-like\n        Power spectrum of the second light curve.\n\n    Returns\n    -------\n    bias : array-like\n        Estimated noise bias per frequency bin.\n    \"\"\"\n\n    mean1 = np.mean(self.rates1)\n    mean2 = np.mean(self.rates2)\n\n    pnoise1 = 2 * (mean1 + self.bkg1) / mean1 ** 2\n    pnoise2 = 2 * (mean2 + self.bkg2) / mean2 ** 2\n\n    bias = (\n        pnoise2 * (power_spectrum1 - pnoise1)\n        + pnoise1 * (power_spectrum2 - pnoise2)\n        + pnoise1 * pnoise2\n    )\n    num_freq = self.count_frequencies_in_bins()\n    bias /= num_freq\n    return bias\n</code></pre>"},{"location":"reference/coherence/#stela_toolkit.coherence.Coherence.compute_coherence","title":"<code>compute_coherence(times1=None, rates1=None, times2=None, rates2=None, subtract_noise_bias=True)</code>","text":"<p>Compute the coherence spectrum between two light curves.</p> <p>Parameters:</p> Name Type Description Default <code>times1</code> <code>array - like</code> <p>Time and rate values for the first light curve. Defaults to object attributes.</p> <code>None</code> <code>rates1</code> <code>array - like</code> <p>Time and rate values for the first light curve. Defaults to object attributes.</p> <code>None</code> <code>times2</code> <code>array - like</code> <p>Time and rate values for the second light curve. Defaults to object attributes.</p> <code>None</code> <code>rates2</code> <code>array - like</code> <p>Time and rate values for the second light curve. Defaults to object attributes.</p> <code>None</code> <code>subtract_noise_bias</code> <code>bool</code> <p>Whether to subtract the estimated noise bias.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>freqs</code> <code>array - like</code> <p>Frequency bin centers.</p> <code>freq_widths</code> <code>array - like</code> <p>Frequency bin widths.</p> <code>coherence</code> <code>array - like</code> <p>Coherence spectrum.</p> <code>None</code> <p>Reserved for compatibility (with the stacked method).</p> Source code in <code>stela_toolkit/coherence.py</code> <pre><code>def compute_coherence(self, times1=None, rates1=None, times2=None, rates2=None, subtract_noise_bias=True):\n    \"\"\"\n    Compute the coherence spectrum between two light curves.\n\n    Parameters\n    ----------\n    times1, rates1 : array-like, optional\n        Time and rate values for the first light curve. Defaults to object attributes.\n    times2, rates2 : array-like, optional\n        Time and rate values for the second light curve. Defaults to object attributes.\n    subtract_noise_bias : bool, optional\n        Whether to subtract the estimated noise bias.\n\n    Returns\n    -------\n    freqs : array-like\n        Frequency bin centers.\n    freq_widths : array-like\n        Frequency bin widths.\n    coherence : array-like\n        Coherence spectrum.\n    None\n        Reserved for compatibility (with the stacked method).\n    \"\"\"\n\n    times1 = self.times1 if times1 is None else times1\n    rates1 = self.rates1 if rates1 is None else rates1\n    times2 = self.times2 if times2 is None else times2\n    rates2 = self.rates2 if rates2 is None else rates2\n\n    lc1 = LightCurve(times=times1, rates=rates1)\n    lc2 = LightCurve(times=times2, rates=rates2)\n    cross_spectrum = CrossSpectrum(\n        lc1, lc2,\n        fmin=self.fmin, fmax=self.fmax,\n        num_bins=self.num_bins, bin_type=self.bin_type, bin_edges=self.bin_edges\n    )\n\n    power_spectrum1 = PowerSpectrum(\n        lc1,\n        fmin=self.fmin, fmax=self.fmax,\n        num_bins=self.num_bins, bin_type=self.bin_type, bin_edges=self.bin_edges\n    )\n    power_spectrum2 = PowerSpectrum(\n        lc2,\n        fmin=self.fmin, fmax=self.fmax,\n        num_bins=self.num_bins, bin_type=self.bin_type, bin_edges=self.bin_edges\n    )\n\n    ps1 = power_spectrum1.powers\n    ps2 = power_spectrum2.powers\n    cs = cross_spectrum.cs\n\n    if subtract_noise_bias:\n        bias = self.compute_bias(ps1, ps2)\n    else:\n        bias = 0\n\n    coherence = (np.abs(cs) ** 2 - bias) / (ps1 * ps2)\n    return power_spectrum1.freqs, power_spectrum1.freq_widths, coherence, None\n</code></pre>"},{"location":"reference/coherence/#stela_toolkit.coherence.Coherence.compute_stacked_coherence","title":"<code>compute_stacked_coherence()</code>","text":"<p>Compute the coherence from stacked realizations of the light curves.</p> <p>For multiple realizations (GP samples), this method computes the coherence for each pair of realizations and returns the mean and standard deviation.</p> <p>Returns:</p> Name Type Description <code>freqs</code> <code>array - like</code> <p>Frequency bin centers.</p> <code>freq_widths</code> <code>array - like</code> <p>Frequency bin widths.</p> <code>coherence_mean</code> <code>array - like</code> <p>Mean coherence spectrum across realizations.</p> <code>coherence_std</code> <code>array - like</code> <p>Standard deviation of the coherence across realizations.</p> Source code in <code>stela_toolkit/coherence.py</code> <pre><code>def compute_stacked_coherence(self):\n    \"\"\"\n    Compute the coherence from stacked realizations of the light curves.\n\n    For multiple realizations (GP samples), this method computes the\n    coherence for each pair of realizations and returns the mean and standard deviation.\n\n    Returns\n    -------\n    freqs : array-like\n        Frequency bin centers.\n    freq_widths : array-like\n        Frequency bin widths.\n    coherence_mean : array-like\n        Mean coherence spectrum across realizations.\n    coherence_std : array-like\n        Standard deviation of the coherence across realizations.\n    \"\"\"\n\n    coherences = []\n    for i in range(self.rates1.shape[0]):\n        coherence_spectrum = self.compute_coherence(times1=self.times1, rates1=self.rates1[i],\n                                                    times2=self.times2, rates2=self.rates2[i],\n                                                    subtract_noise_bias=False\n                                                )\n        freqs, freq_widths, coherence, _ = coherence_spectrum\n        coherences.append(coherence)\n\n    coherences = np.vstack(coherences)\n    coherences_mean = np.mean(coherences, axis=0)\n    coherences_std = np.std(coherences, axis=0)\n\n    return freqs, freq_widths, coherences_mean, coherences_std\n</code></pre>"},{"location":"reference/coherence/#stela_toolkit.coherence.Coherence.count_frequencies_in_bins","title":"<code>count_frequencies_in_bins(fmin=None, fmax=None, num_bins=None, bin_type=None, bin_edges=[])</code>","text":"<p>Counts the number of frequencies in each frequency bin. Wrapper method to use FrequencyBinning.count_frequencies_in_bins with class attributes.</p> Source code in <code>stela_toolkit/coherence.py</code> <pre><code>def count_frequencies_in_bins(self, fmin=None, fmax=None, num_bins=None, bin_type=None, bin_edges=[]):\n    \"\"\"\n    Counts the number of frequencies in each frequency bin.\n    Wrapper method to use FrequencyBinning.count_frequencies_in_bins with class attributes.\n    \"\"\"\n\n    return FrequencyBinning.count_frequencies_in_bins(\n        self, fmin=fmin, fmax=fmax, num_bins=num_bins, bin_type=bin_type, bin_edges=bin_edges\n    )\n</code></pre>"},{"location":"reference/coherence/#stela_toolkit.coherence.Coherence.plot","title":"<code>plot(freqs=None, freq_widths=None, cohs=None, coh_errors=None, **kwargs)</code>","text":"<p>Plot the coherence spectrum.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments for plot customization (e.g., xlabel, xscale).</p> <code>{}</code> Source code in <code>stela_toolkit/coherence.py</code> <pre><code>def plot(self, freqs=None, freq_widths=None, cohs=None, coh_errors=None, **kwargs):\n    \"\"\"\n    Plot the coherence spectrum.\n\n    Parameters\n    ----------\n    **kwargs : dict\n        Additional keyword arguments for plot customization (e.g., xlabel, xscale).\n    \"\"\"\n\n    freqs = self.freqs if freqs is None else freqs\n    freq_widths = self.freq_widths if freq_widths is None else freq_widths\n    cohs = self.cohs if cohs is None else cohs\n    coh_errors = self.coh_errors if coh_errors is None else coh_errors\n\n    kwargs.setdefault('xlabel', 'Frequency')\n    kwargs.setdefault('ylabel', 'Coherence')\n    kwargs.setdefault('xscale', 'log')\n    Plotter.plot(\n        x=freqs, y=cohs, xerr=freq_widths, yerr=coh_errors, **kwargs\n    )\n</code></pre>"},{"location":"reference/cross_correlation/","title":"cross_correlation","text":""},{"location":"reference/cross_correlation/#stela_toolkit.cross_correlation.CrossCorrelation","title":"<code>CrossCorrelation</code>","text":"<p>Compute the time-domain cross-correlation function (CCF) between two light curves or GP models.</p> <p>This class supports three primary use cases:</p> <ol> <li> <p>Regularly sampled <code>LightCurve</code> objects    Computes the CCF directly using Pearson correlation across lag values. Requires aligned time grids.</p> </li> <li> <p>Irregularly sampled <code>LightCurve</code> objects    Uses the interpolated cross-correlation method (ICCF; Gaskell &amp; Peterson 1987), which linearly interpolates    one light curve onto the other\u2019s grid to estimate correlations across lags despite uneven sampling.</p> </li> <li> <p><code>GaussianProcess</code> models    If both inputs are trained GP models with sampled realizations (via <code>.sample()</code>), STELA computes the CCF for each    realization pair, then averages the resulting peak and centroid lags. The final outputs are returned as tuples:    <code>(mean, standard deviation)</code> for peak lag, centroid lag, and maximum correlation (rmax).</p> </li> </ol> <p>If samples have not yet been generated, 1000 realizations will be drawn automatically on a 1000-point time grid.</p> <p>Additionally, optional Monte Carlo resampling is available to assess lag uncertainties from observational errors.</p> <p>Parameters:</p> Name Type Description Default <code>lc_or_model1</code> <code>LightCurve or GaussianProcess</code> <p>First input light curve or trained GP model.</p> required <code>lc_or_model2</code> <code>LightCurve or GaussianProcess</code> <p>Second input light curve or trained GP model.</p> required <code>run_monte_carlo</code> <code>bool</code> <p>Whether to estimate lag uncertainties using Monte Carlo resampling.</p> <code>False</code> <code>n_trials</code> <code>int</code> <p>Number of Monte Carlo trials.</p> <code>1000</code> <code>min_lag</code> <code>float or auto</code> <p>Minimum lag to evaluate. If \"auto\", set to <code>-duration / 2</code>.</p> <code>'auto'</code> <code>max_lag</code> <code>float or auto</code> <p>Maximum lag to evaluate. If \"auto\", set to <code>+duration / 2</code>.</p> <code>'auto'</code> <code>dt</code> <code>float or auto</code> <p>Time step for lag evaluation. If \"auto\", set to \u2155 of the mean sampling interval.</p> <code>'auto'</code> <code>centroid_threshold</code> <code>float</code> <p>Threshold (as a fraction of peak correlation) for defining the centroid lag region.</p> <code>0.8</code> <code>mode</code> <code>(regular, interp)</code> <p>CCF computation mode. Use \"regular\" for direct shifting, or \"interp\" for ICCF-based interpolation.</p> <code>\"regular\"</code> <code>rmax_threshold</code> <code>float</code> <p>Trials with a maximum correlation (rmax) below this threshold are discarded when using Monte Carlo.</p> <code>0.0</code> <p>Attributes:</p> Name Type Description <code>lags</code> <code>ndarray</code> <p>Array of lag values evaluated.</p> <code>ccf</code> <code>ndarray or None</code> <p>Cross-correlation coefficients. Not set when both inputs are GP models.</p> <code>peak_lag</code> <code>float or tuple</code> <p>Peak lag of the CCF. If using GPs, returns (mean, std) across realizations.</p> <code>centroid_lag</code> <code>float or tuple</code> <p>Centroid lag of the high-correlation region. If using GPs, returns (mean, std).</p> <code>rmax</code> <code>float or tuple</code> <p>Maximum correlation value. If using GPs, returns (mean, std).</p> <code>peak_lags_mc</code> <code>ndarray or None</code> <p>Peak lags from Monte Carlo trials, if enabled.</p> <code>centroid_lags_mc</code> <code>ndarray or None</code> <p>Centroid lags from Monte Carlo trials.</p> <code>peak_lag_ci</code> <code>tuple or None</code> <p>68% confidence interval (16<sup>th</sup>\u201384<sup>th</sup> percentile) on peak lag from MC trials.</p> <code>centroid_lag_ci</code> <code>tuple or None</code> <p>68% confidence interval on centroid lag from MC trials.</p> Source code in <code>stela_toolkit/cross_correlation.py</code> <pre><code>class CrossCorrelation:\n    \"\"\"\n    Compute the time-domain cross-correlation function (CCF) between two light curves or GP models.\n\n    This class supports three primary use cases:\n\n    1. **Regularly sampled `LightCurve` objects**  \n       Computes the CCF directly using Pearson correlation across lag values. Requires aligned time grids.\n\n    2. **Irregularly sampled `LightCurve` objects**  \n       Uses the interpolated cross-correlation method (ICCF; Gaskell &amp; Peterson 1987), which linearly interpolates\n       one light curve onto the other\u2019s grid to estimate correlations across lags despite uneven sampling.\n\n    3. **`GaussianProcess` models**  \n       If both inputs are trained GP models with sampled realizations (via `.sample()`), STELA computes the CCF for each\n       realization pair, then averages the resulting peak and centroid lags. The final outputs are returned as tuples:\n       `(mean, standard deviation)` for peak lag, centroid lag, and maximum correlation (rmax).\n\n       If samples have not yet been generated, 1000 realizations will be drawn automatically on a 1000-point time grid.\n\n    Additionally, optional Monte Carlo resampling is available to assess lag uncertainties from observational errors.\n\n    Parameters\n    ----------\n    lc_or_model1 : LightCurve or GaussianProcess\n        First input light curve or trained GP model.\n\n    lc_or_model2 : LightCurve or GaussianProcess\n        Second input light curve or trained GP model.\n\n    run_monte_carlo : bool, optional\n        Whether to estimate lag uncertainties using Monte Carlo resampling.\n\n    n_trials : int, optional\n        Number of Monte Carlo trials.\n\n    min_lag : float or \"auto\", optional\n        Minimum lag to evaluate. If \"auto\", set to `-duration / 2`.\n\n    max_lag : float or \"auto\", optional\n        Maximum lag to evaluate. If \"auto\", set to `+duration / 2`.\n\n    dt : float or \"auto\", optional\n        Time step for lag evaluation. If \"auto\", set to 1/5 of the mean sampling interval.\n\n    centroid_threshold : float, optional\n        Threshold (as a fraction of peak correlation) for defining the centroid lag region.\n\n    mode : {\"regular\", \"interp\"}, optional\n        CCF computation mode. Use \"regular\" for direct shifting, or \"interp\" for ICCF-based interpolation.\n\n    rmax_threshold : float, optional\n        Trials with a maximum correlation (rmax) below this threshold are discarded when using Monte Carlo.\n\n    Attributes\n    ----------\n    lags : ndarray\n        Array of lag values evaluated.\n\n    ccf : ndarray or None\n        Cross-correlation coefficients. Not set when both inputs are GP models.\n\n    peak_lag : float or tuple\n        Peak lag of the CCF. If using GPs, returns (mean, std) across realizations.\n\n    centroid_lag : float or tuple\n        Centroid lag of the high-correlation region. If using GPs, returns (mean, std).\n\n    rmax : float or tuple\n        Maximum correlation value. If using GPs, returns (mean, std).\n\n    peak_lags_mc : ndarray or None\n        Peak lags from Monte Carlo trials, if enabled.\n\n    centroid_lags_mc : ndarray or None\n        Centroid lags from Monte Carlo trials.\n\n    peak_lag_ci : tuple or None\n        68% confidence interval (16th\u201384th percentile) on peak lag from MC trials.\n\n    centroid_lag_ci : tuple or None\n        68% confidence interval on centroid lag from MC trials.\n    \"\"\"\n\n\n    def __init__(self,\n                 lc_or_model1,\n                 lc_or_model2,\n                 run_monte_carlo=False,\n                 n_trials=1000,\n                 min_lag=\"auto\",\n                 max_lag=\"auto\",\n                 dt=\"auto\",\n                 centroid_threshold=0.8,\n                 mode=\"regular\",\n                 rmax_threshold=0.0):\n\n        req_reg_samp = True if mode==\"regular\" else False\n        data1 = _CheckInputs._check_lightcurve_or_model(lc_or_model1, req_reg_samp=req_reg_samp)\n        data2 = _CheckInputs._check_lightcurve_or_model(lc_or_model2, req_reg_samp=req_reg_samp)\n\n        if data1['type'] == 'model':\n            if not hasattr(lc_or_model1, 'samples'):\n                raise ValueError(\"Model 1 must have generated samples via GP.sample().\")\n            self.times = lc_or_model1.pred_times.numpy()\n            self.rates1 = lc_or_model1.samples\n            self.is_model1 = True\n        else:\n            self.times, self.rates1, self.errors1 = data1['data']\n            self.is_model1 = False\n\n        if data2['type'] == 'model':\n            if not hasattr(lc_or_model2, 'samples'):\n                raise ValueError(\"Model 2 must have generated samples via GP.sample().\")\n            self.times = lc_or_model2.pred_times.numpy()\n            self.rates2 = lc_or_model2.samples\n            self.is_model2 = True\n        else:\n            self.times, self.rates2, self.errors2 = data2['data']\n            self.is_model2 = False\n\n        self.n_trials = n_trials\n        self.centroid_threshold = centroid_threshold\n        self.mode = mode\n        self.rmax_threshold = rmax_threshold\n\n        duration = self.times[-1] - self.times[0]\n        self.min_lag = -duration / 2 if min_lag==\"auto\" else min_lag\n        self.max_lag = duration / 2 if max_lag==\"auto\" else max_lag\n\n        if mode == \"regular\":\n            self.dt = np.diff(self.times)[0]\n            self.lags = np.arange(self.min_lag, self.max_lag + self.dt, self.dt)\n\n            if self.is_model1 and self.is_model2:\n                if self.rates1.shape[0] != self.rates2.shape[0]:\n                    raise ValueError(\"Model sample shapes do not match.\")\n\n                peak_lags, centroid_lags, rmaxs = [], [], []\n\n                # Compute ccf and lags for each pair of realizations\n                for i in range(self.rates1.shape[0]):\n                    ccf = self.compute_ccf(self.rates1[i], self.rates2[i])\n                    peak_lag, centroid_lag = self.find_peak_and_centroid(self.lags, ccf)\n                    rmax = np.max(ccf)\n\n                    peak_lags.append(peak_lag)\n                    centroid_lags.append(centroid_lag)\n                    rmaxs.append(rmax)\n\n                self.peak_lag = (np.mean(peak_lags), np.std(peak_lags))\n                self.centroid_lag = (np.mean(centroid_lags), np.std(centroid_lags))\n                self.rmax = (np.mean(rmaxs), np.std(rmaxs))\n\n            else:\n                self.ccf = self.compute_ccf(self.rates1, self.rates2)\n\n        else:\n            self.dt = np.mean(np.diff(self.times)) / 5 if dt==\"auto\" else dt\n            self.lags = np.arange(self.min_lag, self.max_lag + self.dt, self.dt)\n            self.ccf = self.compute_ccf_interp()\n\n            self.rmax = np.max(self.ccf)\n            self.peak_lag, self.centroid_lag = self.find_peak_and_centroid(self.lags, self.ccf)\n\n        self.peak_lags_mc = None\n        self.centroid_lags_mc = None\n        self.peak_lag_ci = None\n        self.centroid_lag_ci = None\n\n        if run_monte_carlo:\n            if np.all(self.errors1 == 0) or np.all(self.errors2 == 0):\n                print(\"Skipping Monte Carlo: zero errors for all points in one or both light curves.\")\n            else:\n                self.peak_lags_mc, self.centroid_lags_mc = self.run_monte_carlo()\n                self.compute_confidence_intervals()\n\n    def compute_ccf(self, rates1, rates2):\n        \"\"\"\n        Compute the cross-correlation function (CCF) via direct shifting.\n\n        Parameters\n        ----------\n        rates1 : ndarray\n            First time series.\n\n        rates2 : ndarray\n            Second time series.\n\n        Returns\n        -------\n        lags : ndarray\n            Lag values.\n\n        ccf : ndarray\n            Pearson correlation coefficients at each lag.\n        \"\"\"\n\n        ccf = []\n\n        for lag in self.lags:\n            shift = int(round(lag / self.dt))\n\n            if shift &lt; 0:\n                x = rates1[:shift]\n                y = rates2[-shift:]\n            elif shift &gt; 0:\n                x = rates1[shift:]\n                y = rates2[:-shift]\n            else:\n                x = rates1\n                y = rates2\n\n            if len(x) &lt; 2:\n                ccf.append(0.0)\n            else:\n                r = np.corrcoef(x, y)[0, 1]\n                ccf.append(r)\n\n        return np.array(ccf)\n\n    def compute_ccf_interp(self):\n        \"\"\"\n        Compute the cross-correlation function using symmetric linear interpolation.\n\n        Returns\n        -------\n        ccf : ndarray\n            Interpolated cross-correlation values for each lag.\n        \"\"\"\n\n        interp1 = interp1d(self.times, self.rates1, bounds_error=False, fill_value=0.0)\n        interp2 = interp1d(self.times, self.rates2, bounds_error=False, fill_value=0.0)\n        ccf = []\n\n        for lag in self.lags:\n            t_shift1 = self.times + lag\n            t_shift2 = self.times - lag\n\n            mask1 = (t_shift1 &gt;= self.times[0]) &amp; (t_shift1 &lt;= self.times[-1])\n            mask2 = (t_shift2 &gt;= self.times[0]) &amp; (t_shift2 &lt;= self.times[-1])\n\n            if np.sum(mask1) &lt; 2 or np.sum(mask2) &lt; 2:\n                ccf.append(0.0)\n                continue\n\n            r1 = np.corrcoef(self.rates1[mask1], interp2(t_shift1[mask1]))[0, 1]\n            r2 = np.corrcoef(self.rates2[mask2], interp1(t_shift2[mask2]))[0, 1]\n            ccf.append((r1 + r2) / 2)\n\n        return np.array(ccf)\n\n    def find_peak_and_centroid(self, lags, ccf):\n        \"\"\"\n        Compute the peak and centroid lag of a cross-correlation function.\n\n        The peak lag corresponds to the lag with the maximum correlation value.\n        The centroid lag is computed using a weighted average of lag values\n        in a contiguous region around the peak where the correlation exceeds\n        a fraction of the peak value.\n\n        Parameters\n        ----------\n        lags : ndarray\n            Array of lag values (assumed sorted).\n\n        ccf : ndarray\n            Cross-correlation values at each lag.\n\n        Returns\n        -------\n        peak_lag : float\n            Lag corresponding to the maximum correlation.\n\n        centroid_lag : float or np.nan\n            Correlation-weighted centroid lag near the peak.\n            Returns NaN if a valid centroid region cannot be identified.\n        \"\"\"\n        if len(lags) != len(ccf) or len(ccf) == 0:\n            raise ValueError(\"lags and ccf must be the same nonzero length\")\n\n        # Locate the peak correlation and corresponding lag\n        peak_idx = np.nanargmax(ccf)\n        peak_lag = lags[peak_idx]\n        peak_val = ccf[peak_idx]\n\n        # Define a local region around the peak above a fractional threshold\n        threshold = self.centroid_threshold\n        cutoff = threshold * peak_val\n\n        # Expand to left of peak\n        i_left = peak_idx\n        while i_left &gt; 0 and ccf[i_left - 1] &gt;= cutoff:\n            i_left -= 1\n\n        # Expand to right of peak\n        i_right = peak_idx\n        while i_right &lt; len(ccf) - 1 and ccf[i_right + 1] &gt;= cutoff:\n            i_right += 1\n\n        # Compute centroid if region is valid\n        if i_right &gt;= i_left:\n            lags_subset = lags[i_left:i_right + 1]\n            ccf_subset = ccf[i_left:i_right + 1]\n            weight_sum = np.sum(ccf_subset)\n            if weight_sum &gt; 0:\n                centroid_lag = np.sum(lags_subset * ccf_subset) / weight_sum\n            else:\n                centroid_lag = np.nan\n        else:\n            centroid_lag = np.nan\n\n        return peak_lag, centroid_lag\n\n\n    def run_monte_carlo(self):\n        \"\"\"\n        Run Monte Carlo simulations to estimate lag uncertainties.\n\n        Perturbs input light curves based on their errors and computes peak and centroid\n        lags for each realization.\n\n        Returns\n        -------\n        peak_lags : ndarray\n            Peak lag values from all trials.\n\n        centroid_lags : ndarray\n            Centroid lag values from all trials.\n        \"\"\"\n\n        peak_lags = []\n        centroid_lags = []\n\n        for _ in range(self.n_trials):\n            r1_pert = np.random.normal(self.rates1, self.errors1)\n            r2_pert = np.random.normal(self.rates2, self.errors2)\n\n            if self.mode == \"interp\":\n                interp1 = interp1d(self.times, r1_pert, bounds_error=False, fill_value=0.0)\n                interp2 = interp1d(self.times, r2_pert, bounds_error=False, fill_value=0.0)\n                ccf = []\n\n                for lag in self.lags:\n                    t_shift1 = self.times + lag\n                    t_shift2 = self.times - lag\n\n                    mask1 = (t_shift1 &gt;= self.times[0]) &amp; (t_shift1 &lt;= self.times[-1])\n                    mask2 = (t_shift2 &gt;= self.times[0]) &amp; (t_shift2 &lt;= self.times[-1])\n\n                    if np.sum(mask1) &lt; 2 or np.sum(mask2) &lt; 2:\n                        ccf.append(0.0)\n                        continue\n\n                    r1 = np.corrcoef(r1_pert[mask1], interp2(t_shift1[mask1]))[0, 1]\n                    r2 = np.corrcoef(r2_pert[mask2], interp1(t_shift2[mask2]))[0, 1]\n                    ccf_val = (r1 + r2) / 2\n                    ccf.append(ccf_val)\n                ccf = np.array(ccf)\n            else:\n                ccf = self.compute_ccf(r1_pert, r2_pert)\n\n            if np.max(ccf) &lt; self.rmax_threshold:\n                continue\n\n            peak, centroid = self.find_peak_and_centroid(self.lags, ccf)\n            peak_lags.append(peak)\n            centroid_lags.append(centroid)\n\n        return np.array(peak_lags), np.array(centroid_lags)\n\n    def compute_confidence_intervals(self, lower_percentile=16, upper_percentile=84):\n        \"\"\"\n        Compute percentile-based confidence intervals for Monte Carlo lag distributions.\n\n        Parameters\n        ----------\n        lower_percentile : float\n            Lower percentile bound (default is 16).\n\n        upper_percentile : float\n            Upper percentile bound (default is 84).\n        \"\"\"\n\n        if self.peak_lags_mc is None or self.centroid_lags_mc is None:\n            print(\"No Monte Carlo results available to compute confidence intervals.\")\n            return\n\n        self.peak_lag_ci = (\n            np.percentile(self.peak_lags_mc, lower_percentile),\n            np.percentile(self.peak_lags_mc, upper_percentile),\n        )\n        self.centroid_lag_ci = (\n            np.percentile(self.centroid_lags_mc, lower_percentile),\n            np.percentile(self.centroid_lags_mc, upper_percentile),\n        )\n\n    def plot(self, show_mc=True):\n        \"\"\"\n        Plot the cross-correlation function and optional Monte Carlo lag distributions.\n\n        Arguments\n        ----------\n        show_mc : bool\n            Plots results of Monte Carlo peak and centroid lag distributions.\n        \"\"\"\n\n        fig, ax = plt.subplots(1, 1, figsize=(7, 4))\n        ax.plot(self.lags, self.ccf, label=\"CCF\", color='black')\n        ax.axvline(self.peak_lag, color='red', linestyle='--',\n                   label=f\"Peak lag = {self.peak_lag:.2f}\")\n        ax.axvline(self.centroid_lag, color='blue', linestyle=':',\n                   label=f\"Centroid lag = {self.centroid_lag:.2f}\")\n        ax.set_xlabel(\"Lag (same unit as input)\")\n        ax.set_ylabel(\"Correlation coefficient\")\n        ax.grid(True)\n        ax.legend()\n\n        if show_mc and self.peak_lags_mc is not None:\n            fig_mc, ax_mc = plt.subplots(1, 2, figsize=(10, 4))\n            ax_mc[0].hist(self.peak_lags_mc, bins=30, color='red', alpha=0.7)\n            ax_mc[0].set_title(\"Peak Lag Distribution (MC)\")\n            ax_mc[0].set_xlabel(\"Lag\")\n            ax_mc[0].set_ylabel(\"Count\")\n            ax_mc[0].grid(True)\n\n            ax_mc[1].hist(self.centroid_lags_mc, bins=30, color='blue', alpha=0.7)\n            ax_mc[1].set_title(\"Centroid Lag Distribution (MC)\")\n            ax_mc[1].set_xlabel(\"Lag\")\n            ax_mc[1].set_ylabel(\"Count\")\n            ax_mc[1].grid(True)\n\n        plt.tight_layout()\n        plt.show()\n</code></pre>"},{"location":"reference/cross_correlation/#stela_toolkit.cross_correlation.CrossCorrelation.compute_ccf","title":"<code>compute_ccf(rates1, rates2)</code>","text":"<p>Compute the cross-correlation function (CCF) via direct shifting.</p> <p>Parameters:</p> Name Type Description Default <code>rates1</code> <code>ndarray</code> <p>First time series.</p> required <code>rates2</code> <code>ndarray</code> <p>Second time series.</p> required <p>Returns:</p> Name Type Description <code>lags</code> <code>ndarray</code> <p>Lag values.</p> <code>ccf</code> <code>ndarray</code> <p>Pearson correlation coefficients at each lag.</p> Source code in <code>stela_toolkit/cross_correlation.py</code> <pre><code>def compute_ccf(self, rates1, rates2):\n    \"\"\"\n    Compute the cross-correlation function (CCF) via direct shifting.\n\n    Parameters\n    ----------\n    rates1 : ndarray\n        First time series.\n\n    rates2 : ndarray\n        Second time series.\n\n    Returns\n    -------\n    lags : ndarray\n        Lag values.\n\n    ccf : ndarray\n        Pearson correlation coefficients at each lag.\n    \"\"\"\n\n    ccf = []\n\n    for lag in self.lags:\n        shift = int(round(lag / self.dt))\n\n        if shift &lt; 0:\n            x = rates1[:shift]\n            y = rates2[-shift:]\n        elif shift &gt; 0:\n            x = rates1[shift:]\n            y = rates2[:-shift]\n        else:\n            x = rates1\n            y = rates2\n\n        if len(x) &lt; 2:\n            ccf.append(0.0)\n        else:\n            r = np.corrcoef(x, y)[0, 1]\n            ccf.append(r)\n\n    return np.array(ccf)\n</code></pre>"},{"location":"reference/cross_correlation/#stela_toolkit.cross_correlation.CrossCorrelation.compute_ccf_interp","title":"<code>compute_ccf_interp()</code>","text":"<p>Compute the cross-correlation function using symmetric linear interpolation.</p> <p>Returns:</p> Name Type Description <code>ccf</code> <code>ndarray</code> <p>Interpolated cross-correlation values for each lag.</p> Source code in <code>stela_toolkit/cross_correlation.py</code> <pre><code>def compute_ccf_interp(self):\n    \"\"\"\n    Compute the cross-correlation function using symmetric linear interpolation.\n\n    Returns\n    -------\n    ccf : ndarray\n        Interpolated cross-correlation values for each lag.\n    \"\"\"\n\n    interp1 = interp1d(self.times, self.rates1, bounds_error=False, fill_value=0.0)\n    interp2 = interp1d(self.times, self.rates2, bounds_error=False, fill_value=0.0)\n    ccf = []\n\n    for lag in self.lags:\n        t_shift1 = self.times + lag\n        t_shift2 = self.times - lag\n\n        mask1 = (t_shift1 &gt;= self.times[0]) &amp; (t_shift1 &lt;= self.times[-1])\n        mask2 = (t_shift2 &gt;= self.times[0]) &amp; (t_shift2 &lt;= self.times[-1])\n\n        if np.sum(mask1) &lt; 2 or np.sum(mask2) &lt; 2:\n            ccf.append(0.0)\n            continue\n\n        r1 = np.corrcoef(self.rates1[mask1], interp2(t_shift1[mask1]))[0, 1]\n        r2 = np.corrcoef(self.rates2[mask2], interp1(t_shift2[mask2]))[0, 1]\n        ccf.append((r1 + r2) / 2)\n\n    return np.array(ccf)\n</code></pre>"},{"location":"reference/cross_correlation/#stela_toolkit.cross_correlation.CrossCorrelation.compute_confidence_intervals","title":"<code>compute_confidence_intervals(lower_percentile=16, upper_percentile=84)</code>","text":"<p>Compute percentile-based confidence intervals for Monte Carlo lag distributions.</p> <p>Parameters:</p> Name Type Description Default <code>lower_percentile</code> <code>float</code> <p>Lower percentile bound (default is 16).</p> <code>16</code> <code>upper_percentile</code> <code>float</code> <p>Upper percentile bound (default is 84).</p> <code>84</code> Source code in <code>stela_toolkit/cross_correlation.py</code> <pre><code>def compute_confidence_intervals(self, lower_percentile=16, upper_percentile=84):\n    \"\"\"\n    Compute percentile-based confidence intervals for Monte Carlo lag distributions.\n\n    Parameters\n    ----------\n    lower_percentile : float\n        Lower percentile bound (default is 16).\n\n    upper_percentile : float\n        Upper percentile bound (default is 84).\n    \"\"\"\n\n    if self.peak_lags_mc is None or self.centroid_lags_mc is None:\n        print(\"No Monte Carlo results available to compute confidence intervals.\")\n        return\n\n    self.peak_lag_ci = (\n        np.percentile(self.peak_lags_mc, lower_percentile),\n        np.percentile(self.peak_lags_mc, upper_percentile),\n    )\n    self.centroid_lag_ci = (\n        np.percentile(self.centroid_lags_mc, lower_percentile),\n        np.percentile(self.centroid_lags_mc, upper_percentile),\n    )\n</code></pre>"},{"location":"reference/cross_correlation/#stela_toolkit.cross_correlation.CrossCorrelation.find_peak_and_centroid","title":"<code>find_peak_and_centroid(lags, ccf)</code>","text":"<p>Compute the peak and centroid lag of a cross-correlation function.</p> <p>The peak lag corresponds to the lag with the maximum correlation value. The centroid lag is computed using a weighted average of lag values in a contiguous region around the peak where the correlation exceeds a fraction of the peak value.</p> <p>Parameters:</p> Name Type Description Default <code>lags</code> <code>ndarray</code> <p>Array of lag values (assumed sorted).</p> required <code>ccf</code> <code>ndarray</code> <p>Cross-correlation values at each lag.</p> required <p>Returns:</p> Name Type Description <code>peak_lag</code> <code>float</code> <p>Lag corresponding to the maximum correlation.</p> <code>centroid_lag</code> <code>float or nan</code> <p>Correlation-weighted centroid lag near the peak. Returns NaN if a valid centroid region cannot be identified.</p> Source code in <code>stela_toolkit/cross_correlation.py</code> <pre><code>def find_peak_and_centroid(self, lags, ccf):\n    \"\"\"\n    Compute the peak and centroid lag of a cross-correlation function.\n\n    The peak lag corresponds to the lag with the maximum correlation value.\n    The centroid lag is computed using a weighted average of lag values\n    in a contiguous region around the peak where the correlation exceeds\n    a fraction of the peak value.\n\n    Parameters\n    ----------\n    lags : ndarray\n        Array of lag values (assumed sorted).\n\n    ccf : ndarray\n        Cross-correlation values at each lag.\n\n    Returns\n    -------\n    peak_lag : float\n        Lag corresponding to the maximum correlation.\n\n    centroid_lag : float or np.nan\n        Correlation-weighted centroid lag near the peak.\n        Returns NaN if a valid centroid region cannot be identified.\n    \"\"\"\n    if len(lags) != len(ccf) or len(ccf) == 0:\n        raise ValueError(\"lags and ccf must be the same nonzero length\")\n\n    # Locate the peak correlation and corresponding lag\n    peak_idx = np.nanargmax(ccf)\n    peak_lag = lags[peak_idx]\n    peak_val = ccf[peak_idx]\n\n    # Define a local region around the peak above a fractional threshold\n    threshold = self.centroid_threshold\n    cutoff = threshold * peak_val\n\n    # Expand to left of peak\n    i_left = peak_idx\n    while i_left &gt; 0 and ccf[i_left - 1] &gt;= cutoff:\n        i_left -= 1\n\n    # Expand to right of peak\n    i_right = peak_idx\n    while i_right &lt; len(ccf) - 1 and ccf[i_right + 1] &gt;= cutoff:\n        i_right += 1\n\n    # Compute centroid if region is valid\n    if i_right &gt;= i_left:\n        lags_subset = lags[i_left:i_right + 1]\n        ccf_subset = ccf[i_left:i_right + 1]\n        weight_sum = np.sum(ccf_subset)\n        if weight_sum &gt; 0:\n            centroid_lag = np.sum(lags_subset * ccf_subset) / weight_sum\n        else:\n            centroid_lag = np.nan\n    else:\n        centroid_lag = np.nan\n\n    return peak_lag, centroid_lag\n</code></pre>"},{"location":"reference/cross_correlation/#stela_toolkit.cross_correlation.CrossCorrelation.plot","title":"<code>plot(show_mc=True)</code>","text":"<p>Plot the cross-correlation function and optional Monte Carlo lag distributions.</p> Arguments <p>show_mc : bool     Plots results of Monte Carlo peak and centroid lag distributions.</p> Source code in <code>stela_toolkit/cross_correlation.py</code> <pre><code>def plot(self, show_mc=True):\n    \"\"\"\n    Plot the cross-correlation function and optional Monte Carlo lag distributions.\n\n    Arguments\n    ----------\n    show_mc : bool\n        Plots results of Monte Carlo peak and centroid lag distributions.\n    \"\"\"\n\n    fig, ax = plt.subplots(1, 1, figsize=(7, 4))\n    ax.plot(self.lags, self.ccf, label=\"CCF\", color='black')\n    ax.axvline(self.peak_lag, color='red', linestyle='--',\n               label=f\"Peak lag = {self.peak_lag:.2f}\")\n    ax.axvline(self.centroid_lag, color='blue', linestyle=':',\n               label=f\"Centroid lag = {self.centroid_lag:.2f}\")\n    ax.set_xlabel(\"Lag (same unit as input)\")\n    ax.set_ylabel(\"Correlation coefficient\")\n    ax.grid(True)\n    ax.legend()\n\n    if show_mc and self.peak_lags_mc is not None:\n        fig_mc, ax_mc = plt.subplots(1, 2, figsize=(10, 4))\n        ax_mc[0].hist(self.peak_lags_mc, bins=30, color='red', alpha=0.7)\n        ax_mc[0].set_title(\"Peak Lag Distribution (MC)\")\n        ax_mc[0].set_xlabel(\"Lag\")\n        ax_mc[0].set_ylabel(\"Count\")\n        ax_mc[0].grid(True)\n\n        ax_mc[1].hist(self.centroid_lags_mc, bins=30, color='blue', alpha=0.7)\n        ax_mc[1].set_title(\"Centroid Lag Distribution (MC)\")\n        ax_mc[1].set_xlabel(\"Lag\")\n        ax_mc[1].set_ylabel(\"Count\")\n        ax_mc[1].grid(True)\n\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"reference/cross_correlation/#stela_toolkit.cross_correlation.CrossCorrelation.run_monte_carlo","title":"<code>run_monte_carlo()</code>","text":"<p>Run Monte Carlo simulations to estimate lag uncertainties.</p> <p>Perturbs input light curves based on their errors and computes peak and centroid lags for each realization.</p> <p>Returns:</p> Name Type Description <code>peak_lags</code> <code>ndarray</code> <p>Peak lag values from all trials.</p> <code>centroid_lags</code> <code>ndarray</code> <p>Centroid lag values from all trials.</p> Source code in <code>stela_toolkit/cross_correlation.py</code> <pre><code>def run_monte_carlo(self):\n    \"\"\"\n    Run Monte Carlo simulations to estimate lag uncertainties.\n\n    Perturbs input light curves based on their errors and computes peak and centroid\n    lags for each realization.\n\n    Returns\n    -------\n    peak_lags : ndarray\n        Peak lag values from all trials.\n\n    centroid_lags : ndarray\n        Centroid lag values from all trials.\n    \"\"\"\n\n    peak_lags = []\n    centroid_lags = []\n\n    for _ in range(self.n_trials):\n        r1_pert = np.random.normal(self.rates1, self.errors1)\n        r2_pert = np.random.normal(self.rates2, self.errors2)\n\n        if self.mode == \"interp\":\n            interp1 = interp1d(self.times, r1_pert, bounds_error=False, fill_value=0.0)\n            interp2 = interp1d(self.times, r2_pert, bounds_error=False, fill_value=0.0)\n            ccf = []\n\n            for lag in self.lags:\n                t_shift1 = self.times + lag\n                t_shift2 = self.times - lag\n\n                mask1 = (t_shift1 &gt;= self.times[0]) &amp; (t_shift1 &lt;= self.times[-1])\n                mask2 = (t_shift2 &gt;= self.times[0]) &amp; (t_shift2 &lt;= self.times[-1])\n\n                if np.sum(mask1) &lt; 2 or np.sum(mask2) &lt; 2:\n                    ccf.append(0.0)\n                    continue\n\n                r1 = np.corrcoef(r1_pert[mask1], interp2(t_shift1[mask1]))[0, 1]\n                r2 = np.corrcoef(r2_pert[mask2], interp1(t_shift2[mask2]))[0, 1]\n                ccf_val = (r1 + r2) / 2\n                ccf.append(ccf_val)\n            ccf = np.array(ccf)\n        else:\n            ccf = self.compute_ccf(r1_pert, r2_pert)\n\n        if np.max(ccf) &lt; self.rmax_threshold:\n            continue\n\n        peak, centroid = self.find_peak_and_centroid(self.lags, ccf)\n        peak_lags.append(peak)\n        centroid_lags.append(centroid)\n\n    return np.array(peak_lags), np.array(centroid_lags)\n</code></pre>"},{"location":"reference/cross_spectrum/","title":"cross_spectrum","text":""},{"location":"reference/cross_spectrum/#stela_toolkit.cross_spectrum.CrossSpectrum","title":"<code>CrossSpectrum</code>","text":"<p>Compute the cross-spectrum between two light curves or trained Gaussian Process models.</p> <p>This class accepts LightCurve objects or GaussianProcess models from this package. For GP models, if posterior samples have already been generated, those are used. If not, the class automatically generates 1000 samples across a 1000-point grid.</p> <p>The cross-spectrum is computed using the Fourier transform of one time series multiplied by the complex conjugate of the other, yielding frequency-dependent phase and amplitude information.</p> <p>If both inputs are GP models, the cross-spectrum is computed across all sample pairs, and the mean and standard deviation across realizations are returned.</p> <p>Frequency binning is available with options for logarithmic, linear, or custom spacing.</p> <p>Parameters:</p> Name Type Description Default <code>lc_or_model1</code> <code>LightCurve or GaussianProcess</code> <p>First input light curve or trained GP model.</p> required <code>lc_or_model2</code> <code>LightCurve or GaussianProcess</code> <p>Second input light curve or trained GP model.</p> required <code>fmin</code> <code>float or auto</code> <p>Minimum frequency to include. If 'auto', uses lowest nonzero FFT frequency.</p> <code>'auto'</code> <code>fmax</code> <code>float or auto</code> <p>Maximum frequency to include. If 'auto', uses the Nyquist frequency.</p> <code>'auto'</code> <code>num_bins</code> <code>int</code> <p>Number of frequency bins.</p> <code>None</code> <code>bin_type</code> <code>str</code> <p>Binning type: 'log' or 'linear'.</p> <code>'log'</code> <code>bin_edges</code> <code>array - like</code> <p>Custom frequency bin edges. Overrides <code>num_bins</code> and <code>bin_type</code> if provided.</p> <code>[]</code> <code>norm</code> <code>bool</code> <p>Whether to normalize the cross-spectrum to variance units (i.e., PSD units).</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>freqs</code> <code>array - like</code> <p>Frequency bin centers.</p> <code>freq_widths</code> <code>array - like</code> <p>Frequency bin widths.</p> <code>cs</code> <code>array - like</code> <p>Complex cross-spectrum values.</p> <code>cs_errors</code> <code>array - like</code> <p>Uncertainties in the binned cross-spectrum (if stacked).</p> Source code in <code>stela_toolkit/cross_spectrum.py</code> <pre><code>class CrossSpectrum:\n    \"\"\"\n    Compute the cross-spectrum between two light curves or trained Gaussian Process models.\n\n    This class accepts LightCurve objects or GaussianProcess models from this package.\n    For GP models, if posterior samples have already been generated, those are used.\n    If not, the class automatically generates 1000 samples across a 1000-point grid.\n\n    The cross-spectrum is computed using the Fourier transform of one time series\n    multiplied by the complex conjugate of the other, yielding frequency-dependent phase\n    and amplitude information.\n\n    If both inputs are GP models, the cross-spectrum is computed across all sample pairs,\n    and the mean and standard deviation across realizations are returned.\n\n    Frequency binning is available with options for logarithmic, linear, or custom spacing.\n\n    Parameters\n    ----------\n    lc_or_model1 : LightCurve or GaussianProcess\n        First input light curve or trained GP model.\n\n    lc_or_model2 : LightCurve or GaussianProcess\n        Second input light curve or trained GP model.\n\n    fmin : float or 'auto', optional\n        Minimum frequency to include. If 'auto', uses lowest nonzero FFT frequency.\n\n    fmax : float or 'auto', optional\n        Maximum frequency to include. If 'auto', uses the Nyquist frequency.\n\n    num_bins : int, optional\n        Number of frequency bins.\n\n    bin_type : str, optional\n        Binning type: 'log' or 'linear'.\n\n    bin_edges : array-like, optional\n        Custom frequency bin edges. Overrides `num_bins` and `bin_type` if provided.\n\n    norm : bool, optional\n        Whether to normalize the cross-spectrum to variance units (i.e., PSD units).\n\n    Attributes\n    ----------\n    freqs : array-like\n        Frequency bin centers.\n\n    freq_widths : array-like\n        Frequency bin widths.\n\n    cs : array-like\n        Complex cross-spectrum values.\n\n    cs_errors : array-like\n        Uncertainties in the binned cross-spectrum (if stacked).\n    \"\"\"\n\n    def __init__(self,\n                 lc_or_model1,\n                 lc_or_model2,\n                 fmin='auto',\n                 fmax='auto',\n                 num_bins=None,\n                 bin_type=\"log\",\n                 bin_edges=[],\n                 norm=True):\n\n        # To do: update main docstring\n        input_data = _CheckInputs._check_lightcurve_or_model(lc_or_model1)\n        if input_data['type'] == 'model':\n            self.times1, self.rates1 = input_data['data']\n        else:\n            self.times1, self.rates1, _ = input_data['data']\n\n        input_data = _CheckInputs._check_lightcurve_or_model(lc_or_model2)\n        if input_data['type'] == 'model':\n            self.times2, self.rates2 = input_data['data']\n        else:\n            self.times2, self.rates2, _ = input_data['data']\n\n        _CheckInputs._check_input_bins(num_bins, bin_type, bin_edges)\n\n        if not np.allclose(self.times1, self.times2):\n            raise ValueError(\"The time arrays of the two light curves must be identical.\")\n\n        # Use absolute min and max frequencies if set to 'auto'\n        self.dt = np.diff(self.times1)[0]\n        self.fmin = np.fft.rfftfreq(len(self.rates1), d=self.dt)[1] if fmin == 'auto' else fmin\n        self.fmax = np.fft.rfftfreq(len(self.rates1), d=self.dt)[-1] if fmax == 'auto' else fmax  # nyquist frequency\n\n        self.num_bins = num_bins\n        self.bin_type = bin_type\n        self.bin_edges = bin_edges\n\n        # Check if the input rates are for multiple realizations\n        # this needs to be corrected for handling different shapes and dim val1 != dim val2\n        if len(self.rates1.shape) == 2 and len(self.rates2.shape) == 2:\n            cross_spectrum = self.compute_stacked_cross_spectrum(norm=norm)\n        else:\n            cross_spectrum = self.compute_cross_spectrum(norm=norm)\n\n        self.freqs, self.freq_widths, self.cs, self.cs_errors = cross_spectrum\n\n    def compute_cross_spectrum(self, times1=None, rates1=None, times2=None, rates2=None, norm=True):\n        \"\"\"\n        Compute the cross-spectrum for a single pair of light curves.\n\n        Parameters\n        ----------\n        times1 : array-like, optional\n            Time values for the first light curve.\n\n        rates1 : array-like, optional\n            Flux or count rate values for the first light curve.\n\n        times2 : array-like, optional\n            Time values for the second light curve.\n\n        rates2 : array-like, optional\n            Flux or count rate values for the second light curve.\n\n        norm : bool, optional\n            Whether to normalize the result to power spectral density units.\n\n        Returns\n        -------\n        freqs : array-like\n            Frequencies at which the cross-spectrum is evaluated.\n\n        freq_widths : array-like\n            Widths of frequency bins (for error bars or plotting).\n\n        cross_spectrum : array-like\n            Complex cross-spectrum values for each frequency bin.\n\n        cross_spectrum_errors : array-like or None\n            Uncertainties in the binned cross-spectrum values. None if unbinned.\n        \"\"\"\n\n        times1 = self.times1 if times1 is None else times1\n        rates1 = self.rates1 if rates1 is None else rates1\n        times2 = self.times2 if times2 is None else times2\n        rates2 = self.rates2 if rates2 is None else rates2\n\n        freqs, fft1 = LightCurve(times=times1, rates=rates1).fft()\n        _, fft2 = LightCurve(times=times2, rates=rates2).fft()\n\n        cross_spectrum = np.conj(fft2) * fft1\n\n        # Filter frequencies within [fmin, fmax]\n        valid_mask = (freqs &gt;= self.fmin) &amp; (freqs &lt;= self.fmax)\n        freqs = freqs[valid_mask]\n        cross_spectrum = cross_spectrum[valid_mask]\n\n        # Normalize power spectrum to units of variance (PSD)\n        if norm:\n            length = len(rates1)\n            norm_factor = length * np.mean(rates1) * np.mean(rates2) / (2 * self.dt)\n            cross_spectrum /= norm_factor\n\n            # negative norm factor shifts the phase by pi\n            if norm_factor &lt; 0:\n                phase = np.angle(cross_spectrum)\n                cross_spectrum = np.abs(cross_spectrum) * np.exp(1j * phase)\n\n        # Apply binning\n        if self.num_bins or self.bin_edges:\n            if self.bin_edges:\n                # use custom bin edges\n                bin_edges = FrequencyBinning.define_bins(\n                    self.fmin, self.fmax, num_bins=self.num_bins,\n                    bin_type=self.bin_type, bin_edges=self.bin_edges\n                )\n            elif self.num_bins:\n                # use equal-width bins in log or linear space\n                bin_edges = FrequencyBinning.define_bins(\n                    self.fmin, self.fmax, num_bins=self.num_bins, bin_type=self.bin_type\n                )\n            else:\n                raise ValueError(\"Either num_bins or bin_edges must be provided.\\n\"\n                                 \"In other words, you must specify the number of bins or the bin edges.\")\n\n            binned_cross_spectrum = FrequencyBinning.bin_data(freqs, cross_spectrum, bin_edges)\n            freqs, freq_widths, cross_spectrum, cross_spectrum_errors = binned_cross_spectrum\n        else:\n            freq_widths, cross_spectrum_errors = None, None\n\n        return freqs, freq_widths, cross_spectrum, cross_spectrum_errors\n\n    def compute_stacked_cross_spectrum(self, norm=True):\n        \"\"\"\n        Compute the cross-spectrum across stacked GP samples.\n\n        Computes the cross-spectrum for each realization and returns the mean and\n        standard deviation across samples.\n\n        Parameters\n        ----------\n        norm : bool, optional\n            Whether to normalize the result to power spectral density units.\n\n        Returns\n        -------\n        freqs : array-like\n            Frequencies of the cross-spectrum.\n\n        freq_widths : array-like\n            Widths of frequency bins.\n\n        cross_spectra_mean : array-like\n            Mean cross-spectrum across GP samples.\n\n        cross_spectra_std : array-like\n            Standard deviation of the cross-spectrum across samples.\n        \"\"\"\n\n        cross_spectra = []\n        for i in range(self.rates1.shape[0]):\n            cross_spectrum = self.compute_cross_spectrum(\n                times1=self.times1, rates1=self.rates1[i],\n                times2=self.times2, rates2=self.rates2[i],\n                norm=norm\n            )\n            cross_spectra.append(cross_spectrum[2])\n\n        cross_spectra = np.vstack(cross_spectra)\n        # Real and imaginary std devs\n        cs_real_mean = np.mean(cross_spectra.real, axis=0)\n        cs_imag_mean = np.mean(cross_spectra.imag, axis=0)\n        cs_real_std = np.std(cross_spectra.real, axis=0)\n        cs_imag_std = np.std(cross_spectra.imag, axis=0)\n\n        cross_spectra_mean = cs_real_mean + 1j * cs_imag_mean\n        cross_spectra_std = cs_real_std + 1j * cs_imag_std\n        freqs, freq_widths = cross_spectrum[0], cross_spectrum[1]\n\n        return freqs, freq_widths, cross_spectra_mean, cross_spectra_std\n\n    def plot(self, freqs=None, freq_widths=None, cs=None, cs_errors=None, **kwargs):\n        \"\"\"\n        Plot the real and imaginary parts of the cross-spectrum.\n\n        Parameters\n        ----------\n        freqs : array-like, optional\n            Frequencies at which the cross-spectrum is evaluated.\n\n        freq_widths : array-like, optional\n            Widths of the frequency bins.\n\n        cs : array-like, optional\n            Cross-spectrum values.\n\n        cs_errors : array-like, optional\n            Uncertainties in the cross-spectrum.\n\n        **kwargs : dict\n            Additional keyword arguments for plot customization.\n        \"\"\"\n        import matplotlib.pyplot as plt\n\n        freqs = self.freqs if freqs is None else freqs\n        freq_widths = self.freq_widths if freq_widths is None else freq_widths\n        cs = self.cs if cs is None else cs\n        cs_errors = self.cs_errors if cs_errors is None else cs_errors\n\n        figsize = kwargs.get('figsize', (8, 4.5))\n        xlabel = kwargs.get('xlabel', 'Frequency')\n        ylabel = kwargs.get('ylabel', 'Cross-Spectrum')\n        xscale = kwargs.get('xscale', 'log')\n        yscale = kwargs.get('yscale', 'log')\n\n        plt.figure(figsize=figsize)\n\n        # Real part\n        if cs_errors is not None:\n            plt.errorbar(freqs, cs.real, xerr=freq_widths, yerr=cs_errors.real,\n                         fmt='o', color='black', ms=3, lw=1.5, label='Real')\n        else:\n            plt.plot(freqs, cs.real, 'o-', color='black', ms=3, lw=1.5, label='Real')\n\n        # Imaginary part\n        if cs_errors is not None:\n            plt.errorbar(freqs, cs.imag, xerr=freq_widths, yerr=cs_errors.imag,\n                         fmt='o', color='red', ms=3, lw=1.5, label='Imag')\n        else:\n            plt.plot(freqs, cs.imag, 'o-', color='red', ms=3, lw=1.5, label='Imag')\n\n        plt.xlabel(xlabel, fontsize=12)\n        plt.ylabel(ylabel, fontsize=12)\n        plt.xscale(xscale)\n        plt.yscale(yscale)\n        plt.legend(loc='best')\n        plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n        plt.tick_params(which='both', direction='in', length=6, width=1,\n                        top=True, right=True, labelsize=12)\n        plt.show()\n\n    def count_frequencies_in_bins(self, fmin=None, fmax=None, num_bins=None, bin_type=None, bin_edges=[]):\n        \"\"\"\n        Counts the number of frequencies in each frequency bin.\n        Wrapper method to use FrequencyBinning.count_frequencies_in_bins with class attributes.\n        \"\"\"\n\n        return FrequencyBinning.count_frequencies_in_bins(\n            self, fmin=fmin, fmax=fmax, num_bins=num_bins, bin_type=bin_type, bin_edges=bin_edges\n        )\n</code></pre>"},{"location":"reference/cross_spectrum/#stela_toolkit.cross_spectrum.CrossSpectrum.compute_cross_spectrum","title":"<code>compute_cross_spectrum(times1=None, rates1=None, times2=None, rates2=None, norm=True)</code>","text":"<p>Compute the cross-spectrum for a single pair of light curves.</p> <p>Parameters:</p> Name Type Description Default <code>times1</code> <code>array - like</code> <p>Time values for the first light curve.</p> <code>None</code> <code>rates1</code> <code>array - like</code> <p>Flux or count rate values for the first light curve.</p> <code>None</code> <code>times2</code> <code>array - like</code> <p>Time values for the second light curve.</p> <code>None</code> <code>rates2</code> <code>array - like</code> <p>Flux or count rate values for the second light curve.</p> <code>None</code> <code>norm</code> <code>bool</code> <p>Whether to normalize the result to power spectral density units.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>freqs</code> <code>array - like</code> <p>Frequencies at which the cross-spectrum is evaluated.</p> <code>freq_widths</code> <code>array - like</code> <p>Widths of frequency bins (for error bars or plotting).</p> <code>cross_spectrum</code> <code>array - like</code> <p>Complex cross-spectrum values for each frequency bin.</p> <code>cross_spectrum_errors</code> <code>array - like or None</code> <p>Uncertainties in the binned cross-spectrum values. None if unbinned.</p> Source code in <code>stela_toolkit/cross_spectrum.py</code> <pre><code>def compute_cross_spectrum(self, times1=None, rates1=None, times2=None, rates2=None, norm=True):\n    \"\"\"\n    Compute the cross-spectrum for a single pair of light curves.\n\n    Parameters\n    ----------\n    times1 : array-like, optional\n        Time values for the first light curve.\n\n    rates1 : array-like, optional\n        Flux or count rate values for the first light curve.\n\n    times2 : array-like, optional\n        Time values for the second light curve.\n\n    rates2 : array-like, optional\n        Flux or count rate values for the second light curve.\n\n    norm : bool, optional\n        Whether to normalize the result to power spectral density units.\n\n    Returns\n    -------\n    freqs : array-like\n        Frequencies at which the cross-spectrum is evaluated.\n\n    freq_widths : array-like\n        Widths of frequency bins (for error bars or plotting).\n\n    cross_spectrum : array-like\n        Complex cross-spectrum values for each frequency bin.\n\n    cross_spectrum_errors : array-like or None\n        Uncertainties in the binned cross-spectrum values. None if unbinned.\n    \"\"\"\n\n    times1 = self.times1 if times1 is None else times1\n    rates1 = self.rates1 if rates1 is None else rates1\n    times2 = self.times2 if times2 is None else times2\n    rates2 = self.rates2 if rates2 is None else rates2\n\n    freqs, fft1 = LightCurve(times=times1, rates=rates1).fft()\n    _, fft2 = LightCurve(times=times2, rates=rates2).fft()\n\n    cross_spectrum = np.conj(fft2) * fft1\n\n    # Filter frequencies within [fmin, fmax]\n    valid_mask = (freqs &gt;= self.fmin) &amp; (freqs &lt;= self.fmax)\n    freqs = freqs[valid_mask]\n    cross_spectrum = cross_spectrum[valid_mask]\n\n    # Normalize power spectrum to units of variance (PSD)\n    if norm:\n        length = len(rates1)\n        norm_factor = length * np.mean(rates1) * np.mean(rates2) / (2 * self.dt)\n        cross_spectrum /= norm_factor\n\n        # negative norm factor shifts the phase by pi\n        if norm_factor &lt; 0:\n            phase = np.angle(cross_spectrum)\n            cross_spectrum = np.abs(cross_spectrum) * np.exp(1j * phase)\n\n    # Apply binning\n    if self.num_bins or self.bin_edges:\n        if self.bin_edges:\n            # use custom bin edges\n            bin_edges = FrequencyBinning.define_bins(\n                self.fmin, self.fmax, num_bins=self.num_bins,\n                bin_type=self.bin_type, bin_edges=self.bin_edges\n            )\n        elif self.num_bins:\n            # use equal-width bins in log or linear space\n            bin_edges = FrequencyBinning.define_bins(\n                self.fmin, self.fmax, num_bins=self.num_bins, bin_type=self.bin_type\n            )\n        else:\n            raise ValueError(\"Either num_bins or bin_edges must be provided.\\n\"\n                             \"In other words, you must specify the number of bins or the bin edges.\")\n\n        binned_cross_spectrum = FrequencyBinning.bin_data(freqs, cross_spectrum, bin_edges)\n        freqs, freq_widths, cross_spectrum, cross_spectrum_errors = binned_cross_spectrum\n    else:\n        freq_widths, cross_spectrum_errors = None, None\n\n    return freqs, freq_widths, cross_spectrum, cross_spectrum_errors\n</code></pre>"},{"location":"reference/cross_spectrum/#stela_toolkit.cross_spectrum.CrossSpectrum.compute_stacked_cross_spectrum","title":"<code>compute_stacked_cross_spectrum(norm=True)</code>","text":"<p>Compute the cross-spectrum across stacked GP samples.</p> <p>Computes the cross-spectrum for each realization and returns the mean and standard deviation across samples.</p> <p>Parameters:</p> Name Type Description Default <code>norm</code> <code>bool</code> <p>Whether to normalize the result to power spectral density units.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>freqs</code> <code>array - like</code> <p>Frequencies of the cross-spectrum.</p> <code>freq_widths</code> <code>array - like</code> <p>Widths of frequency bins.</p> <code>cross_spectra_mean</code> <code>array - like</code> <p>Mean cross-spectrum across GP samples.</p> <code>cross_spectra_std</code> <code>array - like</code> <p>Standard deviation of the cross-spectrum across samples.</p> Source code in <code>stela_toolkit/cross_spectrum.py</code> <pre><code>def compute_stacked_cross_spectrum(self, norm=True):\n    \"\"\"\n    Compute the cross-spectrum across stacked GP samples.\n\n    Computes the cross-spectrum for each realization and returns the mean and\n    standard deviation across samples.\n\n    Parameters\n    ----------\n    norm : bool, optional\n        Whether to normalize the result to power spectral density units.\n\n    Returns\n    -------\n    freqs : array-like\n        Frequencies of the cross-spectrum.\n\n    freq_widths : array-like\n        Widths of frequency bins.\n\n    cross_spectra_mean : array-like\n        Mean cross-spectrum across GP samples.\n\n    cross_spectra_std : array-like\n        Standard deviation of the cross-spectrum across samples.\n    \"\"\"\n\n    cross_spectra = []\n    for i in range(self.rates1.shape[0]):\n        cross_spectrum = self.compute_cross_spectrum(\n            times1=self.times1, rates1=self.rates1[i],\n            times2=self.times2, rates2=self.rates2[i],\n            norm=norm\n        )\n        cross_spectra.append(cross_spectrum[2])\n\n    cross_spectra = np.vstack(cross_spectra)\n    # Real and imaginary std devs\n    cs_real_mean = np.mean(cross_spectra.real, axis=0)\n    cs_imag_mean = np.mean(cross_spectra.imag, axis=0)\n    cs_real_std = np.std(cross_spectra.real, axis=0)\n    cs_imag_std = np.std(cross_spectra.imag, axis=0)\n\n    cross_spectra_mean = cs_real_mean + 1j * cs_imag_mean\n    cross_spectra_std = cs_real_std + 1j * cs_imag_std\n    freqs, freq_widths = cross_spectrum[0], cross_spectrum[1]\n\n    return freqs, freq_widths, cross_spectra_mean, cross_spectra_std\n</code></pre>"},{"location":"reference/cross_spectrum/#stela_toolkit.cross_spectrum.CrossSpectrum.count_frequencies_in_bins","title":"<code>count_frequencies_in_bins(fmin=None, fmax=None, num_bins=None, bin_type=None, bin_edges=[])</code>","text":"<p>Counts the number of frequencies in each frequency bin. Wrapper method to use FrequencyBinning.count_frequencies_in_bins with class attributes.</p> Source code in <code>stela_toolkit/cross_spectrum.py</code> <pre><code>def count_frequencies_in_bins(self, fmin=None, fmax=None, num_bins=None, bin_type=None, bin_edges=[]):\n    \"\"\"\n    Counts the number of frequencies in each frequency bin.\n    Wrapper method to use FrequencyBinning.count_frequencies_in_bins with class attributes.\n    \"\"\"\n\n    return FrequencyBinning.count_frequencies_in_bins(\n        self, fmin=fmin, fmax=fmax, num_bins=num_bins, bin_type=bin_type, bin_edges=bin_edges\n    )\n</code></pre>"},{"location":"reference/cross_spectrum/#stela_toolkit.cross_spectrum.CrossSpectrum.plot","title":"<code>plot(freqs=None, freq_widths=None, cs=None, cs_errors=None, **kwargs)</code>","text":"<p>Plot the real and imaginary parts of the cross-spectrum.</p> <p>Parameters:</p> Name Type Description Default <code>freqs</code> <code>array - like</code> <p>Frequencies at which the cross-spectrum is evaluated.</p> <code>None</code> <code>freq_widths</code> <code>array - like</code> <p>Widths of the frequency bins.</p> <code>None</code> <code>cs</code> <code>array - like</code> <p>Cross-spectrum values.</p> <code>None</code> <code>cs_errors</code> <code>array - like</code> <p>Uncertainties in the cross-spectrum.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments for plot customization.</p> <code>{}</code> Source code in <code>stela_toolkit/cross_spectrum.py</code> <pre><code>def plot(self, freqs=None, freq_widths=None, cs=None, cs_errors=None, **kwargs):\n    \"\"\"\n    Plot the real and imaginary parts of the cross-spectrum.\n\n    Parameters\n    ----------\n    freqs : array-like, optional\n        Frequencies at which the cross-spectrum is evaluated.\n\n    freq_widths : array-like, optional\n        Widths of the frequency bins.\n\n    cs : array-like, optional\n        Cross-spectrum values.\n\n    cs_errors : array-like, optional\n        Uncertainties in the cross-spectrum.\n\n    **kwargs : dict\n        Additional keyword arguments for plot customization.\n    \"\"\"\n    import matplotlib.pyplot as plt\n\n    freqs = self.freqs if freqs is None else freqs\n    freq_widths = self.freq_widths if freq_widths is None else freq_widths\n    cs = self.cs if cs is None else cs\n    cs_errors = self.cs_errors if cs_errors is None else cs_errors\n\n    figsize = kwargs.get('figsize', (8, 4.5))\n    xlabel = kwargs.get('xlabel', 'Frequency')\n    ylabel = kwargs.get('ylabel', 'Cross-Spectrum')\n    xscale = kwargs.get('xscale', 'log')\n    yscale = kwargs.get('yscale', 'log')\n\n    plt.figure(figsize=figsize)\n\n    # Real part\n    if cs_errors is not None:\n        plt.errorbar(freqs, cs.real, xerr=freq_widths, yerr=cs_errors.real,\n                     fmt='o', color='black', ms=3, lw=1.5, label='Real')\n    else:\n        plt.plot(freqs, cs.real, 'o-', color='black', ms=3, lw=1.5, label='Real')\n\n    # Imaginary part\n    if cs_errors is not None:\n        plt.errorbar(freqs, cs.imag, xerr=freq_widths, yerr=cs_errors.imag,\n                     fmt='o', color='red', ms=3, lw=1.5, label='Imag')\n    else:\n        plt.plot(freqs, cs.imag, 'o-', color='red', ms=3, lw=1.5, label='Imag')\n\n    plt.xlabel(xlabel, fontsize=12)\n    plt.ylabel(ylabel, fontsize=12)\n    plt.xscale(xscale)\n    plt.yscale(yscale)\n    plt.legend(loc='best')\n    plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n    plt.tick_params(which='both', direction='in', length=6, width=1,\n                    top=True, right=True, labelsize=12)\n    plt.show()\n</code></pre>"},{"location":"reference/data_loader/","title":"data_loader","text":""},{"location":"reference/data_loader/#stela_toolkit.data_loader.LightCurve","title":"<code>LightCurve</code>","text":"<p>Container for light curve data, including time, rate, and optional error arrays.</p> <p>This class is the standard format for handling time series in the STELA Toolkit. Light curves can be initialized directly from arrays or loaded from supported file formats (FITS, CSV, or plain text). Many analysis modules assume regular time sampling, which is  enforced or checked as needed.</p> <p>Supports basic arithmetic operations (addition, subtraction, division) with other LightCurve  objects and provides utilities for plotting and computing Fourier transforms.</p> <p>Parameters:</p> Name Type Description Default <code>times</code> <code>array - like</code> <p>Array of time values.</p> <code>[]</code> <code>rates</code> <code>array - like</code> <p>Array of measured rates (e.g., flux, count rate).</p> <code>[]</code> <code>errors</code> <code>array - like</code> <p>Array of uncertainties on the rates. Optional but recommended.</p> <code>[]</code> <code>file_path</code> <code>str</code> <p>Path to a file to load light curve data from. Supports FITS and text formats.</p> <code>None</code> <code>file_columns</code> <code>list of int or str</code> <p>List specifying the columns to read as [time, rate, error]. Column names or indices allowed.</p> <code>[0, 1, 2]</code> <p>Attributes:</p> Name Type Description <code>times</code> <code>ndarray</code> <p>Array of time values.</p> <code>rates</code> <code>ndarray</code> <p>Array of rate values.</p> <code>errors</code> <code>ndarray</code> <p>Array of errors, if provided.</p> Source code in <code>stela_toolkit/data_loader.py</code> <pre><code>class LightCurve:\n    \"\"\"\n    Container for light curve data, including time, rate, and optional error arrays.\n\n    This class is the standard format for handling time series in the STELA Toolkit.\n    Light curves can be initialized directly from arrays or loaded from supported file formats\n    (FITS, CSV, or plain text). Many analysis modules assume regular time sampling, which is \n    enforced or checked as needed.\n\n    Supports basic arithmetic operations (addition, subtraction, division) with other LightCurve \n    objects and provides utilities for plotting and computing Fourier transforms.\n\n    Parameters\n    ----------\n    times : array-like, optional\n        Array of time values.\n\n    rates : array-like, optional\n        Array of measured rates (e.g., flux, count rate).\n\n    errors : array-like, optional\n        Array of uncertainties on the rates. Optional but recommended.\n\n    file_path : str, optional\n        Path to a file to load light curve data from. Supports FITS and text formats.\n\n    file_columns : list of int or str, optional\n        List specifying the columns to read as [time, rate, error]. Column names or indices allowed.\n\n    Attributes\n    ----------\n    times : ndarray\n        Array of time values.\n\n    rates : ndarray\n        Array of rate values.\n\n    errors : ndarray\n        Array of errors, if provided.\n    \"\"\"\n\n    def __init__(self,\n                 times=[],\n                 rates=[],\n                 errors=[],\n                 file_path=None,\n                 file_columns=[0, 1, 2]):\n\n        if file_path:\n            if not (2 &lt;= len(file_columns) &lt;= 3):\n                raise ValueError(\n                    \"The 'file_columns' parameter must be a list with 2 or 3 items: \"\n                    \"[time_column, rate_column, optional error_column].\"\n                )\n\n            file_data = self.load_file(file_path, file_columns=file_columns)\n            times, rates, errors = file_data\n\n        elif len(times) &gt; 0 and len(rates) &gt; 0:\n            pass\n\n        else:\n            raise ValueError(\n                \"Please provide time and rate arrays or a file path.\"\n            )\n\n        self.times, self.rates, self.errors = _CheckInputs._check_input_data(lightcurve=None,\n                                                                             times=times,\n                                                                             rates=rates,\n                                                                             errors=errors,\n                                                                             req_reg_samp=False\n                                                                             )\n\n    @property\n    def mean(self):\n        \"\"\"\n        Return the mean of the light curve rates.\n        \"\"\"\n        return np.mean(self.rates)\n\n    @property\n    def std(self):\n        \"\"\"\n        Return the standard deviation of the light curve rates.\n        \"\"\"\n        return np.std(self.rates)\n\n    def load_file(self, file_path, file_columns=[0, 1, 2]):\n        \"\"\"\n        Load light curve data from a FITS or text-based file.\n\n        Parameters\n        ----------\n        file_path : str\n            Path to the input file.\n\n        file_columns : list of int or str, optional\n            Column indices or names to use for [time, rate, error].\n            Defaults to [0, 1, 2].\n\n        Returns\n        -------\n        times : ndarray\n            Array of time values.\n\n        rates : ndarray\n            Array of flux or count rate values.\n\n        errors : ndarray\n            Array of measurement uncertainties.\n        \"\"\"\n\n        try:\n            times, rates, errors = self.load_fits(file_path, file_columns)\n\n        except:\n            try:\n                times, rates, errors = self.load_text_file(file_path, file_columns)\n            except Exception as e:\n                raise RuntimeError(\n                    f\"Failed to read the file '{file_path}' with fits or text-based loader.\\n\"\n                    \"Verify the file path and file_columns, or file format unsupported.\\n\"\n                    f\"Error message: {e}\"\n                )\n\n        return times, rates, errors\n\n    def load_fits(self, file_path, file_columns=[0, 1, 2], hdu=1):\n        \"\"\"\n        Load light curve data from a specified HDU of a FITS file.\n\n        Parameters\n        ----------\n        file_path : str\n            Path to the FITS file.\n\n        file_columns : list of int or str\n            Columns to extract, specified as [time, rate, error].\n\n        hdu : int\n            Index of the Header/Data Unit (HDU) to read from.\n\n        Returns\n        -------\n        times : ndarray\n            Array of time values.\n\n        rates : ndarray\n            Array of flux or count rate values.\n\n        errors : ndarray\n            Array of measurement uncertainties.\n        \"\"\"\n\n        time_column, rate_column = file_columns[0], file_columns[1]\n        error_column = file_columns[2] if len(file_columns) == 3 else None\n\n        with fits.open(file_path) as hdul:\n            try:\n                data = hdul[hdu].data\n            except IndexError:\n                raise ValueError(f\"HDU {hdu} does not exist in the FITS file.\")\n\n            try:\n                times = np.array(\n                    data.field(time_column) if isinstance(time_column, int)\n                    else data[time_column]\n                ).astype(float)\n\n                rates = np.array(\n                    data.field(rate_column) if isinstance(rate_column, int)\n                    else data[rate_column]\n                ).astype(float)\n\n                if error_column:\n                    errors = np.array(\n                        data.field(error_column) if isinstance(error_column, int)\n                        else data[error_column]\n                    ).astype(float)\n                else:\n                    errors = []\n\n            except KeyError:\n                raise ValueError(\n                    \"Specified column/s not found in the FITS file.\"\n                )\n\n        return times, rates, errors\n\n    def load_text_file(self, file_path, file_columns=[0, 1, 2], delimiter=None):\n        \"\"\"\n        Load light curve data from a CSV or plain-text file.\n\n        Parameters\n        ----------\n        file_path : str\n            Path to the input text file.\n\n        file_columns : list of int or str\n            Columns to extract, specified as [time, rate, error].\n\n        delimiter : str, optional\n            Delimiter used in the file (e.g., ',' for CSV, '\\\\t' for tab-separated).\n            If not provided, the delimiter is inferred from the file extension.\n\n        Returns\n        -------\n        times : ndarray\n            Array of time values.\n\n        rates : ndarray\n            Array of flux or count rate values.\n\n        errors : ndarray\n            Array of measurement uncertainties.\n        \"\"\"\n\n        time_column, rate_column = file_columns[0], file_columns[1]\n        error_column = file_columns[2] if len(file_columns) == 3 else None\n\n        # Load data, assuming delimiter based on file extension if unspecified\n        if delimiter is None:\n            delimiter = ',' if file_path.endswith('.csv') else None\n\n        try:\n            data = np.genfromtxt(\n                file_path,\n                delimiter=delimiter,\n            )\n\n        except Exception as e:\n            raise (f\"Failed to read the file '{file_path}' with np.genfromtxt.\")\n\n        # Retrieve file_columns by name or index directly, simplifying access\n        times = np.array(\n            data[time_column] if isinstance(time_column, str)\n            else data[:, time_column]\n        ).astype(float)\n\n        rates = np.array(\n            data[rate_column] if isinstance(rate_column, str)\n            else data[:, rate_column]\n        ).astype(float)\n\n        if error_column:\n            errors = np.array(\n                data[error_column]if isinstance(error_column, str)\n                else data[:, error_column]\n            ).astype(float)\n\n        else:\n            errors = []\n\n        return times, rates, errors\n\n    def plot(self, **kwargs):\n        \"\"\"\n        Plot the light curve.\n\n        Parameters\n        ----------\n        **kwargs : dict\n            Additional keyword arguments for customizing the plot.\n        \"\"\"\n\n        kwargs.setdefault('xlabel', 'Time')\n        kwargs.setdefault('ylabel', 'Rate')\n        Plotter.plot(x=self.times, y=self.rates, yerr=self.errors, **kwargs)\n\n    def fft(self):\n        \"\"\"\n        Compute the Fast Fourier Transform (FFT) of the light curve.\n\n        Returns\n        -------\n        freqs : ndarray\n            Frequencies of the FFT.\n\n        fft_values : ndarray\n            Complex FFT values.\n\n        Raises\n        ------\n        ValueError\n            If the time sampling is not uniform. Interpolation is required before applying FFT.\n        \"\"\"\n\n        time_diffs = np.round(np.diff(self.times), 10)\n        if np.unique(time_diffs).size &gt; 1:\n            raise ValueError(\"Light curve must have a uniform sampling interval.\\n\"\n                             \"Interpolate the data to a uniform grid first.\"\n                             )\n        dt = np.diff(self.times)[0]\n        length = len(self.rates)\n\n        fft_values = np.fft.rfft(self.rates)\n        freqs = np.fft.rfftfreq(length, d=dt)\n\n        return freqs, fft_values\n\n    def __add__(self, other_lightcurve):\n        \"\"\"\n        Add two LightCurve objects element-wise.\n\n        Returns\n        -------\n        LightCurve\n            New LightCurve with summed rates and propagated uncertainties.\n        \"\"\"\n\n        if not isinstance(other_lightcurve, LightCurve):\n            raise TypeError(\n                \"Both light curve must be an instance of the LightCurve class.\"\n            )\n\n        if not np.array_equal(self.times, other_lightcurve.times):\n            raise ValueError(\"Time arrays do not match.\")\n\n        new_rates = self.rates + other_lightcurve.rates\n        if self.errors.size == 0 or other_lightcurve.errors.size == 0:\n            new_errors = []\n\n        else:\n            new_errors = np.sqrt(self.errors**2 + other_lightcurve.errors**2)\n\n        return LightCurve(times=self.times,\n                          rates=new_rates,\n                          errors=new_errors)\n\n    def __sub__(self, other_lightcurve):\n        \"\"\"\n        Subtract one LightCurve from another element-wise.\n\n        Returns\n        -------\n        LightCurve\n            New LightCurve with difference of rates and propagated uncertainties.\n        \"\"\"\n\n        if not isinstance(other_lightcurve, LightCurve):\n            raise TypeError(\n                \"Both light curve must be an instance of the LightCurve class.\"\n            )\n\n        if not np.array_equal(self.times, other_lightcurve.times):\n            raise ValueError(\"Time arrays do not match.\")\n\n        new_rates = self.rates - other_lightcurve.rates\n        if self.errors.size == 0 or other_lightcurve.errors.size == 0:\n            new_errors = []\n\n        else:\n            new_errors = np.sqrt(self.errors**2 + other_lightcurve.errors**2)\n\n        return LightCurve(times=self.times,\n                          rates=new_rates,\n                          errors=new_errors\n                          )\n\n    def __truediv__(self, other_lightcurve):\n        \"\"\"\n        Divide one LightCurve by another element-wise.\n\n        Returns\n        -------\n        LightCurve\n            New LightCurve with element-wise division and propagated relative uncertainties.\n        \"\"\"\n\n        if not isinstance(other_lightcurve, LightCurve):\n            raise TypeError(\n                \"Both light curve must be an instance of the LightCurve class.\"\n            )\n\n        if not np.array_equal(self.times, other_lightcurve.times):\n            raise ValueError(\"Time arrays do not match.\")\n\n        new_rates = self.rates / other_lightcurve.rates\n        if self.errors.size == 0 or other_lightcurve.errors.size == 0:\n            new_errors = []\n\n        else:\n            new_errors = np.sqrt(\n                (self.errors / self.rates) ** 2\n                + (other_lightcurve.errors / other_lightcurve.rates) ** 2\n            )\n\n        return LightCurve(times=self.times,\n                          rates=new_rates,\n                          errors=new_errors)\n</code></pre>"},{"location":"reference/data_loader/#stela_toolkit.data_loader.LightCurve.mean","title":"<code>mean</code>  <code>property</code>","text":"<p>Return the mean of the light curve rates.</p>"},{"location":"reference/data_loader/#stela_toolkit.data_loader.LightCurve.std","title":"<code>std</code>  <code>property</code>","text":"<p>Return the standard deviation of the light curve rates.</p>"},{"location":"reference/data_loader/#stela_toolkit.data_loader.LightCurve.__add__","title":"<code>__add__(other_lightcurve)</code>","text":"<p>Add two LightCurve objects element-wise.</p> <p>Returns:</p> Type Description <code>LightCurve</code> <p>New LightCurve with summed rates and propagated uncertainties.</p> Source code in <code>stela_toolkit/data_loader.py</code> <pre><code>def __add__(self, other_lightcurve):\n    \"\"\"\n    Add two LightCurve objects element-wise.\n\n    Returns\n    -------\n    LightCurve\n        New LightCurve with summed rates and propagated uncertainties.\n    \"\"\"\n\n    if not isinstance(other_lightcurve, LightCurve):\n        raise TypeError(\n            \"Both light curve must be an instance of the LightCurve class.\"\n        )\n\n    if not np.array_equal(self.times, other_lightcurve.times):\n        raise ValueError(\"Time arrays do not match.\")\n\n    new_rates = self.rates + other_lightcurve.rates\n    if self.errors.size == 0 or other_lightcurve.errors.size == 0:\n        new_errors = []\n\n    else:\n        new_errors = np.sqrt(self.errors**2 + other_lightcurve.errors**2)\n\n    return LightCurve(times=self.times,\n                      rates=new_rates,\n                      errors=new_errors)\n</code></pre>"},{"location":"reference/data_loader/#stela_toolkit.data_loader.LightCurve.__sub__","title":"<code>__sub__(other_lightcurve)</code>","text":"<p>Subtract one LightCurve from another element-wise.</p> <p>Returns:</p> Type Description <code>LightCurve</code> <p>New LightCurve with difference of rates and propagated uncertainties.</p> Source code in <code>stela_toolkit/data_loader.py</code> <pre><code>def __sub__(self, other_lightcurve):\n    \"\"\"\n    Subtract one LightCurve from another element-wise.\n\n    Returns\n    -------\n    LightCurve\n        New LightCurve with difference of rates and propagated uncertainties.\n    \"\"\"\n\n    if not isinstance(other_lightcurve, LightCurve):\n        raise TypeError(\n            \"Both light curve must be an instance of the LightCurve class.\"\n        )\n\n    if not np.array_equal(self.times, other_lightcurve.times):\n        raise ValueError(\"Time arrays do not match.\")\n\n    new_rates = self.rates - other_lightcurve.rates\n    if self.errors.size == 0 or other_lightcurve.errors.size == 0:\n        new_errors = []\n\n    else:\n        new_errors = np.sqrt(self.errors**2 + other_lightcurve.errors**2)\n\n    return LightCurve(times=self.times,\n                      rates=new_rates,\n                      errors=new_errors\n                      )\n</code></pre>"},{"location":"reference/data_loader/#stela_toolkit.data_loader.LightCurve.__truediv__","title":"<code>__truediv__(other_lightcurve)</code>","text":"<p>Divide one LightCurve by another element-wise.</p> <p>Returns:</p> Type Description <code>LightCurve</code> <p>New LightCurve with element-wise division and propagated relative uncertainties.</p> Source code in <code>stela_toolkit/data_loader.py</code> <pre><code>def __truediv__(self, other_lightcurve):\n    \"\"\"\n    Divide one LightCurve by another element-wise.\n\n    Returns\n    -------\n    LightCurve\n        New LightCurve with element-wise division and propagated relative uncertainties.\n    \"\"\"\n\n    if not isinstance(other_lightcurve, LightCurve):\n        raise TypeError(\n            \"Both light curve must be an instance of the LightCurve class.\"\n        )\n\n    if not np.array_equal(self.times, other_lightcurve.times):\n        raise ValueError(\"Time arrays do not match.\")\n\n    new_rates = self.rates / other_lightcurve.rates\n    if self.errors.size == 0 or other_lightcurve.errors.size == 0:\n        new_errors = []\n\n    else:\n        new_errors = np.sqrt(\n            (self.errors / self.rates) ** 2\n            + (other_lightcurve.errors / other_lightcurve.rates) ** 2\n        )\n\n    return LightCurve(times=self.times,\n                      rates=new_rates,\n                      errors=new_errors)\n</code></pre>"},{"location":"reference/data_loader/#stela_toolkit.data_loader.LightCurve.fft","title":"<code>fft()</code>","text":"<p>Compute the Fast Fourier Transform (FFT) of the light curve.</p> <p>Returns:</p> Name Type Description <code>freqs</code> <code>ndarray</code> <p>Frequencies of the FFT.</p> <code>fft_values</code> <code>ndarray</code> <p>Complex FFT values.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the time sampling is not uniform. Interpolation is required before applying FFT.</p> Source code in <code>stela_toolkit/data_loader.py</code> <pre><code>def fft(self):\n    \"\"\"\n    Compute the Fast Fourier Transform (FFT) of the light curve.\n\n    Returns\n    -------\n    freqs : ndarray\n        Frequencies of the FFT.\n\n    fft_values : ndarray\n        Complex FFT values.\n\n    Raises\n    ------\n    ValueError\n        If the time sampling is not uniform. Interpolation is required before applying FFT.\n    \"\"\"\n\n    time_diffs = np.round(np.diff(self.times), 10)\n    if np.unique(time_diffs).size &gt; 1:\n        raise ValueError(\"Light curve must have a uniform sampling interval.\\n\"\n                         \"Interpolate the data to a uniform grid first.\"\n                         )\n    dt = np.diff(self.times)[0]\n    length = len(self.rates)\n\n    fft_values = np.fft.rfft(self.rates)\n    freqs = np.fft.rfftfreq(length, d=dt)\n\n    return freqs, fft_values\n</code></pre>"},{"location":"reference/data_loader/#stela_toolkit.data_loader.LightCurve.load_file","title":"<code>load_file(file_path, file_columns=[0, 1, 2])</code>","text":"<p>Load light curve data from a FITS or text-based file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the input file.</p> required <code>file_columns</code> <code>list of int or str</code> <p>Column indices or names to use for [time, rate, error]. Defaults to [0, 1, 2].</p> <code>[0, 1, 2]</code> <p>Returns:</p> Name Type Description <code>times</code> <code>ndarray</code> <p>Array of time values.</p> <code>rates</code> <code>ndarray</code> <p>Array of flux or count rate values.</p> <code>errors</code> <code>ndarray</code> <p>Array of measurement uncertainties.</p> Source code in <code>stela_toolkit/data_loader.py</code> <pre><code>def load_file(self, file_path, file_columns=[0, 1, 2]):\n    \"\"\"\n    Load light curve data from a FITS or text-based file.\n\n    Parameters\n    ----------\n    file_path : str\n        Path to the input file.\n\n    file_columns : list of int or str, optional\n        Column indices or names to use for [time, rate, error].\n        Defaults to [0, 1, 2].\n\n    Returns\n    -------\n    times : ndarray\n        Array of time values.\n\n    rates : ndarray\n        Array of flux or count rate values.\n\n    errors : ndarray\n        Array of measurement uncertainties.\n    \"\"\"\n\n    try:\n        times, rates, errors = self.load_fits(file_path, file_columns)\n\n    except:\n        try:\n            times, rates, errors = self.load_text_file(file_path, file_columns)\n        except Exception as e:\n            raise RuntimeError(\n                f\"Failed to read the file '{file_path}' with fits or text-based loader.\\n\"\n                \"Verify the file path and file_columns, or file format unsupported.\\n\"\n                f\"Error message: {e}\"\n            )\n\n    return times, rates, errors\n</code></pre>"},{"location":"reference/data_loader/#stela_toolkit.data_loader.LightCurve.load_fits","title":"<code>load_fits(file_path, file_columns=[0, 1, 2], hdu=1)</code>","text":"<p>Load light curve data from a specified HDU of a FITS file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the FITS file.</p> required <code>file_columns</code> <code>list of int or str</code> <p>Columns to extract, specified as [time, rate, error].</p> <code>[0, 1, 2]</code> <code>hdu</code> <code>int</code> <p>Index of the Header/Data Unit (HDU) to read from.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>times</code> <code>ndarray</code> <p>Array of time values.</p> <code>rates</code> <code>ndarray</code> <p>Array of flux or count rate values.</p> <code>errors</code> <code>ndarray</code> <p>Array of measurement uncertainties.</p> Source code in <code>stela_toolkit/data_loader.py</code> <pre><code>def load_fits(self, file_path, file_columns=[0, 1, 2], hdu=1):\n    \"\"\"\n    Load light curve data from a specified HDU of a FITS file.\n\n    Parameters\n    ----------\n    file_path : str\n        Path to the FITS file.\n\n    file_columns : list of int or str\n        Columns to extract, specified as [time, rate, error].\n\n    hdu : int\n        Index of the Header/Data Unit (HDU) to read from.\n\n    Returns\n    -------\n    times : ndarray\n        Array of time values.\n\n    rates : ndarray\n        Array of flux or count rate values.\n\n    errors : ndarray\n        Array of measurement uncertainties.\n    \"\"\"\n\n    time_column, rate_column = file_columns[0], file_columns[1]\n    error_column = file_columns[2] if len(file_columns) == 3 else None\n\n    with fits.open(file_path) as hdul:\n        try:\n            data = hdul[hdu].data\n        except IndexError:\n            raise ValueError(f\"HDU {hdu} does not exist in the FITS file.\")\n\n        try:\n            times = np.array(\n                data.field(time_column) if isinstance(time_column, int)\n                else data[time_column]\n            ).astype(float)\n\n            rates = np.array(\n                data.field(rate_column) if isinstance(rate_column, int)\n                else data[rate_column]\n            ).astype(float)\n\n            if error_column:\n                errors = np.array(\n                    data.field(error_column) if isinstance(error_column, int)\n                    else data[error_column]\n                ).astype(float)\n            else:\n                errors = []\n\n        except KeyError:\n            raise ValueError(\n                \"Specified column/s not found in the FITS file.\"\n            )\n\n    return times, rates, errors\n</code></pre>"},{"location":"reference/data_loader/#stela_toolkit.data_loader.LightCurve.load_text_file","title":"<code>load_text_file(file_path, file_columns=[0, 1, 2], delimiter=None)</code>","text":"<p>Load light curve data from a CSV or plain-text file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the input text file.</p> required <code>file_columns</code> <code>list of int or str</code> <p>Columns to extract, specified as [time, rate, error].</p> <code>[0, 1, 2]</code> <code>delimiter</code> <code>str</code> <p>Delimiter used in the file (e.g., ',' for CSV, '\\t' for tab-separated). If not provided, the delimiter is inferred from the file extension.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>times</code> <code>ndarray</code> <p>Array of time values.</p> <code>rates</code> <code>ndarray</code> <p>Array of flux or count rate values.</p> <code>errors</code> <code>ndarray</code> <p>Array of measurement uncertainties.</p> Source code in <code>stela_toolkit/data_loader.py</code> <pre><code>def load_text_file(self, file_path, file_columns=[0, 1, 2], delimiter=None):\n    \"\"\"\n    Load light curve data from a CSV or plain-text file.\n\n    Parameters\n    ----------\n    file_path : str\n        Path to the input text file.\n\n    file_columns : list of int or str\n        Columns to extract, specified as [time, rate, error].\n\n    delimiter : str, optional\n        Delimiter used in the file (e.g., ',' for CSV, '\\\\t' for tab-separated).\n        If not provided, the delimiter is inferred from the file extension.\n\n    Returns\n    -------\n    times : ndarray\n        Array of time values.\n\n    rates : ndarray\n        Array of flux or count rate values.\n\n    errors : ndarray\n        Array of measurement uncertainties.\n    \"\"\"\n\n    time_column, rate_column = file_columns[0], file_columns[1]\n    error_column = file_columns[2] if len(file_columns) == 3 else None\n\n    # Load data, assuming delimiter based on file extension if unspecified\n    if delimiter is None:\n        delimiter = ',' if file_path.endswith('.csv') else None\n\n    try:\n        data = np.genfromtxt(\n            file_path,\n            delimiter=delimiter,\n        )\n\n    except Exception as e:\n        raise (f\"Failed to read the file '{file_path}' with np.genfromtxt.\")\n\n    # Retrieve file_columns by name or index directly, simplifying access\n    times = np.array(\n        data[time_column] if isinstance(time_column, str)\n        else data[:, time_column]\n    ).astype(float)\n\n    rates = np.array(\n        data[rate_column] if isinstance(rate_column, str)\n        else data[:, rate_column]\n    ).astype(float)\n\n    if error_column:\n        errors = np.array(\n            data[error_column]if isinstance(error_column, str)\n            else data[:, error_column]\n        ).astype(float)\n\n    else:\n        errors = []\n\n    return times, rates, errors\n</code></pre>"},{"location":"reference/data_loader/#stela_toolkit.data_loader.LightCurve.plot","title":"<code>plot(**kwargs)</code>","text":"<p>Plot the light curve.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments for customizing the plot.</p> <code>{}</code> Source code in <code>stela_toolkit/data_loader.py</code> <pre><code>def plot(self, **kwargs):\n    \"\"\"\n    Plot the light curve.\n\n    Parameters\n    ----------\n    **kwargs : dict\n        Additional keyword arguments for customizing the plot.\n    \"\"\"\n\n    kwargs.setdefault('xlabel', 'Time')\n    kwargs.setdefault('ylabel', 'Rate')\n    Plotter.plot(x=self.times, y=self.rates, yerr=self.errors, **kwargs)\n</code></pre>"},{"location":"reference/data_simulator/","title":"data_simulator","text":""},{"location":"reference/data_simulator/#stela_toolkit.data_simulator.SimulateLightCurve","title":"<code>SimulateLightCurve</code>","text":"<p>Simulates light curves with a specified power spectral density (PSD) and optional lag injection via an impulse response function (IRF), using the Timmer &amp; Koenig (1995) method.</p> <p>The light curve is generated in Fourier space, with random phases and amplitudes shaped by the PSD. It is then inverse-Fourier transformed to the time domain, rescaled to the desired mean and standard deviation, and optionally passed through Poisson or Gaussian noise models.</p> <p>Supports both regularly and irregularly sampled time grids: - For regular grids: oversamples the light curve before trimming to the desired grid. - For irregular grids: simulates on a fine grid and selects nearest times (no interpolation).</p> <p>Optional lag injection is applied via convolution with an impulse response kernel.</p> <p>Parameters:</p> Name Type Description Default <code>time_grid</code> <code>ndarray</code> <p>Target time grid for the simulated light curve (must be sorted).</p> required <code>psd_type</code> <code>str</code> <p>Type of PSD to simulate. Options are:</p> <ul> <li>'powerlaw'</li> <li>'broken_powerlaw'</li> </ul> required <code>psd_params</code> <code>dict</code> <p>Parameters for the PSD model. Depends on <code>psd_type</code>:</p> <ul> <li>'powerlaw': {'slope', 'plnorm'}</li> <li>'broken_powerlaw': {'slope1', 'f_break', 'slope2', 'plnorm'}</li> </ul> required <code>mean</code> <code>float</code> <p>Desired mean of the light curve after rescaling.</p> required <code>std</code> <code>float</code> <p>Desired standard deviation of the light curve after rescaling.</p> required <code>add_noise</code> <code>str or None</code> <p>Type of noise to add. Options are:</p> <ul> <li>\"Poisson\": adds Poisson-distributed noise with optional background</li> <li>\"Gaussian\": adds Gaussian noise using <code>gaussian_frac_err</code></li> <li>None: no noise is added</li> </ul> <code>None</code> <code>gaussian_frac_err</code> <code>float or None</code> <p>Fractional error to use when <code>add_noise=\"Gaussian\"</code>.</p> <code>None</code> <code>bkg_rate</code> <code>float</code> <p>Background count rate (used in Poisson noise simulation). Default is 0.</p> <code>0.0</code> <code>oversample</code> <code>int</code> <p>Oversampling factor for regularly spaced grids. Default is 10.</p> <code>10</code> <code>fine_factor</code> <code>int</code> <p>Factor controlling the resolution of simulation for irregular grids. Default is 100.</p> <code>100</code> <code>inject_lag</code> <code>bool</code> <p>Whether to apply a lag by convolving with an impulse response function. Default is False.</p> <code>False</code> <code>response_type</code> <code>str or None</code> <p>Type of IRF for lag injection. Options are:</p> <ul> <li>'delta', 'normal', 'lognormal', 'manual', or None</li> </ul> <code>None</code> <code>response_params</code> <code>dict or None</code> <p>Parameters for the IRF. Depends on <code>response_type</code>:</p> <ul> <li>'delta': {'lag'}</li> <li>'normal': {'mean', 'sigma', 'duration' (optional)}</li> <li>'lognormal': {'median', 'sigma', 'duration' (optional)}</li> <li>'manual': {'response': array_like}</li> </ul> <code>None</code> <p>Attributes:</p> Name Type Description <code>rates</code> <code>ndarray</code> <p>Simulated light curve values after noise is added.</p> <code>errors</code> <code>ndarray</code> <p>Estimated uncertainties for each time point.</p> <code>simlc</code> <code>LightCurve</code> <p>The simulated light curve object (with noise).</p> <code>simlc_lagged</code> <code>LightCurve or None</code> <p>Lagged version of the light curve, if <code>inject_lag=True</code>. Otherwise, None.</p> Source code in <code>stela_toolkit/data_simulator.py</code> <pre><code>class SimulateLightCurve:\n    \"\"\"\n    Simulates light curves with a specified power spectral density (PSD) and optional lag injection\n    via an impulse response function (IRF), using the Timmer &amp; Koenig (1995) method.\n\n    The light curve is generated in Fourier space, with random phases and amplitudes\n    shaped by the PSD. It is then inverse-Fourier transformed to the time domain,\n    rescaled to the desired mean and standard deviation, and optionally passed through\n    Poisson or Gaussian noise models.\n\n    Supports both regularly and irregularly sampled time grids:\n    - For regular grids: oversamples the light curve before trimming to the desired grid.\n    - For irregular grids: simulates on a fine grid and selects nearest times (no interpolation).\n\n    Optional lag injection is applied via convolution with an impulse response kernel.\n\n    Parameters\n    ----------\n    time_grid : ndarray\n        Target time grid for the simulated light curve (must be sorted).\n\n    psd_type : str\n        Type of PSD to simulate. Options are:\n\n        - 'powerlaw'\n        - 'broken_powerlaw'\n\n    psd_params : dict\n        Parameters for the PSD model. Depends on `psd_type`:\n\n         - 'powerlaw': {'slope', 'plnorm'}\n        - 'broken_powerlaw': {'slope1', 'f_break', 'slope2', 'plnorm'}\n\n    mean : float\n        Desired mean of the light curve after rescaling.\n\n    std : float\n        Desired standard deviation of the light curve after rescaling.\n\n    add_noise : str or None, optional\n        Type of noise to add. Options are:\n\n        - \"Poisson\": adds Poisson-distributed noise with optional background\n        - \"Gaussian\": adds Gaussian noise using `gaussian_frac_err`\n        - None: no noise is added\n\n    gaussian_frac_err : float or None, optional\n        Fractional error to use when `add_noise=\"Gaussian\"`.\n\n    bkg_rate : float, optional\n        Background count rate (used in Poisson noise simulation). Default is 0.\n\n    oversample : int, optional\n        Oversampling factor for regularly spaced grids. Default is 10.\n\n    fine_factor : int, optional\n        Factor controlling the resolution of simulation for irregular grids. Default is 100.\n\n    inject_lag : bool, optional\n        Whether to apply a lag by convolving with an impulse response function. Default is False.\n\n    response_type : str or None, optional\n        Type of IRF for lag injection. Options are:\n\n        - 'delta', 'normal', 'lognormal', 'manual', or None\n\n    response_params : dict or None, optional\n        Parameters for the IRF. Depends on `response_type`:\n\n        - 'delta': {'lag'}\n        - 'normal': {'mean', 'sigma', 'duration' (optional)}\n        - 'lognormal': {'median', 'sigma', 'duration' (optional)}\n        - 'manual': {'response': array_like}\n\n    Attributes\n    ----------\n    rates : ndarray\n        Simulated light curve values after noise is added.\n\n    errors : ndarray\n        Estimated uncertainties for each time point.\n\n    simlc : LightCurve\n        The simulated light curve object (with noise).\n\n    simlc_lagged : LightCurve or None\n        Lagged version of the light curve, if `inject_lag=True`. Otherwise, None.\n    \"\"\"\n\n    def __init__(self,\n                 time_grid,\n                 psd_type,\n                 psd_params,\n                 mean,\n                 std,\n                 add_noise=None,\n                 gaussian_frac_err=None,\n                 bkg_rate=0.0,\n                 oversample=10,\n                 fine_factor=100,\n                 inject_lag=False,\n                 response_type=None,\n                 response_params=None):\n\n        self.time_grid = np.asarray(time_grid)\n        self.psd_type = psd_type\n        self.psd_params = psd_params\n        self.mean = mean\n        self.std = std\n        self.oversample = oversample\n        self.fine_factor = fine_factor\n        self.bkg_rate = bkg_rate\n        self.inject_lag = inject_lag\n        self.response_type = response_type\n        self.response_params = response_params\n\n        result = self.generate(self.time_grid)\n        if isinstance(result, tuple):\n            rates, rates_lagged = result\n        else:\n            rates = result\n            rates_lagged = None\n\n        errors = np.zeros(len(rates))\n        if add_noise.lower() == \"poisson\":\n            rates, errors = self.add_poisson_noise(rates, self.time_grid, bkg_rate=self.bkg_rate)\n\n            if rates_lagged is not None:\n                rates_lagged, _ = self.add_poisson_noise(rates_lagged, self.time_grid,\n                                                        bkg_rate=self.bkg_rate)\n\n        elif add_noise.lower() == \"gaussian\":\n            rates, errors = self.add_gaussian_noise(rates, frac_err=gaussian_frac_err)\n            if rates_lagged is not None:\n                rates_lagged, _ = self.add_gaussian_noise(rates_lagged, frac_err=gaussian_frac_err)\n\n        self.rates = rates\n        self.errors = errors\n        self.simlc = LightCurve(times=self.time_grid, rates=rates, errors=errors)\n        self.simlc_lagged = (\n            LightCurve(times=self.time_grid, rates=rates_lagged, errors=errors)\n            if rates_lagged is not None else None\n        )\n\n    def generate(self, time_grid):\n        \"\"\"\n        Generate the clean (noise-free) light curve.\n\n        Handles regular vs. irregular time grids and applies normalization.\n        If lag injection is enabled, convolves with a response function.\n\n        Parameters\n        ----------\n        time_grid : array-like\n            Desired output time grid.\n\n        Returns\n        -------\n        rates : ndarray\n            Simulated light curve values.\n\n        rates_lagged : ndarray or None\n            Lagged version of the light curve if `inject_lag` is True.\n        \"\"\"\n\n        time_grid = np.array(time_grid)\n        n_target = len(time_grid)\n        dt_array = np.diff(time_grid)\n        is_regular = np.allclose(dt_array, dt_array[0], rtol=1e-5)\n\n        if is_regular:\n            n_sim = int(self.oversample * n_target)\n            t_sim = np.linspace(time_grid[0], time_grid[-1], n_sim)\n            lc_sim = self._simulate_on_grid(t_sim)\n\n            start = (n_sim - n_target) // 2\n            end = start + n_target\n            lc = lc_sim[start:end]\n\n            lc -= np.mean(lc)\n            lc /= np.std(lc)\n            lc = lc * self.std + self.mean\n\n            if self.inject_lag:\n                kernel = self._build_impulse_response(time_grid)\n                convolved_full = fftconvolve(lc_sim, kernel, mode=\"full\")\n                convolved = convolved_full[start:end]\n\n                convolved -= np.mean(convolved)\n                convolved /= np.std(convolved)\n                convolved = convolved * self.std + self.mean\n                return lc, convolved\n            else:\n                return lc\n\n        else:\n            n_fine = int(self.fine_factor * len(time_grid))\n            t_fine = np.linspace(time_grid.min(), time_grid.max(), n_fine)\n            lc_fine = self._simulate_on_grid(t_fine)\n\n            lc_fine -= np.mean(lc_fine)\n            lc_fine /= np.std(lc_fine)\n            lc_fine = lc_fine * self.std + self.mean\n\n            if self.inject_lag:\n                kernel = self._build_impulse_response(t_fine)\n                lc_fine_lagged_full = fftconvolve(lc_fine, kernel, mode=\"full\")\n                lc_fine_lagged = lc_fine_lagged_full[:len(t_fine)]\n            else:\n                lc_fine_lagged = None\n\n            indices = np.searchsorted(t_fine, time_grid, side=\"left\")\n            indices = np.clip(indices, 0, n_fine - 1)\n            for i, ti in enumerate(time_grid):\n                if indices[i] &gt; 0 and abs(t_fine[indices[i] - 1] - ti) &lt; abs(t_fine[indices[i]] - ti):\n                    indices[i] -= 1\n\n            lc = lc_fine[indices]\n            if lc_fine_lagged is not None:\n                lc_lagged = lc_fine_lagged[indices]\n                return lc, lc_lagged\n            else:\n                return lc\n\n    def add_poisson_noise(self, lc, time_grid, bkg_rate=0.0, exposure_times=None, min_error_floor=1e-10):\n        \"\"\"\n        Add Poisson noise to a simulated light curve.\n\n        Parameters\n        ----------\n        lc : ndarray\n            Clean light curve values.\n\n        time_grid : ndarray\n            Time values.\n\n        bkg_rate : float, optional\n            Background count rate.\n\n        exposure_times : ndarray or None, optional\n            Exposure duration for each point. If None, use time spacing\n            to approximate integration time per bin.\n\n        min_error_floor : float, optional\n            Minimum uncertainty to avoid zeros.\n\n        Returns\n        -------\n        noisy_lc : ndarray\n            Noisy light curve.\n\n        noise_estimate : ndarray\n            Estimated error bars.\n        \"\"\"\n\n        lc = np.asarray(lc)\n        time_grid = np.asarray(time_grid)\n\n        if len(time_grid) &lt; 2:\n            raise ValueError(\"time_grid must have at least two points.\")\n\n        if exposure_times is not None:\n            dt = np.asarray(exposure_times)\n        else:\n            dt_array = np.diff(time_grid)\n            if np.allclose(dt_array, dt_array[0], rtol=1e-5):\n                dt = dt_array[0]\n            else:\n                dt = np.zeros_like(time_grid)\n                dt[1:-1] = (time_grid[2:] - time_grid[:-2]) / 2\n                dt[0] = time_grid[1] - time_grid[0]\n                dt[-1] = time_grid[-1] - time_grid[-2]\n\n        counts = lc * dt\n        counts = np.clip(counts, 0, None)\n\n        noisy_counts = np.random.poisson(counts)\n\n        if bkg_rate &gt; 0:\n            bkg_counts1 = np.random.poisson(bkg_rate * dt)\n            bkg_counts2 = np.random.poisson(bkg_rate * dt)\n            noisy_counts += bkg_counts1\n            noisy_counts -= bkg_counts2\n\n        noisy_lc = noisy_counts / dt\n\n        with np.errstate(divide='ignore', invalid='ignore'):\n            noise_estimate = np.sqrt(np.clip(noisy_counts, 0, None)) / dt\n            noise_estimate = np.where(noise_estimate == 0, min_error_floor, noise_estimate)\n\n        return noisy_lc, noise_estimate\n\n    def add_gaussian_noise(self, lc, frac_err=0.05, min_error_floor=1e-10):\n        \"\"\"\n        Add Gaussian noise to a simulated light curve in flux units.\n\n        Parameters\n        ----------\n        lc : ndarray\n            Simulated light curve values (flux units).\n\n        frac_err : float\n            Fractional error (e.g., 0.05 = 5%).\n\n        min_error_floor : float\n            Minimum allowed error value to prevent zeros.\n\n        Returns\n        -------\n        noisy_lc : ndarray\n            Light curve with added Gaussian noise.\n\n        errors : ndarray\n            Standard deviation of the Gaussian noise at each point.\n        \"\"\"\n\n        lc = np.asarray(lc)\n        errors = np.clip(np.abs(lc) * frac_err, min_error_floor, None)\n        noisy_lc = lc + np.random.normal(0, errors)\n        return noisy_lc, errors\n\n\n    def add_regular_gaps(self, lc, time_grid, gap_period, gap_duration):\n        \"\"\"\n        Simulate regular gaps in the light curve.\n\n        Parameters\n        ----------\n        lc : ndarray\n            Input light curve values.\n\n        time_grid : ndarray\n            Time values.\n\n        gap_period : float\n            Period between gaps.\n\n        gap_duration : float\n            Duration of each gap.\n\n        Returns\n        -------\n        gapped_lc : ndarray\n            Light curve with NaNs inserted for gaps.\n        \"\"\"\n\n        lc = np.asarray(lc)\n        time_grid = np.asarray(time_grid)\n        gapped_lc = lc.copy()\n\n        time_since_start = (time_grid - time_grid[0]) % gap_period\n        in_gap = time_since_start &lt; gap_duration\n        gapped_lc[in_gap] = np.nan\n\n        return gapped_lc\n\n    def create_psd(self, freq):\n        \"\"\"\n        Construct the PSD array based on the selected type and parameters.\n\n        Parameters\n        ----------\n        freq : ndarray\n            Frequency array.\n\n        Returns\n        -------\n        psd : ndarray\n            Power spectral density values.\n        \"\"\"\n\n        freq = np.abs(freq)\n        psd = np.zeros_like(freq)\n        nonzero_mask = freq &gt; 0  # avoid division by 0\n        plnorm = self.psd_params.get(\"plnorm\", 1.0)\n\n        if self.psd_type == \"powerlaw\":\n            slope = self.psd_params.get(\"slope\")\n            psd[nonzero_mask] = plnorm * (2 * np.pi * freq[nonzero_mask]) ** (-slope / 2)\n\n        elif self.psd_type == \"broken_powerlaw\":\n            slope1 = self.psd_params.get(\"slope1\")\n            f_break = self.psd_params.get(\"f_break\")\n            slope2 = self.psd_params.get(\"slope2\")\n            psd[nonzero_mask] = np.where(\n                freq[nonzero_mask] &lt;= f_break,\n                plnorm * (2 * np.pi * freq[nonzero_mask]) ** (-slope1 / 2),\n                plnorm * ((2 * np.pi * f_break) ** ((slope2 - slope1) / 2)) *\n                (2 * np.pi * freq[nonzero_mask]) ** (-slope2 / 2)\n            )\n        else:\n            raise ValueError(f\"Unsupported PSD type: {self.psd_type}\")\n\n        # set freq=0 psd to 0 to avoid infinite psd\n        # the value of this will be adjusted by the mean during rescaling\n        psd[freq==0] = 0\n        return psd\n\n    def plot(self):\n        \"\"\"\n        Plot the simulated light curve/s. Shows both the original and lagged data (if available).\n        \"\"\"\n        plt.figure(figsize=(8, 4.5))\n\n        # Main simlc\n        if hasattr(self.simlc, \"errors\") and np.any(self.simlc.errors &gt; 0):\n            plt.errorbar(self.simlc.times, self.simlc.rates, yerr=self.simlc.errors,\n                        fmt='o', label='Simulated', lw=1.5, capsize=1)\n        else:\n            plt.plot(self.simlc.times, self.simlc.rates, label='Simulated', lw=1.5)\n\n        # Lagged simlc\n        if self.simlc_lagged is not None:\n            if hasattr(self.simlc_lagged, \"errors\") and np.any(self.simlc_lagged.errors &gt; 0):\n                plt.errorbar(self.simlc_lagged.times, self.simlc_lagged.rates,\n                            yerr=self.simlc_lagged.errors,\n                            fmt='o', label='Lagged', lw=1.5, capsize=1, alpha=0.8)\n            else:\n                plt.plot(self.simlc_lagged.times, self.simlc_lagged.rates,\n                        label='Lagged', lw=1.5, alpha=0.8)\n\n        plt.xlabel(\"Time\")\n        plt.ylabel(\"Flux\")\n        plt.grid(True)\n        plt.legend()\n        plt.show()\n\n    def _simulate_on_grid(self, t_sim):\n        \"\"\"\n        Generate a Fourier-based light curve realization on a time grid.\n        \"\"\"\n        n_sim = len(t_sim)\n        dt = t_sim[1] - t_sim[0]\n        freqs = np.fft.fftfreq(n_sim, d=dt)\n\n        psd = self.create_psd(freqs)\n        phases = np.random.uniform(0, 2*np.pi, n_sim)\n        amplitudes = np.sqrt(psd)\n\n        ft = amplitudes * np.exp(1j * phases)\n\n        # check for hermitian symmetry to ensure real-valued IFFT\n        if n_sim % 2 == 0:\n            ft[int(n_sim / 2) + 1:] = np.conj(ft[1:int(n_sim / 2)][::-1])\n        else:\n            ft[int(n_sim / 2) + 1:] = np.conj(ft[1:int(n_sim / 2) + 1][::-1])\n\n        lc_sim = np.fft.ifft(ft).real\n        return lc_sim\n\n    def _build_impulse_response(self, times):\n        \"\"\"\n        Generate an impulse response function for lag injection.\n        Supported types: 'delta', 'normal', 'lognormal', 'manual'.\n        \"\"\"\n        dt = times[1] - times[0]\n\n        if self.response_type == \"delta\":\n            lag = self.response_params[\"lag\"]\n            lag_n = int(round(lag / dt))\n            response = unit_impulse(len(times), lag_n)\n            return response\n\n        elif self.response_type == \"normal\":\n            mu = self.response_params[\"mean\"]\n            sigma = self.response_params[\"sigma\"]\n            duration = self.response_params.get(\"duration\", 5 * sigma)\n            t = np.arange(0, duration, dt)\n            kernel = norm.pdf(t, loc=mu, scale=sigma)\n            return kernel / np.sum(kernel)\n\n        elif self.response_type == \"lognormal\":\n            median = self.response_params[\"median\"]\n            sigma = self.response_params[\"sigma\"]\n            duration = self.response_params.get(\"duration\", 5 * median)\n            t = np.arange(dt, duration, dt)  # starts at dt to avoid log(0)\n\n            # Convert median + sigma to shape and scale for lognorm\n            # lognorm.pdf(x, s, loc=0, scale=median)\n            s = sigma\n            scale = median\n            kernel = lognorm.pdf(t, s=s, scale=scale)\n            return kernel / np.sum(kernel)\n\n        elif self.response_type == \"manual\":\n            return np.asarray(self.response_params[\"response\"])\n\n        else:\n            raise ValueError(f\"Unsupported response_type: {self.response_type}\")\n</code></pre>"},{"location":"reference/data_simulator/#stela_toolkit.data_simulator.SimulateLightCurve.add_gaussian_noise","title":"<code>add_gaussian_noise(lc, frac_err=0.05, min_error_floor=1e-10)</code>","text":"<p>Add Gaussian noise to a simulated light curve in flux units.</p> <p>Parameters:</p> Name Type Description Default <code>lc</code> <code>ndarray</code> <p>Simulated light curve values (flux units).</p> required <code>frac_err</code> <code>float</code> <p>Fractional error (e.g., 0.05 = 5%).</p> <code>0.05</code> <code>min_error_floor</code> <code>float</code> <p>Minimum allowed error value to prevent zeros.</p> <code>1e-10</code> <p>Returns:</p> Name Type Description <code>noisy_lc</code> <code>ndarray</code> <p>Light curve with added Gaussian noise.</p> <code>errors</code> <code>ndarray</code> <p>Standard deviation of the Gaussian noise at each point.</p> Source code in <code>stela_toolkit/data_simulator.py</code> <pre><code>def add_gaussian_noise(self, lc, frac_err=0.05, min_error_floor=1e-10):\n    \"\"\"\n    Add Gaussian noise to a simulated light curve in flux units.\n\n    Parameters\n    ----------\n    lc : ndarray\n        Simulated light curve values (flux units).\n\n    frac_err : float\n        Fractional error (e.g., 0.05 = 5%).\n\n    min_error_floor : float\n        Minimum allowed error value to prevent zeros.\n\n    Returns\n    -------\n    noisy_lc : ndarray\n        Light curve with added Gaussian noise.\n\n    errors : ndarray\n        Standard deviation of the Gaussian noise at each point.\n    \"\"\"\n\n    lc = np.asarray(lc)\n    errors = np.clip(np.abs(lc) * frac_err, min_error_floor, None)\n    noisy_lc = lc + np.random.normal(0, errors)\n    return noisy_lc, errors\n</code></pre>"},{"location":"reference/data_simulator/#stela_toolkit.data_simulator.SimulateLightCurve.add_poisson_noise","title":"<code>add_poisson_noise(lc, time_grid, bkg_rate=0.0, exposure_times=None, min_error_floor=1e-10)</code>","text":"<p>Add Poisson noise to a simulated light curve.</p> <p>Parameters:</p> Name Type Description Default <code>lc</code> <code>ndarray</code> <p>Clean light curve values.</p> required <code>time_grid</code> <code>ndarray</code> <p>Time values.</p> required <code>bkg_rate</code> <code>float</code> <p>Background count rate.</p> <code>0.0</code> <code>exposure_times</code> <code>ndarray or None</code> <p>Exposure duration for each point. If None, use time spacing to approximate integration time per bin.</p> <code>None</code> <code>min_error_floor</code> <code>float</code> <p>Minimum uncertainty to avoid zeros.</p> <code>1e-10</code> <p>Returns:</p> Name Type Description <code>noisy_lc</code> <code>ndarray</code> <p>Noisy light curve.</p> <code>noise_estimate</code> <code>ndarray</code> <p>Estimated error bars.</p> Source code in <code>stela_toolkit/data_simulator.py</code> <pre><code>def add_poisson_noise(self, lc, time_grid, bkg_rate=0.0, exposure_times=None, min_error_floor=1e-10):\n    \"\"\"\n    Add Poisson noise to a simulated light curve.\n\n    Parameters\n    ----------\n    lc : ndarray\n        Clean light curve values.\n\n    time_grid : ndarray\n        Time values.\n\n    bkg_rate : float, optional\n        Background count rate.\n\n    exposure_times : ndarray or None, optional\n        Exposure duration for each point. If None, use time spacing\n        to approximate integration time per bin.\n\n    min_error_floor : float, optional\n        Minimum uncertainty to avoid zeros.\n\n    Returns\n    -------\n    noisy_lc : ndarray\n        Noisy light curve.\n\n    noise_estimate : ndarray\n        Estimated error bars.\n    \"\"\"\n\n    lc = np.asarray(lc)\n    time_grid = np.asarray(time_grid)\n\n    if len(time_grid) &lt; 2:\n        raise ValueError(\"time_grid must have at least two points.\")\n\n    if exposure_times is not None:\n        dt = np.asarray(exposure_times)\n    else:\n        dt_array = np.diff(time_grid)\n        if np.allclose(dt_array, dt_array[0], rtol=1e-5):\n            dt = dt_array[0]\n        else:\n            dt = np.zeros_like(time_grid)\n            dt[1:-1] = (time_grid[2:] - time_grid[:-2]) / 2\n            dt[0] = time_grid[1] - time_grid[0]\n            dt[-1] = time_grid[-1] - time_grid[-2]\n\n    counts = lc * dt\n    counts = np.clip(counts, 0, None)\n\n    noisy_counts = np.random.poisson(counts)\n\n    if bkg_rate &gt; 0:\n        bkg_counts1 = np.random.poisson(bkg_rate * dt)\n        bkg_counts2 = np.random.poisson(bkg_rate * dt)\n        noisy_counts += bkg_counts1\n        noisy_counts -= bkg_counts2\n\n    noisy_lc = noisy_counts / dt\n\n    with np.errstate(divide='ignore', invalid='ignore'):\n        noise_estimate = np.sqrt(np.clip(noisy_counts, 0, None)) / dt\n        noise_estimate = np.where(noise_estimate == 0, min_error_floor, noise_estimate)\n\n    return noisy_lc, noise_estimate\n</code></pre>"},{"location":"reference/data_simulator/#stela_toolkit.data_simulator.SimulateLightCurve.add_regular_gaps","title":"<code>add_regular_gaps(lc, time_grid, gap_period, gap_duration)</code>","text":"<p>Simulate regular gaps in the light curve.</p> <p>Parameters:</p> Name Type Description Default <code>lc</code> <code>ndarray</code> <p>Input light curve values.</p> required <code>time_grid</code> <code>ndarray</code> <p>Time values.</p> required <code>gap_period</code> <code>float</code> <p>Period between gaps.</p> required <code>gap_duration</code> <code>float</code> <p>Duration of each gap.</p> required <p>Returns:</p> Name Type Description <code>gapped_lc</code> <code>ndarray</code> <p>Light curve with NaNs inserted for gaps.</p> Source code in <code>stela_toolkit/data_simulator.py</code> <pre><code>def add_regular_gaps(self, lc, time_grid, gap_period, gap_duration):\n    \"\"\"\n    Simulate regular gaps in the light curve.\n\n    Parameters\n    ----------\n    lc : ndarray\n        Input light curve values.\n\n    time_grid : ndarray\n        Time values.\n\n    gap_period : float\n        Period between gaps.\n\n    gap_duration : float\n        Duration of each gap.\n\n    Returns\n    -------\n    gapped_lc : ndarray\n        Light curve with NaNs inserted for gaps.\n    \"\"\"\n\n    lc = np.asarray(lc)\n    time_grid = np.asarray(time_grid)\n    gapped_lc = lc.copy()\n\n    time_since_start = (time_grid - time_grid[0]) % gap_period\n    in_gap = time_since_start &lt; gap_duration\n    gapped_lc[in_gap] = np.nan\n\n    return gapped_lc\n</code></pre>"},{"location":"reference/data_simulator/#stela_toolkit.data_simulator.SimulateLightCurve.create_psd","title":"<code>create_psd(freq)</code>","text":"<p>Construct the PSD array based on the selected type and parameters.</p> <p>Parameters:</p> Name Type Description Default <code>freq</code> <code>ndarray</code> <p>Frequency array.</p> required <p>Returns:</p> Name Type Description <code>psd</code> <code>ndarray</code> <p>Power spectral density values.</p> Source code in <code>stela_toolkit/data_simulator.py</code> <pre><code>def create_psd(self, freq):\n    \"\"\"\n    Construct the PSD array based on the selected type and parameters.\n\n    Parameters\n    ----------\n    freq : ndarray\n        Frequency array.\n\n    Returns\n    -------\n    psd : ndarray\n        Power spectral density values.\n    \"\"\"\n\n    freq = np.abs(freq)\n    psd = np.zeros_like(freq)\n    nonzero_mask = freq &gt; 0  # avoid division by 0\n    plnorm = self.psd_params.get(\"plnorm\", 1.0)\n\n    if self.psd_type == \"powerlaw\":\n        slope = self.psd_params.get(\"slope\")\n        psd[nonzero_mask] = plnorm * (2 * np.pi * freq[nonzero_mask]) ** (-slope / 2)\n\n    elif self.psd_type == \"broken_powerlaw\":\n        slope1 = self.psd_params.get(\"slope1\")\n        f_break = self.psd_params.get(\"f_break\")\n        slope2 = self.psd_params.get(\"slope2\")\n        psd[nonzero_mask] = np.where(\n            freq[nonzero_mask] &lt;= f_break,\n            plnorm * (2 * np.pi * freq[nonzero_mask]) ** (-slope1 / 2),\n            plnorm * ((2 * np.pi * f_break) ** ((slope2 - slope1) / 2)) *\n            (2 * np.pi * freq[nonzero_mask]) ** (-slope2 / 2)\n        )\n    else:\n        raise ValueError(f\"Unsupported PSD type: {self.psd_type}\")\n\n    # set freq=0 psd to 0 to avoid infinite psd\n    # the value of this will be adjusted by the mean during rescaling\n    psd[freq==0] = 0\n    return psd\n</code></pre>"},{"location":"reference/data_simulator/#stela_toolkit.data_simulator.SimulateLightCurve.generate","title":"<code>generate(time_grid)</code>","text":"<p>Generate the clean (noise-free) light curve.</p> <p>Handles regular vs. irregular time grids and applies normalization. If lag injection is enabled, convolves with a response function.</p> <p>Parameters:</p> Name Type Description Default <code>time_grid</code> <code>array - like</code> <p>Desired output time grid.</p> required <p>Returns:</p> Name Type Description <code>rates</code> <code>ndarray</code> <p>Simulated light curve values.</p> <code>rates_lagged</code> <code>ndarray or None</code> <p>Lagged version of the light curve if <code>inject_lag</code> is True.</p> Source code in <code>stela_toolkit/data_simulator.py</code> <pre><code>def generate(self, time_grid):\n    \"\"\"\n    Generate the clean (noise-free) light curve.\n\n    Handles regular vs. irregular time grids and applies normalization.\n    If lag injection is enabled, convolves with a response function.\n\n    Parameters\n    ----------\n    time_grid : array-like\n        Desired output time grid.\n\n    Returns\n    -------\n    rates : ndarray\n        Simulated light curve values.\n\n    rates_lagged : ndarray or None\n        Lagged version of the light curve if `inject_lag` is True.\n    \"\"\"\n\n    time_grid = np.array(time_grid)\n    n_target = len(time_grid)\n    dt_array = np.diff(time_grid)\n    is_regular = np.allclose(dt_array, dt_array[0], rtol=1e-5)\n\n    if is_regular:\n        n_sim = int(self.oversample * n_target)\n        t_sim = np.linspace(time_grid[0], time_grid[-1], n_sim)\n        lc_sim = self._simulate_on_grid(t_sim)\n\n        start = (n_sim - n_target) // 2\n        end = start + n_target\n        lc = lc_sim[start:end]\n\n        lc -= np.mean(lc)\n        lc /= np.std(lc)\n        lc = lc * self.std + self.mean\n\n        if self.inject_lag:\n            kernel = self._build_impulse_response(time_grid)\n            convolved_full = fftconvolve(lc_sim, kernel, mode=\"full\")\n            convolved = convolved_full[start:end]\n\n            convolved -= np.mean(convolved)\n            convolved /= np.std(convolved)\n            convolved = convolved * self.std + self.mean\n            return lc, convolved\n        else:\n            return lc\n\n    else:\n        n_fine = int(self.fine_factor * len(time_grid))\n        t_fine = np.linspace(time_grid.min(), time_grid.max(), n_fine)\n        lc_fine = self._simulate_on_grid(t_fine)\n\n        lc_fine -= np.mean(lc_fine)\n        lc_fine /= np.std(lc_fine)\n        lc_fine = lc_fine * self.std + self.mean\n\n        if self.inject_lag:\n            kernel = self._build_impulse_response(t_fine)\n            lc_fine_lagged_full = fftconvolve(lc_fine, kernel, mode=\"full\")\n            lc_fine_lagged = lc_fine_lagged_full[:len(t_fine)]\n        else:\n            lc_fine_lagged = None\n\n        indices = np.searchsorted(t_fine, time_grid, side=\"left\")\n        indices = np.clip(indices, 0, n_fine - 1)\n        for i, ti in enumerate(time_grid):\n            if indices[i] &gt; 0 and abs(t_fine[indices[i] - 1] - ti) &lt; abs(t_fine[indices[i]] - ti):\n                indices[i] -= 1\n\n        lc = lc_fine[indices]\n        if lc_fine_lagged is not None:\n            lc_lagged = lc_fine_lagged[indices]\n            return lc, lc_lagged\n        else:\n            return lc\n</code></pre>"},{"location":"reference/data_simulator/#stela_toolkit.data_simulator.SimulateLightCurve.plot","title":"<code>plot()</code>","text":"<p>Plot the simulated light curve/s. Shows both the original and lagged data (if available).</p> Source code in <code>stela_toolkit/data_simulator.py</code> <pre><code>def plot(self):\n    \"\"\"\n    Plot the simulated light curve/s. Shows both the original and lagged data (if available).\n    \"\"\"\n    plt.figure(figsize=(8, 4.5))\n\n    # Main simlc\n    if hasattr(self.simlc, \"errors\") and np.any(self.simlc.errors &gt; 0):\n        plt.errorbar(self.simlc.times, self.simlc.rates, yerr=self.simlc.errors,\n                    fmt='o', label='Simulated', lw=1.5, capsize=1)\n    else:\n        plt.plot(self.simlc.times, self.simlc.rates, label='Simulated', lw=1.5)\n\n    # Lagged simlc\n    if self.simlc_lagged is not None:\n        if hasattr(self.simlc_lagged, \"errors\") and np.any(self.simlc_lagged.errors &gt; 0):\n            plt.errorbar(self.simlc_lagged.times, self.simlc_lagged.rates,\n                        yerr=self.simlc_lagged.errors,\n                        fmt='o', label='Lagged', lw=1.5, capsize=1, alpha=0.8)\n        else:\n            plt.plot(self.simlc_lagged.times, self.simlc_lagged.rates,\n                    label='Lagged', lw=1.5, alpha=0.8)\n\n    plt.xlabel(\"Time\")\n    plt.ylabel(\"Flux\")\n    plt.grid(True)\n    plt.legend()\n    plt.show()\n</code></pre>"},{"location":"reference/frequency_binning/","title":"frequency_binning","text":""},{"location":"reference/frequency_binning/#stela_toolkit.frequency_binning.FrequencyBinning","title":"<code>FrequencyBinning</code>","text":"<p>A utility class for binning data over frequency space.</p> <p>Provides methods for defining bins (linear or logarithmic, or user-defined), binning frequency data and corresponding values, and calculating statistics for binned data.</p> Source code in <code>stela_toolkit/frequency_binning.py</code> <pre><code>class FrequencyBinning:\n    \"\"\"\n    A utility class for binning data over frequency space.\n\n    Provides methods for defining bins (linear or logarithmic, or user-defined), binning frequency\n    data and corresponding values, and calculating statistics for binned data.\n    \"\"\"\n\n    @staticmethod\n    def define_bins(fmin, fmax, num_bins=None, bin_type=\"log\", bin_edges=[]):\n        \"\"\"\n        Defines bin edges for a frequency range using the specified binning type.\n\n        If `bin_edges` are provided, they are used directly. Otherwise, bin edges are\n        computed between `fmin` and `fmax` using either logarithmic or linear spacing.\n\n        Parameters\n        ----------\n        fmin : float\n            Minimum frequency value.\n\n        fmax : float\n            Maximum frequency value.\n\n        num_bins : int, optional\n            Number of bins to create (used only if `bin_edges` is not provided).\n\n        bin_type : str, optional\n            Type of binning to use: \"log\" for logarithmic or \"linear\" for linear spacing.\n\n        bin_edges : array-like, optional\n            Custom array of bin edges. If provided, overrides `fmin`, `fmax`, and `num_bins`.\n\n        Returns\n        -------\n        bin_edges : array-like\n            Array of frequency bin edges based on the specified settings.\n        \"\"\"\n        if len(bin_edges) &gt; 0:\n            # Use custom bins\n            bin_edges = np.array(bin_edges)\n        else:\n\n            if bin_type == \"log\":\n                # Define logarithmic bins\n                bin_edges = np.logspace(np.log10(fmin), np.log10(fmax), num_bins + 1)\n\n            elif bin_type == \"linear\":\n                # Define linear bins\n                bin_edges = np.linspace(fmin, fmax, num_bins + 1)\n\n            else:\n                raise ValueError(\n                    f\"Unsupported bin_type '{bin_type}'. Choose 'log', 'linear', or provide custom bins.\")\n\n        return bin_edges\n\n    @staticmethod\n    def bin_data(freqs, values, bin_edges):\n        \"\"\"\n        Bins frequencies and corresponding values into the specified bin edges.\n\n        For each bin, computes the mean frequency, bin half-width (for error bars),\n        mean value, and standard deviation of values within the bin.\n\n        Parameters\n        ----------\n        freqs : array-like\n            Array of frequency values to be binned.\n\n        values : array-like\n            Array of values corresponding to each frequency.\n\n        bin_edges : array-like\n            Array of bin edges that define the frequency bins.\n\n        Returns\n        -------\n        binned_freqs : array-like\n            Mean frequency for each bin.\n\n        binned_freq_widths : array-like\n            Half-width of each frequency bin (for plotting error bars).\n\n        binned_values : array-like\n            Mean value of the data within each bin.\n\n        binned_value_errors : array-like\n            Standard deviation of the values in each bin.\n        \"\"\"\n        binned_freqs = []\n        binned_freq_widths = []\n        binned_values = []\n        binned_value_errors = []\n\n        for i in range(len(bin_edges) - 1):\n            mask = (freqs &gt;= bin_edges[i]) &amp; (freqs &lt; bin_edges[i + 1])\n            num_freqs = np.sum(mask)\n\n            if mask.any():\n                lower_bound = bin_edges[i]\n                upper_bound = bin_edges[i + 1]\n                bin_cent = (upper_bound + lower_bound) / 2\n\n                binned_freqs.append(bin_cent)\n                binned_freq_widths.append(bin_cent - lower_bound)\n                binned_values.append(\n                    np.mean(values[mask])\n                )\n                binned_value_errors.append(\n                    np.std(values[mask]) / np.sqrt(num_freqs)\n                )\n\n        return (\n            np.array(binned_freqs),\n            np.array(binned_freq_widths),\n            np.array(binned_values),\n            np.array(binned_value_errors),\n        )\n\n    @staticmethod\n    def count_frequencies_in_bins(spectrum, fmin=None, fmax=None, num_bins=None, bin_type=None, bin_edges=[]):\n        \"\"\"\n        Counts the number of frequencies in each bin for the power spectrum.\n\n        If `bin_edges` are provided, they are used directly. Otherwise, bins are\n        defined using `fmin`, `fmax`, `num_bins`, and `bin_type`.\n\n        Parameters\n        ----------\n        spectrum : object\n            Object containing attributes like `times`, `fmin`, and `fmax`.\n\n        fmin : float, optional\n            Minimum frequency. If not provided, defaults to `spectrum.fmin`.\n\n        fmax : float, optional\n            Maximum frequency. If not provided, defaults to `spectrum.fmax`.\n\n        num_bins : int, optional\n            Number of bins to create (used only if `bin_edges` is not provided).\n\n        bin_type : str, optional\n            Type of binning to use: \"log\" or \"linear\".\n\n        bin_edges : array-like, optional\n            Custom array of bin edges. If provided, overrides `fmin`, `fmax`, and `num_bins`.\n\n        Returns\n        -------\n        bin_counts : list of int\n            List containing the number of frequencies in each bin.\n        \"\"\"\n        # Use spectrum's attributes if not provided\n        fmin = spectrum.fmin if fmin is None else fmin\n        fmax = spectrum.fmax if fmax is None else fmax\n        num_bins = spectrum.num_bins if num_bins is None else num_bins\n        bin_type = spectrum.bin_type if bin_type is None else bin_type\n        bin_edges = spectrum.bin_edges if bin_edges is None else bin_edges\n\n        # Define time array from input class\n        if hasattr(spectrum, 'times'):  \n            times = spectrum.times\n        elif hasattr(spectrum, 'times1'):\n            times = spectrum.times1\n        else:\n            raise AttributeError('Input class object for frequency binning does not have a time array properly defined.')\n\n        length = len(times) \n        dt = np.diff(times)[0]\n        freqs = np.fft.rfftfreq(length, d=dt)\n        freq_mask = (freqs &gt;= fmin) &amp; (freqs &lt;= fmax)\n        freqs = freqs[freq_mask]\n\n        # if neither num_bins nor bin_edges have been provided, no binning\n        if not any([num_bins, bin_edges]):\n            return np.ones(len(freqs))\n\n        # Check if bin_edges or num_bins provided\n        if len(bin_edges) == 0 and fmin and fmax and num_bins:\n            bin_edges = FrequencyBinning.define_bins(fmin, fmax, num_bins=num_bins, \n                                                     bin_type=bin_type, bin_edges=bin_edges\n                                                    )\n        elif len(bin_edges) &gt; 0:\n            bin_edges = np.array(bin_edges)\n        else:\n            raise ValueError(\n                \"Frequency binning requires either 1) defined bin edges, 2) num_bins + fmin + fmax., \\\n                3) all defined as none to leave products unbinned.\"\n            )\n\n        # Count frequencies in bins\n        bin_counts = np.histogram(freqs, bins=bin_edges)[0]\n        return bin_counts\n</code></pre>"},{"location":"reference/frequency_binning/#stela_toolkit.frequency_binning.FrequencyBinning.bin_data","title":"<code>bin_data(freqs, values, bin_edges)</code>  <code>staticmethod</code>","text":"<p>Bins frequencies and corresponding values into the specified bin edges.</p> <p>For each bin, computes the mean frequency, bin half-width (for error bars), mean value, and standard deviation of values within the bin.</p> <p>Parameters:</p> Name Type Description Default <code>freqs</code> <code>array - like</code> <p>Array of frequency values to be binned.</p> required <code>values</code> <code>array - like</code> <p>Array of values corresponding to each frequency.</p> required <code>bin_edges</code> <code>array - like</code> <p>Array of bin edges that define the frequency bins.</p> required <p>Returns:</p> Name Type Description <code>binned_freqs</code> <code>array - like</code> <p>Mean frequency for each bin.</p> <code>binned_freq_widths</code> <code>array - like</code> <p>Half-width of each frequency bin (for plotting error bars).</p> <code>binned_values</code> <code>array - like</code> <p>Mean value of the data within each bin.</p> <code>binned_value_errors</code> <code>array - like</code> <p>Standard deviation of the values in each bin.</p> Source code in <code>stela_toolkit/frequency_binning.py</code> <pre><code>@staticmethod\ndef bin_data(freqs, values, bin_edges):\n    \"\"\"\n    Bins frequencies and corresponding values into the specified bin edges.\n\n    For each bin, computes the mean frequency, bin half-width (for error bars),\n    mean value, and standard deviation of values within the bin.\n\n    Parameters\n    ----------\n    freqs : array-like\n        Array of frequency values to be binned.\n\n    values : array-like\n        Array of values corresponding to each frequency.\n\n    bin_edges : array-like\n        Array of bin edges that define the frequency bins.\n\n    Returns\n    -------\n    binned_freqs : array-like\n        Mean frequency for each bin.\n\n    binned_freq_widths : array-like\n        Half-width of each frequency bin (for plotting error bars).\n\n    binned_values : array-like\n        Mean value of the data within each bin.\n\n    binned_value_errors : array-like\n        Standard deviation of the values in each bin.\n    \"\"\"\n    binned_freqs = []\n    binned_freq_widths = []\n    binned_values = []\n    binned_value_errors = []\n\n    for i in range(len(bin_edges) - 1):\n        mask = (freqs &gt;= bin_edges[i]) &amp; (freqs &lt; bin_edges[i + 1])\n        num_freqs = np.sum(mask)\n\n        if mask.any():\n            lower_bound = bin_edges[i]\n            upper_bound = bin_edges[i + 1]\n            bin_cent = (upper_bound + lower_bound) / 2\n\n            binned_freqs.append(bin_cent)\n            binned_freq_widths.append(bin_cent - lower_bound)\n            binned_values.append(\n                np.mean(values[mask])\n            )\n            binned_value_errors.append(\n                np.std(values[mask]) / np.sqrt(num_freqs)\n            )\n\n    return (\n        np.array(binned_freqs),\n        np.array(binned_freq_widths),\n        np.array(binned_values),\n        np.array(binned_value_errors),\n    )\n</code></pre>"},{"location":"reference/frequency_binning/#stela_toolkit.frequency_binning.FrequencyBinning.count_frequencies_in_bins","title":"<code>count_frequencies_in_bins(spectrum, fmin=None, fmax=None, num_bins=None, bin_type=None, bin_edges=[])</code>  <code>staticmethod</code>","text":"<p>Counts the number of frequencies in each bin for the power spectrum.</p> <p>If <code>bin_edges</code> are provided, they are used directly. Otherwise, bins are defined using <code>fmin</code>, <code>fmax</code>, <code>num_bins</code>, and <code>bin_type</code>.</p> <p>Parameters:</p> Name Type Description Default <code>spectrum</code> <code>object</code> <p>Object containing attributes like <code>times</code>, <code>fmin</code>, and <code>fmax</code>.</p> required <code>fmin</code> <code>float</code> <p>Minimum frequency. If not provided, defaults to <code>spectrum.fmin</code>.</p> <code>None</code> <code>fmax</code> <code>float</code> <p>Maximum frequency. If not provided, defaults to <code>spectrum.fmax</code>.</p> <code>None</code> <code>num_bins</code> <code>int</code> <p>Number of bins to create (used only if <code>bin_edges</code> is not provided).</p> <code>None</code> <code>bin_type</code> <code>str</code> <p>Type of binning to use: \"log\" or \"linear\".</p> <code>None</code> <code>bin_edges</code> <code>array - like</code> <p>Custom array of bin edges. If provided, overrides <code>fmin</code>, <code>fmax</code>, and <code>num_bins</code>.</p> <code>[]</code> <p>Returns:</p> Name Type Description <code>bin_counts</code> <code>list of int</code> <p>List containing the number of frequencies in each bin.</p> Source code in <code>stela_toolkit/frequency_binning.py</code> <pre><code>@staticmethod\ndef count_frequencies_in_bins(spectrum, fmin=None, fmax=None, num_bins=None, bin_type=None, bin_edges=[]):\n    \"\"\"\n    Counts the number of frequencies in each bin for the power spectrum.\n\n    If `bin_edges` are provided, they are used directly. Otherwise, bins are\n    defined using `fmin`, `fmax`, `num_bins`, and `bin_type`.\n\n    Parameters\n    ----------\n    spectrum : object\n        Object containing attributes like `times`, `fmin`, and `fmax`.\n\n    fmin : float, optional\n        Minimum frequency. If not provided, defaults to `spectrum.fmin`.\n\n    fmax : float, optional\n        Maximum frequency. If not provided, defaults to `spectrum.fmax`.\n\n    num_bins : int, optional\n        Number of bins to create (used only if `bin_edges` is not provided).\n\n    bin_type : str, optional\n        Type of binning to use: \"log\" or \"linear\".\n\n    bin_edges : array-like, optional\n        Custom array of bin edges. If provided, overrides `fmin`, `fmax`, and `num_bins`.\n\n    Returns\n    -------\n    bin_counts : list of int\n        List containing the number of frequencies in each bin.\n    \"\"\"\n    # Use spectrum's attributes if not provided\n    fmin = spectrum.fmin if fmin is None else fmin\n    fmax = spectrum.fmax if fmax is None else fmax\n    num_bins = spectrum.num_bins if num_bins is None else num_bins\n    bin_type = spectrum.bin_type if bin_type is None else bin_type\n    bin_edges = spectrum.bin_edges if bin_edges is None else bin_edges\n\n    # Define time array from input class\n    if hasattr(spectrum, 'times'):  \n        times = spectrum.times\n    elif hasattr(spectrum, 'times1'):\n        times = spectrum.times1\n    else:\n        raise AttributeError('Input class object for frequency binning does not have a time array properly defined.')\n\n    length = len(times) \n    dt = np.diff(times)[0]\n    freqs = np.fft.rfftfreq(length, d=dt)\n    freq_mask = (freqs &gt;= fmin) &amp; (freqs &lt;= fmax)\n    freqs = freqs[freq_mask]\n\n    # if neither num_bins nor bin_edges have been provided, no binning\n    if not any([num_bins, bin_edges]):\n        return np.ones(len(freqs))\n\n    # Check if bin_edges or num_bins provided\n    if len(bin_edges) == 0 and fmin and fmax and num_bins:\n        bin_edges = FrequencyBinning.define_bins(fmin, fmax, num_bins=num_bins, \n                                                 bin_type=bin_type, bin_edges=bin_edges\n                                                )\n    elif len(bin_edges) &gt; 0:\n        bin_edges = np.array(bin_edges)\n    else:\n        raise ValueError(\n            \"Frequency binning requires either 1) defined bin edges, 2) num_bins + fmin + fmax., \\\n            3) all defined as none to leave products unbinned.\"\n        )\n\n    # Count frequencies in bins\n    bin_counts = np.histogram(freqs, bins=bin_edges)[0]\n    return bin_counts\n</code></pre>"},{"location":"reference/frequency_binning/#stela_toolkit.frequency_binning.FrequencyBinning.define_bins","title":"<code>define_bins(fmin, fmax, num_bins=None, bin_type='log', bin_edges=[])</code>  <code>staticmethod</code>","text":"<p>Defines bin edges for a frequency range using the specified binning type.</p> <p>If <code>bin_edges</code> are provided, they are used directly. Otherwise, bin edges are computed between <code>fmin</code> and <code>fmax</code> using either logarithmic or linear spacing.</p> <p>Parameters:</p> Name Type Description Default <code>fmin</code> <code>float</code> <p>Minimum frequency value.</p> required <code>fmax</code> <code>float</code> <p>Maximum frequency value.</p> required <code>num_bins</code> <code>int</code> <p>Number of bins to create (used only if <code>bin_edges</code> is not provided).</p> <code>None</code> <code>bin_type</code> <code>str</code> <p>Type of binning to use: \"log\" for logarithmic or \"linear\" for linear spacing.</p> <code>'log'</code> <code>bin_edges</code> <code>array - like</code> <p>Custom array of bin edges. If provided, overrides <code>fmin</code>, <code>fmax</code>, and <code>num_bins</code>.</p> <code>[]</code> <p>Returns:</p> Name Type Description <code>bin_edges</code> <code>array - like</code> <p>Array of frequency bin edges based on the specified settings.</p> Source code in <code>stela_toolkit/frequency_binning.py</code> <pre><code>@staticmethod\ndef define_bins(fmin, fmax, num_bins=None, bin_type=\"log\", bin_edges=[]):\n    \"\"\"\n    Defines bin edges for a frequency range using the specified binning type.\n\n    If `bin_edges` are provided, they are used directly. Otherwise, bin edges are\n    computed between `fmin` and `fmax` using either logarithmic or linear spacing.\n\n    Parameters\n    ----------\n    fmin : float\n        Minimum frequency value.\n\n    fmax : float\n        Maximum frequency value.\n\n    num_bins : int, optional\n        Number of bins to create (used only if `bin_edges` is not provided).\n\n    bin_type : str, optional\n        Type of binning to use: \"log\" for logarithmic or \"linear\" for linear spacing.\n\n    bin_edges : array-like, optional\n        Custom array of bin edges. If provided, overrides `fmin`, `fmax`, and `num_bins`.\n\n    Returns\n    -------\n    bin_edges : array-like\n        Array of frequency bin edges based on the specified settings.\n    \"\"\"\n    if len(bin_edges) &gt; 0:\n        # Use custom bins\n        bin_edges = np.array(bin_edges)\n    else:\n\n        if bin_type == \"log\":\n            # Define logarithmic bins\n            bin_edges = np.logspace(np.log10(fmin), np.log10(fmax), num_bins + 1)\n\n        elif bin_type == \"linear\":\n            # Define linear bins\n            bin_edges = np.linspace(fmin, fmax, num_bins + 1)\n\n        else:\n            raise ValueError(\n                f\"Unsupported bin_type '{bin_type}'. Choose 'log', 'linear', or provide custom bins.\")\n\n    return bin_edges\n</code></pre>"},{"location":"reference/gaussian_process/","title":"gaussian_process","text":""},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess","title":"<code>GaussianProcess</code>","text":"<p>Fit and sample from a Gaussian Process (GP) model for light curve data.</p> <p>This class allows you to model a light curve as a continuous, probabilistic function using a Gaussian Process. You provide a LightCurve object, and the model will fit a smooth function to the observed rates, incorporating measurement uncertainties and capturing the underlying variability with flexible kernel choices.</p> <p>By default, the model will try to make things easy for you:</p> <ul> <li>If the flux distribution is not normally distributed, we can apply a Box-Cox transformation to make it more Gaussian.</li> <li>The data is standardized (zero mean, unit variance) before training to improve numerical stability.</li> <li>A white noise term can be added to account for extra variance not captured by measurement errors.</li> <li>If you don\u2019t specify a kernel, the model will try several standard ones and pick the best using AIC.</li> </ul> <p>Once trained, the model allows you to generate samples from the posterior predictive distribution\u2014 these are realizations of what the light curve could look like, given the data and uncertainties. These samples are central to downstream STELA analyses like coherence, cross-spectrum, and lag measurements, which will automatically use the most recently generated GP samples if a model is passed in.</p> <p>If you haven\u2019t generated any samples yet, don\u2019t worry\u2014those modules will do it for you using default settings.</p> <p>Noise handling is flexible:</p> <ul> <li>If your light curve has error bars, they\u2019re passed directly into the GP as a fixed noise model.</li> <li>If not, you can still include a learned white noise term to capture unmodeled variability.</li> <li>You can control whether the model uses just your errors, or also learns extra noise.</li> </ul> <p>Model training uses exact inference with GPyTorch and is done via gradient descent. You can control the number of iterations, the optimizer learning rate, and whether to plot the training progress.</p> <p>Parameters:</p> Name Type Description Default <code>lightcurve</code> <code>LightCurve</code> <p>The input light curve to model.</p> required <code>kernel_form</code> <code>str or list</code> <p>Kernel type to use (e.g., 'Matern32', 'RBF', 'SpectralMixture, N'), or list of types for auto-selection. If 'auto', we try several and choose the best using AIC.</p> <code>'auto'</code> <code>white_noise</code> <code>bool</code> <p>Whether to include a white noise component in addition to measurement errors.</p> <code>True</code> <code>enforce_normality</code> <code>bool</code> <p>Whether to apply a Box-Cox transformation to make the flux distribution more Gaussian.</p> <code>False</code> <code>run_training</code> <code>bool</code> <p>Whether to train the GP model on initialization.</p> <code>True</code> <code>plot_training</code> <code>bool</code> <p>Whether to plot the training loss during optimization.</p> <code>False</code> <code>num_iter</code> <code>int</code> <p>Number of iterations for GP training.</p> <code>500</code> <code>learn_rate</code> <code>float</code> <p>Learning rate for the optimizer.</p> <code>0.1</code> <code>sample_time_grid</code> <code>array - like</code> <p>Time grid on which to draw posterior samples after training.</p> <code>[]</code> <code>num_samples</code> <code>int</code> <p>Number of GP samples to draw from the posterior.</p> <code>1000</code> <code>verbose</code> <code>bool</code> <p>Whether to print model selection, training progress, and sampling diagnostics.</p> <code>False</code> <p>Attributes:</p> Name Type Description <code>model</code> <code>ExactGP</code> <p>The trained GP model used for prediction and sampling.</p> <code>likelihood</code> <code>Likelihood</code> <p>The likelihood model used (e.g., Gaussian with or without fixed noise).</p> <code>train_times</code> <code>Tensor</code> <p>Time points used for training the GP.</p> <code>train_rates</code> <code>Tensor</code> <p>Rate values used for training.</p> <code>train_errors</code> <code>Tensor</code> <p>Measurement uncertainties (empty if not provided).</p> <code>samples</code> <code>ndarray</code> <p>Posterior samples drawn after training (used by downstream STELA modules).</p> <code>pred_times</code> <code>Tensor</code> <p>Time grid on which posterior samples were drawn.</p> <code>kernel_form</code> <code>str</code> <p>Name of the kernel used in the final trained model.</p> Source code in <code>stela_toolkit/gaussian_process.py</code> <pre><code>class GaussianProcess:\n    \"\"\"\n    Fit and sample from a Gaussian Process (GP) model for light curve data.\n\n    This class allows you to model a light curve as a continuous, probabilistic function using a Gaussian Process.\n    You provide a LightCurve object, and the model will fit a smooth function to the observed rates,\n    incorporating measurement uncertainties and capturing the underlying variability with flexible kernel choices.\n\n    By default, the model will try to make things easy for you:\n\n    - If the flux distribution is not normally distributed, we can apply a Box-Cox transformation to make it more Gaussian.\n    - The data is standardized (zero mean, unit variance) before training to improve numerical stability.\n    - A white noise term can be added to account for extra variance not captured by measurement errors.\n    - If you don\u2019t specify a kernel, the model will try several standard ones and pick the best using AIC.\n\n    Once trained, the model allows you to generate samples from the posterior predictive distribution\u2014\n    these are realizations of what the light curve *could* look like, given the data and uncertainties.\n    These samples are central to downstream STELA analyses like coherence, cross-spectrum, and lag measurements,\n    which will automatically use the most recently generated GP samples if a model is passed in.\n\n    If you haven\u2019t generated any samples yet, don\u2019t worry\u2014those modules will do it for you using default settings.\n\n    Noise handling is flexible:\n\n    - If your light curve has error bars, they\u2019re passed directly into the GP as a fixed noise model.\n    - If not, you can still include a learned white noise term to capture unmodeled variability.\n    - You can control whether the model uses just your errors, or also learns extra noise.\n\n    Model training uses exact inference with GPyTorch and is done via gradient descent.\n    You can control the number of iterations, the optimizer learning rate, and whether to plot the training progress.\n\n    Parameters\n    ----------\n    lightcurve : LightCurve\n        The input light curve to model.\n\n    kernel_form : str or list, optional\n        Kernel type to use (e.g., 'Matern32', 'RBF', 'SpectralMixture, N'), or list of types for auto-selection.\n        If 'auto', we try several and choose the best using AIC.\n\n    white_noise : bool, optional\n        Whether to include a white noise component in addition to measurement errors.\n\n    enforce_normality : bool, optional\n        Whether to apply a Box-Cox transformation to make the flux distribution more Gaussian.\n\n    run_training : bool, optional\n        Whether to train the GP model on initialization.\n\n    plot_training : bool, optional\n        Whether to plot the training loss during optimization.\n\n    num_iter : int, optional\n        Number of iterations for GP training.\n\n    learn_rate : float, optional\n        Learning rate for the optimizer.\n\n    sample_time_grid : array-like, optional\n        Time grid on which to draw posterior samples after training.\n\n    num_samples : int, optional\n        Number of GP samples to draw from the posterior.\n\n    verbose : bool, optional\n        Whether to print model selection, training progress, and sampling diagnostics.\n\n    Attributes\n    ----------\n    model : gpytorch.models.ExactGP\n        The trained GP model used for prediction and sampling.\n\n    likelihood : gpytorch.likelihoods.Likelihood\n        The likelihood model used (e.g., Gaussian with or without fixed noise).\n\n    train_times : torch.Tensor\n        Time points used for training the GP.\n\n    train_rates : torch.Tensor\n        Rate values used for training.\n\n    train_errors : torch.Tensor\n        Measurement uncertainties (empty if not provided).\n\n    samples : ndarray\n        Posterior samples drawn after training (used by downstream STELA modules).\n\n    pred_times : torch.Tensor\n        Time grid on which posterior samples were drawn.\n\n    kernel_form : str\n        Name of the kernel used in the final trained model.\n    \"\"\"\n\n    def __init__(self,\n                 lightcurve,\n                 kernel_form='auto',\n                 white_noise=True,\n                 enforce_normality=False,\n                 run_training=True,\n                 plot_training=False,\n                 num_iter=500,\n                 learn_rate=1e-1,\n                 sample_time_grid=[],\n                 num_samples=1000,\n                 verbose=False):\n\n        # To Do: reconsider noise prior, add a mean function function for forecasting, more verbose options\n        _CheckInputs._check_input_data(lightcurve, req_reg_samp=False)\n        self.lc = deepcopy(lightcurve)\n\n        # Save original mean, std, boxcox parameter for reversing standardization\n        self.lc_mean = getattr(self.lc, 'unstandard_mean', np.mean(self.lc.rates))\n        self.lc_std = getattr(self.lc, 'unstandard_std', np.std(self.lc.rates))\n        self.lambda_boxcox = getattr(self.lc, \"lambda_boxcox\", None)\n\n        # Check normality and apply boxcox if user specifies\n        if enforce_normality:\n            self.enforce_normality()\n\n        # Standardize data\n        if not getattr(self.lc, \"is_standard\", False):\n            Preprocessing.standardize(self.lc)\n\n        # Convert light curve data to pytorch tensors\n        self.train_times = torch.tensor(self.lc.times, dtype=torch.float32)\n        self.train_rates = torch.tensor(self.lc.rates, dtype=torch.float32)\n        if self.lc.errors.size &gt; 0:\n            self.train_errors = torch.tensor(self.lc.errors, dtype=torch.float32)\n        else:\n            self.train_errors = torch.tensor([])\n\n        # Training\n        self.white_noise = white_noise\n        if kernel_form == 'auto' or isinstance(kernel_form, list):\n            # Automatically select the best kernel based on AIC\n            if isinstance(kernel_form, list):\n                kernel_list = kernel_form\n            else:\n                kernel_list = ['Matern12', 'Matern32',\n                               'Matern52', 'RQ', 'RBF', 'SpectralMixture, 4']\n\n            best_model, best_likelihood = self.find_best_kernel(\n                kernel_list, num_iter=num_iter, learn_rate=learn_rate, verbose=verbose\n            )\n            self.model = best_model\n            self.likelihood = best_likelihood\n        else:\n            # Use specified kernel\n            self.likelihood = self.set_likelihood(self.white_noise, train_errors=self.train_errors)\n            self.model = self.create_gp_model(self.likelihood, kernel_form)\n\n            # Separate training needed only if kernel not automatically selected\n            if run_training:\n                self.train(num_iter=num_iter, learn_rate=learn_rate, plot=plot_training, verbose=verbose)\n\n        # Generate samples if sample_time_grid is provided\n        if sample_time_grid:\n            self.samples = self.sample(sample_time_grid, num_samples=num_samples)\n            if verbose:\n                print(f\"Samples generated: {self.samples.shape}, access with 'samples' attribute.\")\n\n        # Unstandardize the data\n        Preprocessing.unstandardize(self.lc)\n\n        # Undo boxcox transformation if needed\n        if getattr(self.lc, \"is_boxcox_transformed\", False):\n            Preprocessing.reverse_boxcox_transform(self.lc)\n\n    def enforce_normality(self):\n        \"\"\"\n        Check normality of the input data and apply a Box-Cox transformation if needed.\n\n        This method first checks if the light curve's flux distribution appears normal.\n        If not, a Box-Cox transformation is applied to improve it. STELA automatically\n        selects the most appropriate test (Shapiro-Wilk or Lilliefors) based on sample size.\n        \"\"\"\n        print(\"Checking normality of input light curve...\")\n\n        is_normal_before, pval_before = Preprocessing.check_normal(self.lc, plot=False, verbose=False)\n\n        if is_normal_before:\n            print(f\"\\n - Light curve appears normal (p = {pval_before:.4f}). No transformation applied.\")\n            return\n\n        print(f\"\\n - Light curve is not normal (p = {pval_before:.4f}). Applying Box-Cox transformation...\")\n\n        # Apply Box-Cox transformation\n        Preprocessing.boxcox_transform(self.lc)\n\n        if self.lambda_boxcox is not None:\n            print(\" -- Note: The input was already Box-Cox transformed. No additional transformation made.\")\n        else:\n            self.lambda_boxcox = getattr(self.lc, \"lambda_boxcox\", None)\n\n        # Re-check normality\n        is_normal_after, pval_after = Preprocessing.check_normal(self.lc, plot=False, verbose=False)\n\n        if is_normal_after:\n            print(f\" - Normality sufficiently achieved after Box-Cox (p = {pval_after:.4f})! Proceed as normal!\\n\")\n        else:\n            print(f\" - Data still not normal after Box-Cox (p = {pval_after:.4f}). Proceed with caution.\\n\")\n\n\n    def create_gp_model(self, likelihood, kernel_form):\n        \"\"\"\n        Build a GP model with the specified likelihood and kernel.\n\n        Parameters\n        ----------\n        likelihood : gpytorch.likelihoods.Likelihood\n            The likelihood model to use (e.g., Gaussian or FixedNoise).\n\n        kernel_form : str\n            The kernel type (e.g., 'Matern32', 'SpectralMixture, 4').\n\n        Returns\n        -------\n        GPModel\n            A subclass of gpytorch.models.ExactGP for training.\n        \"\"\"\n\n        class GPModel(gpytorch.models.ExactGP):\n            def __init__(gp_self, train_times, train_rates, likelihood):\n                super(GPModel, gp_self).__init__(train_times, train_rates, likelihood)\n                gp_self.mean_module = gpytorch.means.ZeroMean()\n                gp_self.covar_module = self.set_kernel(kernel_form)\n\n            def forward(gp_self, x):\n                mean_x = gp_self.mean_module(x)\n                covar_x = gp_self.covar_module(x)\n                return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\n        return GPModel(self.train_times, self.train_rates, likelihood)\n\n    def set_likelihood(self, white_noise, train_errors=torch.tensor([])):\n        \"\"\"\n        Set up the GP likelihood model based on user input and data characteristics.\n\n        If error bars are available, uses a FixedNoiseGaussianLikelihood. Otherwise, defaults to a\n        GaussianLikelihood with optional white noise. If white noise is enabled, the noise level\n        is initialized based on Poisson statistics or variance in the data.\n\n        Parameters\n        ----------\n        white_noise : bool\n            Whether to include a learnable noise term in the model.\n\n        train_errors : torch.Tensor, optional\n            Measurement errors from the light curve.\n\n        Returns\n        -------\n        likelihood : gpytorch.likelihoods.Likelihood\n            GPyTorch subclass, also used for training.\n        \"\"\"\n\n        if white_noise:\n            noise_constraint = gpytorch.constraints.Interval(1e-5, 1)\n        else:\n            noise_constraint = gpytorch.constraints.Interval(1e-40, 1e-39)\n\n        if train_errors.size(dim=0) &gt; 0:\n            likelihood = gpytorch.likelihoods.FixedNoiseGaussianLikelihood(\n                noise=self.train_errors ** 2,\n                learn_additional_noise=white_noise,\n                noise_constraint=noise_constraint\n            )\n\n        else:\n            likelihood = gpytorch.likelihoods.GaussianLikelihood(\n                noise_constraint=noise_constraint,\n            )\n\n            if white_noise:\n                counts = np.abs(self.train_rates[1:].numpy()) * np.diff(self.train_times.numpy())\n                # begin with a slight underestimation to prevent overfitting\n                norm_poisson_var = 1 / (2 * np.mean(counts))\n                likelihood.noise = norm_poisson_var\n\n        # initialize noise parameter at the variance of the data\n        return likelihood\n\n    def set_kernel(self, kernel_form):\n        \"\"\"\n        Set the GP kernel (covariance function) based on user input.\n\n        Handles spectral mixture, Matern, RBF, and other kernel types supported by GPyTorch.\n        Applies reasonable defaults for lengthscale initialization.\n\n        Parameters\n        ----------\n        kernel_form : str\n            Name of the kernel, or 'SpectralMixture, N' to set the number of components.\n\n        Returns\n        -------\n        covar_module : gpytorch.kernels.Kernel\n        \"\"\"\n\n        kernel_form = kernel_form.strip()\n        if 'SpectralMixture' in kernel_form:\n            if ',' not in kernel_form:\n                raise ValueError(\n                    \"Invalid Spectral Mixture kernel format (use 'SpectralMixture, N').\\n\"\n                    \"N=4 is a good starting point.\"\n                )\n            else:\n                kernel_form, num_mixtures_str = kernel_form.split(',')\n                num_mixtures = int(num_mixtures_str.strip())\n        else:\n            num_mixtures = 4  # set num_mixtures for kernel_mapping when Spectral Mixture kernel not used\n\n        kernel_mapping = {\n            'Matern12': gpytorch.kernels.MaternKernel(nu=0.5),\n            'Matern32': gpytorch.kernels.MaternKernel(nu=1.5),\n            'Matern52': gpytorch.kernels.MaternKernel(nu=2.5),\n            'RQ': gpytorch.kernels.RQKernel(),\n            'RBF': gpytorch.kernels.RBFKernel(),\n            'SpectralMixture': gpytorch.kernels.SpectralMixtureKernel(num_mixtures=num_mixtures),\n            'Periodic': gpytorch.kernels.PeriodicKernel()\n        }\n\n        # Assign kernel if type is valid\n        if kernel_form in kernel_mapping:\n            kernel = kernel_mapping[kernel_form]\n        else:\n            raise ValueError(\n                f\"Invalid kernel functional form '{kernel_form}'. Choose from {list(kernel_mapping.keys())}.\")\n\n        if kernel_form == 'SpectralMixture':\n            kernel.initialize_from_data(self.train_times, self.train_rates)\n        else:\n            init_lengthscale = (self.train_times[-1] - self.train_times[0]) / 10\n            kernel.lengthscale = init_lengthscale\n\n        # Scale the kernel by a constant\n        covar_module = gpytorch.kernels.ScaleKernel(kernel)\n        self.kernel_form = kernel_form\n\n        return covar_module\n\n    def train(self, num_iter=500, learn_rate=1e-1, plot=False, verbose=False):\n        \"\"\"\n        Train the GP model using the Adam optimizer to minimize the negative log marginal likelihood (NLML).\n\n        By default, prints progress periodically and optionally plots the NLML loss curve over training iterations.\n        This function is typically called after initialization unless `run_training=True` was set earlier.\n\n        Parameters\n        ----------\n        num_iter : int, optional\n            Number of optimization steps to perform. Default is 500.\n\n        learn_rate : float, optional\n            Learning rate for the Adam optimizer. Default is 0.1.\n\n        plot : bool, optional\n            If True, display a plot of the NLML loss as training progresses.\n\n        verbose : bool, optional\n            If True, print progress updates at regular intervals during training.\n        \"\"\"\n\n        self.model.train()\n        self.likelihood.train()\n\n        optimizer = torch.optim.Adam(self.model.parameters(), lr=learn_rate)\n        mll = gpytorch.mlls.ExactMarginalLogLikelihood(self.likelihood, self.model)\n\n        print_every = max(1, num_iter // 20)\n\n        if plot:\n            plt.figure(figsize=(8, 5))\n\n        for i in range(num_iter):\n            optimizer.zero_grad()\n            output = self.model(self.train_times)\n            loss = -mll(output, self.train_rates)\n            loss.backward()\n\n            if verbose and (i == num_iter - 1 or i % print_every == 0):\n                if self.white_noise:\n                    if self.train_errors.size(dim=0) &gt; 0:\n                        noise_param = self.model.likelihood.second_noise.item()\n                    else:\n                        noise_param = self.model.likelihood.noise.item()\n\n                if self.kernel_form == 'SpectralMixture':\n                    mixture_scales = self.model.covar_module.base_kernel.mixture_scales\n                    mixture_scales = mixture_scales.detach().numpy().flatten()\n                    mixture_weights = self.model.covar_module.base_kernel.mixture_weights\n                    mixture_weights = mixture_weights.detach().numpy().flatten()\n\n                    if self.white_noise:\n                        print('Iter %d/%d - loss: %.3f   mixture_lengthscales: %s   mixture_weights: %s   noise: %.1e' % (\n                            i + 1, num_iter, loss.item(),\n                            mixture_scales.round(3),\n                            mixture_weights.round(3),\n                            noise_param\n                        ))\n                    else:\n                        print('Iter %d/%d - loss: %.3f   mixture_lengthscales: %s   mixture_weights: %s' % (\n                            i + 1, num_iter, loss.item(),\n                            mixture_scales.round(3),\n                            mixture_weights.round(3)\n                        ))\n\n                elif self.kernel_form == 'Periodic':\n                    if self.white_noise:\n                        print('Iter %d/%d - loss: %.3f   period length: %.3f   lengthscale: %.3f   noise: %.1e' % (\n                            i + 1, num_iter, loss.item(),\n                            self.model.covar_module.base_kernel.period_length.item(),\n                            self.model.covar_module.base_kernel.lengthscale.item(),\n                            noise_param\n                        ))\n                    else:\n                        print('Iter %d/%d - loss: %.3f   lengthscale: %.1e' % (\n                            i + 1, num_iter, loss.item(),\n                            self.model.covar_module.base_kernel.lengthscale.item()\n                        ))\n\n                else:\n                    if self.white_noise:\n                        print('Iter %d/%d - loss: %.3f   lengthscale: %.3f   noise: %.1e' % (\n                            i + 1, num_iter, loss.item(),\n                            self.model.covar_module.base_kernel.lengthscale.item(),\n                            noise_param\n                        ))\n                    else:\n                        print('Iter %d/%d - loss: %.3f   lengthscale: %.1e' % (\n                            i + 1, num_iter, loss.item(),\n                            self.model.covar_module.base_kernel.lengthscale.item()\n                        ))\n\n            optimizer.step()\n\n            if plot:\n                plt.scatter(i, loss.item(), color='black', s=2)\n\n        if verbose:\n            final_hypers = self.get_hyperparameters()\n            print(\n                \"Training complete. \\n\"\n                f\"   - Final loss: {loss.item():0.5}\\n\"\n                f\"   - Final hyperparameters:\")\n            for key, value in final_hypers.items():\n                print(f\"      {key:42}: {np.round(value, 4)}\")\n\n        if plot:\n            plt.xlabel('Iteration')\n            plt.ylabel('Negative Marginal Log Likelihood')\n            plt.title('Training Progress')\n            plt.show()\n\n    def find_best_kernel(self, kernel_list, num_iter=500, learn_rate=1e-1, verbose=False):\n        \"\"\"\n        Search over a list of kernels and return the best one by AIC.\n\n        Trains the model separately with each kernel in the list, computes the AIC,\n        and returns the model with the lowest value.\n\n        Parameters\n        ----------\n        kernel_list : list of str\n            Kernel names to try.\n\n        num_iter : int\n            Number of iterations per training run.\n\n        learn_rate : float\n            Learning rate for the optimizer.\n\n        verbose : bool\n            Whether to print progress for each kernel.\n\n        Returns\n        -------\n        best_model : GPModel\n            The model trained with the best-performing kernel.\n\n        best_likelihood : gpytorch.likelihoods.Likelihood\n            Corresponding likelihood for the best model.\n        \"\"\"\n\n        aics = []\n        best_model = None\n        for kernel_form in kernel_list:\n            self.likelihood = self.set_likelihood(self.white_noise, train_errors=self.train_errors)\n            self.model = self.create_gp_model(self.likelihood, kernel_form)\n            # suppress output, even for verbose=True\n            self.train(num_iter=num_iter, learn_rate=learn_rate, verbose=False)\n\n            # compute aic and store best model\n            aic = self.aic()\n            aics.append(aic)\n            if aic &lt;= min(aics):\n                best_model = self.model\n                best_likelihood = self.likelihood\n\n        best_aic = min(aics)\n        best_kernel = kernel_list[aics.index(best_aic)]\n\n        if verbose:\n            kernel_results = zip(kernel_list, aics)\n            print(\n                \"Kernel selection complete.\\n\"\n                f\"   Kernel AICs (lower is better):\"\n            )\n            for kernel, aic in kernel_results:\n                print(f\"     - {kernel:15}: {aic:0.5}\")\n\n            print(f\"   Best kernel: {best_kernel} (AIC: {best_aic:0.5})\")\n\n        self.kernel_form = best_kernel\n        return best_model, best_likelihood\n\n    def get_hyperparameters(self):\n        \"\"\"\n        Return the learned GP hyperparameters (lengthscale, noise, weights, etc.).\n\n        Returns\n        -------\n        hyper_dict : dict\n            Dictionary mapping parameter names to their (transformed) values.\n                Note: All rate-associated hyperparameters (e.g., not lengthscale) \n                are in units of the standardized data, not the original flux/time units.\n        \"\"\"\n\n        raw_hypers = self.model.named_parameters()\n        hypers = {}\n        for param_name, param in raw_hypers:\n            # Split the parameter name into hierarchy\n            parts = param_name.split('.')\n            module = self.model\n\n            # Traverse structure of the model to get the constraint\n            for part in parts[:-1]:  # last part is parameter\n                module = getattr(module, part, None)\n                if module is None:\n                    raise AttributeError(\n                        f\"Module '{part}' not found while traversing '{param_name}'.\")\n\n            final_param_name = parts[-1]\n            constraint_name = f\"{final_param_name}_constraint\"\n            constraint = getattr(module, constraint_name, None)\n\n            if constraint is None:\n                raise AttributeError(\n                    f\"Constraint '{constraint_name}' not found in module '{module}'.\")\n\n            # Transform the parameter using the constraint\n            transform_param = constraint.transform(param)\n\n            # Remove 'raw_' prefix from the parameter name for readability\n            param_name_withoutraw = param_name.replace('raw_', '')\n\n            if self.kernel_form == 'SpectralMixture':\n                transform_param = transform_param.detach().numpy().flatten()\n            else:\n                transform_param = transform_param.item()\n\n            hypers[param_name_withoutraw] = transform_param\n\n        return hypers\n\n    def bic(self):\n        \"\"\"\n        Compute the Bayesian Information Criterion (BIC) for the trained model.\n\n        Returns\n        -------\n        bic : float\n            The BIC value (lower is better).\n        \"\"\"\n\n        mll = gpytorch.mlls.ExactMarginalLogLikelihood(self.likelihood, self.model)\n        log_marg_like = mll(\n            self.model(self.train_times), self.train_rates\n        ).item()\n\n        num_params = sum([p.numel() for p in self.model.parameters()])\n        num_data = len(self.train_times)\n\n        bic = -2 * log_marg_like + num_params * np.log(num_data)\n        return bic\n\n    def aic(self):\n        \"\"\"\n        Compute the Akaike Information Criterion (AIC) for the trained model.\n\n        Returns\n        -------\n        aic : float\n            The AIC value (lower is better).\n        \"\"\"\n\n        mll = gpytorch.mlls.ExactMarginalLogLikelihood(self.likelihood, self.model)\n        log_marg_like = mll(\n            self.model(self.train_times), self.train_rates\n        ).item()\n\n        num_params = sum([p.numel() for p in self.model.parameters()])\n\n        aic = -2 * log_marg_like + 2 * num_params\n        return aic\n\n    def sample(self, pred_times, num_samples, save_path=None, _save_to_state=True):\n        \"\"\"\n        Generate posterior samples from the trained GP model.\n\n        These samples represent plausible realizations of the light curve. These are what is used\n        by the coherence, power spectrum, and lag modules when a GP model is passed in.\n\n        Parameters\n        ----------\n        pred_times : array-like\n            Time points where samples should be drawn.\n\n        num_samples : int\n            Number of realizations to generate.\n\n        save_path : str, optional\n            File path to save the samples.\n\n        _save_to_state : bool, optional\n            Whether to store results in the object (used by other classes).\n\n        Returns\n        -------\n        samples : ndarray\n            Array of sampled light curves with shape (num_samples, len(pred_times)).\n        \"\"\"\n\n        pred_times_tensor = torch.tensor(pred_times, dtype=torch.float32)\n        self.model.eval()\n        self.likelihood.eval()\n\n        # Make predictions\n        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n            pred_dist = self.likelihood(self.model(pred_times_tensor))\n            post_samples = pred_dist.sample(sample_shape=torch.Size([num_samples]))\n\n        samples = post_samples.numpy()\n        samples = self._undo_transforms(samples)\n\n        if save_path:\n            samples_with_time = np.insert(pred_times, num_samples, 0)\n            file_ext = save_path.split(\".\")[-1]\n            if file_ext == \"npy\":\n                np.save(save_path, samples_with_time)\n            else:\n                np.savetxt(save_path, samples_with_time)\n\n        if _save_to_state:\n            self.pred_times = pred_times\n            self.samples = samples\n        return samples\n\n    def predict(self, pred_times):\n        \"\"\"\n        Compute the posterior mean and 2-sigma confidence intervals at specified times.\n\n        Parameters\n        ----------\n        pred_times : array-like\n            Time values to predict.\n\n        Returns\n        -------\n        mean, lower, upper : ndarray\n            Predicted mean and lower/upper bounds of the 95 percent confidence interval.\n        \"\"\"\n\n        # Check if pred_times is a torch tensor\n        if not isinstance(pred_times, torch.Tensor):\n            try:\n                pred_times = torch.tensor(pred_times, dtype=torch.float32)\n            except TypeError:\n                raise TypeError(\"pred_times must be a torch tensor or convertible to one.\")\n\n        self.model.eval()\n        self.likelihood.eval()\n\n        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n            pred_dist = self.likelihood(self.model(pred_times))\n            mean = pred_dist.mean\n            lower, upper = pred_dist.confidence_region()\n\n        # Unstandardize/unboxcox\n        mean = self._undo_transforms(mean)\n        lower = self._undo_transforms(lower)\n        upper = self._undo_transforms(upper)\n        return mean.numpy(), lower.numpy(), upper.numpy()\n\n    def plot(self, pred_times=None):\n        \"\"\"\n        Plot the GP fit including mean, confidence intervals, one posterior sample, and data.\n\n        Parameters\n        ----------\n        pred_times : array_like, optional\n            Time grid to show prediction, samples. If not specificed, a grid of 1000 points will be automatically used.\n        \"\"\"\n\n        if pred_times is None:\n            step = (self.train_times.max() - self.train_times.min()) / 1000\n            pred_times = np.arange(self.train_times.min(), self.train_times.max() + step, step)\n\n        predict_mean, predict_lower, predict_upper = self.predict(pred_times)\n\n        plt.figure(figsize=(8, 4.5))\n\n        plt.fill_between(pred_times, predict_lower, predict_upper,\n                         color='dodgerblue', alpha=0.2, label=r'Prediction 2$\\sigma$ CI')\n        plt.plot(pred_times, predict_mean, color='dodgerblue', label='Prediction Mean')\n\n        sample = self.sample(pred_times, num_samples=1, _save_to_state=False)\n        plt.plot(pred_times, sample[0], color='orange', lw=1, label='Sample')\n\n        if self.train_errors.size(dim=0) &gt; 0:\n            plt.errorbar(self.lc.times, self.lc.rates, yerr=self.lc.errors,\n                         fmt='o', color='black', lw=1.5, ms=3)\n        else:\n            plt.scatter(self.lc.times, self.lc.rates, color='black', s=6)\n\n        plt.xlabel('Time', fontsize=12)\n        plt.ylabel('Rate', fontsize=12)\n        plt.legend()\n        plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n        plt.tick_params(which='both', direction='in', length=6, width=1,\n                        top=True, right=True, labelsize=12)\n        plt.show()\n\n    def save(self, file_path):\n        \"\"\"\n        Save the trained GP model to a file using pickle.\n\n        Parameters\n        ----------\n        file_path : str\n            Path to save the model.\n        \"\"\"\n\n        with open(file_path, \"wb\") as f:\n            pickle.dump(self, f)\n        print(f\"GaussianProcess instance saved to {file_path}.\")\n\n    @staticmethod\n    def load(file_path):\n        \"\"\"\n        Load a saved GaussianProcess model from file.\n\n        Parameters\n        ----------\n        file_path : str\n            Path to the saved file.\n\n        Returns\n        -------\n        GaussianProcess\n            Restored instance of the model.\n        \"\"\"\n\n        with open(file_path, \"rb\") as f:\n            instance = pickle.load(f)\n        print(f\"GaussianProcess instance loaded from {file_path}.\")\n        return instance\n\n    def _undo_transforms(self, array):\n        \"\"\"\n        Reverse Box-Cox and standardization transformations applied to GP outputs.\n\n        Parameters\n        ----------\n        array : ndarray\n            Input values in transformed space.\n\n        Returns\n        -------\n        array : ndarray\n            Values in original flux units.\n        \"\"\"\n\n        if self.lambda_boxcox is not None:\n            if self.lambda_boxcox == 0:\n                array = np.exp(array)\n            else:\n                array = (array * self.lambda_boxcox + 1) ** (1 / self.lambda_boxcox)\n\n        array = array * self.lc_std + self.lc_mean\n        return array\n</code></pre>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess.aic","title":"<code>aic()</code>","text":"<p>Compute the Akaike Information Criterion (AIC) for the trained model.</p> <p>Returns:</p> Name Type Description <code>aic</code> <code>float</code> <p>The AIC value (lower is better).</p> Source code in <code>stela_toolkit/gaussian_process.py</code> <pre><code>def aic(self):\n    \"\"\"\n    Compute the Akaike Information Criterion (AIC) for the trained model.\n\n    Returns\n    -------\n    aic : float\n        The AIC value (lower is better).\n    \"\"\"\n\n    mll = gpytorch.mlls.ExactMarginalLogLikelihood(self.likelihood, self.model)\n    log_marg_like = mll(\n        self.model(self.train_times), self.train_rates\n    ).item()\n\n    num_params = sum([p.numel() for p in self.model.parameters()])\n\n    aic = -2 * log_marg_like + 2 * num_params\n    return aic\n</code></pre>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess.bic","title":"<code>bic()</code>","text":"<p>Compute the Bayesian Information Criterion (BIC) for the trained model.</p> <p>Returns:</p> Name Type Description <code>bic</code> <code>float</code> <p>The BIC value (lower is better).</p> Source code in <code>stela_toolkit/gaussian_process.py</code> <pre><code>def bic(self):\n    \"\"\"\n    Compute the Bayesian Information Criterion (BIC) for the trained model.\n\n    Returns\n    -------\n    bic : float\n        The BIC value (lower is better).\n    \"\"\"\n\n    mll = gpytorch.mlls.ExactMarginalLogLikelihood(self.likelihood, self.model)\n    log_marg_like = mll(\n        self.model(self.train_times), self.train_rates\n    ).item()\n\n    num_params = sum([p.numel() for p in self.model.parameters()])\n    num_data = len(self.train_times)\n\n    bic = -2 * log_marg_like + num_params * np.log(num_data)\n    return bic\n</code></pre>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess.create_gp_model","title":"<code>create_gp_model(likelihood, kernel_form)</code>","text":"<p>Build a GP model with the specified likelihood and kernel.</p> <p>Parameters:</p> Name Type Description Default <code>likelihood</code> <code>Likelihood</code> <p>The likelihood model to use (e.g., Gaussian or FixedNoise).</p> required <code>kernel_form</code> <code>str</code> <p>The kernel type (e.g., 'Matern32', 'SpectralMixture, 4').</p> required <p>Returns:</p> Type Description <code>GPModel</code> <p>A subclass of gpytorch.models.ExactGP for training.</p> Source code in <code>stela_toolkit/gaussian_process.py</code> <pre><code>def create_gp_model(self, likelihood, kernel_form):\n    \"\"\"\n    Build a GP model with the specified likelihood and kernel.\n\n    Parameters\n    ----------\n    likelihood : gpytorch.likelihoods.Likelihood\n        The likelihood model to use (e.g., Gaussian or FixedNoise).\n\n    kernel_form : str\n        The kernel type (e.g., 'Matern32', 'SpectralMixture, 4').\n\n    Returns\n    -------\n    GPModel\n        A subclass of gpytorch.models.ExactGP for training.\n    \"\"\"\n\n    class GPModel(gpytorch.models.ExactGP):\n        def __init__(gp_self, train_times, train_rates, likelihood):\n            super(GPModel, gp_self).__init__(train_times, train_rates, likelihood)\n            gp_self.mean_module = gpytorch.means.ZeroMean()\n            gp_self.covar_module = self.set_kernel(kernel_form)\n\n        def forward(gp_self, x):\n            mean_x = gp_self.mean_module(x)\n            covar_x = gp_self.covar_module(x)\n            return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n\n    return GPModel(self.train_times, self.train_rates, likelihood)\n</code></pre>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess.enforce_normality","title":"<code>enforce_normality()</code>","text":"<p>Check normality of the input data and apply a Box-Cox transformation if needed.</p> <p>This method first checks if the light curve's flux distribution appears normal. If not, a Box-Cox transformation is applied to improve it. STELA automatically selects the most appropriate test (Shapiro-Wilk or Lilliefors) based on sample size.</p> Source code in <code>stela_toolkit/gaussian_process.py</code> <pre><code>def enforce_normality(self):\n    \"\"\"\n    Check normality of the input data and apply a Box-Cox transformation if needed.\n\n    This method first checks if the light curve's flux distribution appears normal.\n    If not, a Box-Cox transformation is applied to improve it. STELA automatically\n    selects the most appropriate test (Shapiro-Wilk or Lilliefors) based on sample size.\n    \"\"\"\n    print(\"Checking normality of input light curve...\")\n\n    is_normal_before, pval_before = Preprocessing.check_normal(self.lc, plot=False, verbose=False)\n\n    if is_normal_before:\n        print(f\"\\n - Light curve appears normal (p = {pval_before:.4f}). No transformation applied.\")\n        return\n\n    print(f\"\\n - Light curve is not normal (p = {pval_before:.4f}). Applying Box-Cox transformation...\")\n\n    # Apply Box-Cox transformation\n    Preprocessing.boxcox_transform(self.lc)\n\n    if self.lambda_boxcox is not None:\n        print(\" -- Note: The input was already Box-Cox transformed. No additional transformation made.\")\n    else:\n        self.lambda_boxcox = getattr(self.lc, \"lambda_boxcox\", None)\n\n    # Re-check normality\n    is_normal_after, pval_after = Preprocessing.check_normal(self.lc, plot=False, verbose=False)\n\n    if is_normal_after:\n        print(f\" - Normality sufficiently achieved after Box-Cox (p = {pval_after:.4f})! Proceed as normal!\\n\")\n    else:\n        print(f\" - Data still not normal after Box-Cox (p = {pval_after:.4f}). Proceed with caution.\\n\")\n</code></pre>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess.find_best_kernel","title":"<code>find_best_kernel(kernel_list, num_iter=500, learn_rate=0.1, verbose=False)</code>","text":"<p>Search over a list of kernels and return the best one by AIC.</p> <p>Trains the model separately with each kernel in the list, computes the AIC, and returns the model with the lowest value.</p> <p>Parameters:</p> Name Type Description Default <code>kernel_list</code> <code>list of str</code> <p>Kernel names to try.</p> required <code>num_iter</code> <code>int</code> <p>Number of iterations per training run.</p> <code>500</code> <code>learn_rate</code> <code>float</code> <p>Learning rate for the optimizer.</p> <code>0.1</code> <code>verbose</code> <code>bool</code> <p>Whether to print progress for each kernel.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>best_model</code> <code>GPModel</code> <p>The model trained with the best-performing kernel.</p> <code>best_likelihood</code> <code>Likelihood</code> <p>Corresponding likelihood for the best model.</p> Source code in <code>stela_toolkit/gaussian_process.py</code> <pre><code>def find_best_kernel(self, kernel_list, num_iter=500, learn_rate=1e-1, verbose=False):\n    \"\"\"\n    Search over a list of kernels and return the best one by AIC.\n\n    Trains the model separately with each kernel in the list, computes the AIC,\n    and returns the model with the lowest value.\n\n    Parameters\n    ----------\n    kernel_list : list of str\n        Kernel names to try.\n\n    num_iter : int\n        Number of iterations per training run.\n\n    learn_rate : float\n        Learning rate for the optimizer.\n\n    verbose : bool\n        Whether to print progress for each kernel.\n\n    Returns\n    -------\n    best_model : GPModel\n        The model trained with the best-performing kernel.\n\n    best_likelihood : gpytorch.likelihoods.Likelihood\n        Corresponding likelihood for the best model.\n    \"\"\"\n\n    aics = []\n    best_model = None\n    for kernel_form in kernel_list:\n        self.likelihood = self.set_likelihood(self.white_noise, train_errors=self.train_errors)\n        self.model = self.create_gp_model(self.likelihood, kernel_form)\n        # suppress output, even for verbose=True\n        self.train(num_iter=num_iter, learn_rate=learn_rate, verbose=False)\n\n        # compute aic and store best model\n        aic = self.aic()\n        aics.append(aic)\n        if aic &lt;= min(aics):\n            best_model = self.model\n            best_likelihood = self.likelihood\n\n    best_aic = min(aics)\n    best_kernel = kernel_list[aics.index(best_aic)]\n\n    if verbose:\n        kernel_results = zip(kernel_list, aics)\n        print(\n            \"Kernel selection complete.\\n\"\n            f\"   Kernel AICs (lower is better):\"\n        )\n        for kernel, aic in kernel_results:\n            print(f\"     - {kernel:15}: {aic:0.5}\")\n\n        print(f\"   Best kernel: {best_kernel} (AIC: {best_aic:0.5})\")\n\n    self.kernel_form = best_kernel\n    return best_model, best_likelihood\n</code></pre>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess.get_hyperparameters","title":"<code>get_hyperparameters()</code>","text":"<p>Return the learned GP hyperparameters (lengthscale, noise, weights, etc.).</p> <p>Returns:</p> Name Type Description <code>hyper_dict</code> <code>dict</code> <p>Dictionary mapping parameter names to their (transformed) values.     Note: All rate-associated hyperparameters (e.g., not lengthscale)      are in units of the standardized data, not the original flux/time units.</p> Source code in <code>stela_toolkit/gaussian_process.py</code> <pre><code>def get_hyperparameters(self):\n    \"\"\"\n    Return the learned GP hyperparameters (lengthscale, noise, weights, etc.).\n\n    Returns\n    -------\n    hyper_dict : dict\n        Dictionary mapping parameter names to their (transformed) values.\n            Note: All rate-associated hyperparameters (e.g., not lengthscale) \n            are in units of the standardized data, not the original flux/time units.\n    \"\"\"\n\n    raw_hypers = self.model.named_parameters()\n    hypers = {}\n    for param_name, param in raw_hypers:\n        # Split the parameter name into hierarchy\n        parts = param_name.split('.')\n        module = self.model\n\n        # Traverse structure of the model to get the constraint\n        for part in parts[:-1]:  # last part is parameter\n            module = getattr(module, part, None)\n            if module is None:\n                raise AttributeError(\n                    f\"Module '{part}' not found while traversing '{param_name}'.\")\n\n        final_param_name = parts[-1]\n        constraint_name = f\"{final_param_name}_constraint\"\n        constraint = getattr(module, constraint_name, None)\n\n        if constraint is None:\n            raise AttributeError(\n                f\"Constraint '{constraint_name}' not found in module '{module}'.\")\n\n        # Transform the parameter using the constraint\n        transform_param = constraint.transform(param)\n\n        # Remove 'raw_' prefix from the parameter name for readability\n        param_name_withoutraw = param_name.replace('raw_', '')\n\n        if self.kernel_form == 'SpectralMixture':\n            transform_param = transform_param.detach().numpy().flatten()\n        else:\n            transform_param = transform_param.item()\n\n        hypers[param_name_withoutraw] = transform_param\n\n    return hypers\n</code></pre>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess.load","title":"<code>load(file_path)</code>  <code>staticmethod</code>","text":"<p>Load a saved GaussianProcess model from file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the saved file.</p> required <p>Returns:</p> Type Description <code>GaussianProcess</code> <p>Restored instance of the model.</p> Source code in <code>stela_toolkit/gaussian_process.py</code> <pre><code>@staticmethod\ndef load(file_path):\n    \"\"\"\n    Load a saved GaussianProcess model from file.\n\n    Parameters\n    ----------\n    file_path : str\n        Path to the saved file.\n\n    Returns\n    -------\n    GaussianProcess\n        Restored instance of the model.\n    \"\"\"\n\n    with open(file_path, \"rb\") as f:\n        instance = pickle.load(f)\n    print(f\"GaussianProcess instance loaded from {file_path}.\")\n    return instance\n</code></pre>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess.plot","title":"<code>plot(pred_times=None)</code>","text":"<p>Plot the GP fit including mean, confidence intervals, one posterior sample, and data.</p> <p>Parameters:</p> Name Type Description Default <code>pred_times</code> <code>array_like</code> <p>Time grid to show prediction, samples. If not specificed, a grid of 1000 points will be automatically used.</p> <code>None</code> Source code in <code>stela_toolkit/gaussian_process.py</code> <pre><code>def plot(self, pred_times=None):\n    \"\"\"\n    Plot the GP fit including mean, confidence intervals, one posterior sample, and data.\n\n    Parameters\n    ----------\n    pred_times : array_like, optional\n        Time grid to show prediction, samples. If not specificed, a grid of 1000 points will be automatically used.\n    \"\"\"\n\n    if pred_times is None:\n        step = (self.train_times.max() - self.train_times.min()) / 1000\n        pred_times = np.arange(self.train_times.min(), self.train_times.max() + step, step)\n\n    predict_mean, predict_lower, predict_upper = self.predict(pred_times)\n\n    plt.figure(figsize=(8, 4.5))\n\n    plt.fill_between(pred_times, predict_lower, predict_upper,\n                     color='dodgerblue', alpha=0.2, label=r'Prediction 2$\\sigma$ CI')\n    plt.plot(pred_times, predict_mean, color='dodgerblue', label='Prediction Mean')\n\n    sample = self.sample(pred_times, num_samples=1, _save_to_state=False)\n    plt.plot(pred_times, sample[0], color='orange', lw=1, label='Sample')\n\n    if self.train_errors.size(dim=0) &gt; 0:\n        plt.errorbar(self.lc.times, self.lc.rates, yerr=self.lc.errors,\n                     fmt='o', color='black', lw=1.5, ms=3)\n    else:\n        plt.scatter(self.lc.times, self.lc.rates, color='black', s=6)\n\n    plt.xlabel('Time', fontsize=12)\n    plt.ylabel('Rate', fontsize=12)\n    plt.legend()\n    plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n    plt.tick_params(which='both', direction='in', length=6, width=1,\n                    top=True, right=True, labelsize=12)\n    plt.show()\n</code></pre>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess.predict","title":"<code>predict(pred_times)</code>","text":"<p>Compute the posterior mean and 2-sigma confidence intervals at specified times.</p> <p>Parameters:</p> Name Type Description Default <code>pred_times</code> <code>array - like</code> <p>Time values to predict.</p> required <p>Returns:</p> Type Description <code>mean, lower, upper : ndarray</code> <p>Predicted mean and lower/upper bounds of the 95 percent confidence interval.</p> Source code in <code>stela_toolkit/gaussian_process.py</code> <pre><code>def predict(self, pred_times):\n    \"\"\"\n    Compute the posterior mean and 2-sigma confidence intervals at specified times.\n\n    Parameters\n    ----------\n    pred_times : array-like\n        Time values to predict.\n\n    Returns\n    -------\n    mean, lower, upper : ndarray\n        Predicted mean and lower/upper bounds of the 95 percent confidence interval.\n    \"\"\"\n\n    # Check if pred_times is a torch tensor\n    if not isinstance(pred_times, torch.Tensor):\n        try:\n            pred_times = torch.tensor(pred_times, dtype=torch.float32)\n        except TypeError:\n            raise TypeError(\"pred_times must be a torch tensor or convertible to one.\")\n\n    self.model.eval()\n    self.likelihood.eval()\n\n    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n        pred_dist = self.likelihood(self.model(pred_times))\n        mean = pred_dist.mean\n        lower, upper = pred_dist.confidence_region()\n\n    # Unstandardize/unboxcox\n    mean = self._undo_transforms(mean)\n    lower = self._undo_transforms(lower)\n    upper = self._undo_transforms(upper)\n    return mean.numpy(), lower.numpy(), upper.numpy()\n</code></pre>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess.sample","title":"<code>sample(pred_times, num_samples, save_path=None, _save_to_state=True)</code>","text":"<p>Generate posterior samples from the trained GP model.</p> <p>These samples represent plausible realizations of the light curve. These are what is used by the coherence, power spectrum, and lag modules when a GP model is passed in.</p> <p>Parameters:</p> Name Type Description Default <code>pred_times</code> <code>array - like</code> <p>Time points where samples should be drawn.</p> required <code>num_samples</code> <code>int</code> <p>Number of realizations to generate.</p> required <code>save_path</code> <code>str</code> <p>File path to save the samples.</p> <code>None</code> <code>_save_to_state</code> <code>bool</code> <p>Whether to store results in the object (used by other classes).</p> <code>True</code> <p>Returns:</p> Name Type Description <code>samples</code> <code>ndarray</code> <p>Array of sampled light curves with shape (num_samples, len(pred_times)).</p> Source code in <code>stela_toolkit/gaussian_process.py</code> <pre><code>def sample(self, pred_times, num_samples, save_path=None, _save_to_state=True):\n    \"\"\"\n    Generate posterior samples from the trained GP model.\n\n    These samples represent plausible realizations of the light curve. These are what is used\n    by the coherence, power spectrum, and lag modules when a GP model is passed in.\n\n    Parameters\n    ----------\n    pred_times : array-like\n        Time points where samples should be drawn.\n\n    num_samples : int\n        Number of realizations to generate.\n\n    save_path : str, optional\n        File path to save the samples.\n\n    _save_to_state : bool, optional\n        Whether to store results in the object (used by other classes).\n\n    Returns\n    -------\n    samples : ndarray\n        Array of sampled light curves with shape (num_samples, len(pred_times)).\n    \"\"\"\n\n    pred_times_tensor = torch.tensor(pred_times, dtype=torch.float32)\n    self.model.eval()\n    self.likelihood.eval()\n\n    # Make predictions\n    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n        pred_dist = self.likelihood(self.model(pred_times_tensor))\n        post_samples = pred_dist.sample(sample_shape=torch.Size([num_samples]))\n\n    samples = post_samples.numpy()\n    samples = self._undo_transforms(samples)\n\n    if save_path:\n        samples_with_time = np.insert(pred_times, num_samples, 0)\n        file_ext = save_path.split(\".\")[-1]\n        if file_ext == \"npy\":\n            np.save(save_path, samples_with_time)\n        else:\n            np.savetxt(save_path, samples_with_time)\n\n    if _save_to_state:\n        self.pred_times = pred_times\n        self.samples = samples\n    return samples\n</code></pre>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess.save","title":"<code>save(file_path)</code>","text":"<p>Save the trained GP model to a file using pickle.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to save the model.</p> required Source code in <code>stela_toolkit/gaussian_process.py</code> <pre><code>def save(self, file_path):\n    \"\"\"\n    Save the trained GP model to a file using pickle.\n\n    Parameters\n    ----------\n    file_path : str\n        Path to save the model.\n    \"\"\"\n\n    with open(file_path, \"wb\") as f:\n        pickle.dump(self, f)\n    print(f\"GaussianProcess instance saved to {file_path}.\")\n</code></pre>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess.set_kernel","title":"<code>set_kernel(kernel_form)</code>","text":"<p>Set the GP kernel (covariance function) based on user input.</p> <p>Handles spectral mixture, Matern, RBF, and other kernel types supported by GPyTorch. Applies reasonable defaults for lengthscale initialization.</p> <p>Parameters:</p> Name Type Description Default <code>kernel_form</code> <code>str</code> <p>Name of the kernel, or 'SpectralMixture, N' to set the number of components.</p> required <p>Returns:</p> Name Type Description <code>covar_module</code> <code>Kernel</code> Source code in <code>stela_toolkit/gaussian_process.py</code> <pre><code>def set_kernel(self, kernel_form):\n    \"\"\"\n    Set the GP kernel (covariance function) based on user input.\n\n    Handles spectral mixture, Matern, RBF, and other kernel types supported by GPyTorch.\n    Applies reasonable defaults for lengthscale initialization.\n\n    Parameters\n    ----------\n    kernel_form : str\n        Name of the kernel, or 'SpectralMixture, N' to set the number of components.\n\n    Returns\n    -------\n    covar_module : gpytorch.kernels.Kernel\n    \"\"\"\n\n    kernel_form = kernel_form.strip()\n    if 'SpectralMixture' in kernel_form:\n        if ',' not in kernel_form:\n            raise ValueError(\n                \"Invalid Spectral Mixture kernel format (use 'SpectralMixture, N').\\n\"\n                \"N=4 is a good starting point.\"\n            )\n        else:\n            kernel_form, num_mixtures_str = kernel_form.split(',')\n            num_mixtures = int(num_mixtures_str.strip())\n    else:\n        num_mixtures = 4  # set num_mixtures for kernel_mapping when Spectral Mixture kernel not used\n\n    kernel_mapping = {\n        'Matern12': gpytorch.kernels.MaternKernel(nu=0.5),\n        'Matern32': gpytorch.kernels.MaternKernel(nu=1.5),\n        'Matern52': gpytorch.kernels.MaternKernel(nu=2.5),\n        'RQ': gpytorch.kernels.RQKernel(),\n        'RBF': gpytorch.kernels.RBFKernel(),\n        'SpectralMixture': gpytorch.kernels.SpectralMixtureKernel(num_mixtures=num_mixtures),\n        'Periodic': gpytorch.kernels.PeriodicKernel()\n    }\n\n    # Assign kernel if type is valid\n    if kernel_form in kernel_mapping:\n        kernel = kernel_mapping[kernel_form]\n    else:\n        raise ValueError(\n            f\"Invalid kernel functional form '{kernel_form}'. Choose from {list(kernel_mapping.keys())}.\")\n\n    if kernel_form == 'SpectralMixture':\n        kernel.initialize_from_data(self.train_times, self.train_rates)\n    else:\n        init_lengthscale = (self.train_times[-1] - self.train_times[0]) / 10\n        kernel.lengthscale = init_lengthscale\n\n    # Scale the kernel by a constant\n    covar_module = gpytorch.kernels.ScaleKernel(kernel)\n    self.kernel_form = kernel_form\n\n    return covar_module\n</code></pre>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess.set_likelihood","title":"<code>set_likelihood(white_noise, train_errors=torch.tensor([]))</code>","text":"<p>Set up the GP likelihood model based on user input and data characteristics.</p> <p>If error bars are available, uses a FixedNoiseGaussianLikelihood. Otherwise, defaults to a GaussianLikelihood with optional white noise. If white noise is enabled, the noise level is initialized based on Poisson statistics or variance in the data.</p> <p>Parameters:</p> Name Type Description Default <code>white_noise</code> <code>bool</code> <p>Whether to include a learnable noise term in the model.</p> required <code>train_errors</code> <code>Tensor</code> <p>Measurement errors from the light curve.</p> <code>tensor([])</code> <p>Returns:</p> Name Type Description <code>likelihood</code> <code>Likelihood</code> <p>GPyTorch subclass, also used for training.</p> Source code in <code>stela_toolkit/gaussian_process.py</code> <pre><code>def set_likelihood(self, white_noise, train_errors=torch.tensor([])):\n    \"\"\"\n    Set up the GP likelihood model based on user input and data characteristics.\n\n    If error bars are available, uses a FixedNoiseGaussianLikelihood. Otherwise, defaults to a\n    GaussianLikelihood with optional white noise. If white noise is enabled, the noise level\n    is initialized based on Poisson statistics or variance in the data.\n\n    Parameters\n    ----------\n    white_noise : bool\n        Whether to include a learnable noise term in the model.\n\n    train_errors : torch.Tensor, optional\n        Measurement errors from the light curve.\n\n    Returns\n    -------\n    likelihood : gpytorch.likelihoods.Likelihood\n        GPyTorch subclass, also used for training.\n    \"\"\"\n\n    if white_noise:\n        noise_constraint = gpytorch.constraints.Interval(1e-5, 1)\n    else:\n        noise_constraint = gpytorch.constraints.Interval(1e-40, 1e-39)\n\n    if train_errors.size(dim=0) &gt; 0:\n        likelihood = gpytorch.likelihoods.FixedNoiseGaussianLikelihood(\n            noise=self.train_errors ** 2,\n            learn_additional_noise=white_noise,\n            noise_constraint=noise_constraint\n        )\n\n    else:\n        likelihood = gpytorch.likelihoods.GaussianLikelihood(\n            noise_constraint=noise_constraint,\n        )\n\n        if white_noise:\n            counts = np.abs(self.train_rates[1:].numpy()) * np.diff(self.train_times.numpy())\n            # begin with a slight underestimation to prevent overfitting\n            norm_poisson_var = 1 / (2 * np.mean(counts))\n            likelihood.noise = norm_poisson_var\n\n    # initialize noise parameter at the variance of the data\n    return likelihood\n</code></pre>"},{"location":"reference/gaussian_process/#stela_toolkit.gaussian_process.GaussianProcess.train","title":"<code>train(num_iter=500, learn_rate=0.1, plot=False, verbose=False)</code>","text":"<p>Train the GP model using the Adam optimizer to minimize the negative log marginal likelihood (NLML).</p> <p>By default, prints progress periodically and optionally plots the NLML loss curve over training iterations. This function is typically called after initialization unless <code>run_training=True</code> was set earlier.</p> <p>Parameters:</p> Name Type Description Default <code>num_iter</code> <code>int</code> <p>Number of optimization steps to perform. Default is 500.</p> <code>500</code> <code>learn_rate</code> <code>float</code> <p>Learning rate for the Adam optimizer. Default is 0.1.</p> <code>0.1</code> <code>plot</code> <code>bool</code> <p>If True, display a plot of the NLML loss as training progresses.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>If True, print progress updates at regular intervals during training.</p> <code>False</code> Source code in <code>stela_toolkit/gaussian_process.py</code> <pre><code>def train(self, num_iter=500, learn_rate=1e-1, plot=False, verbose=False):\n    \"\"\"\n    Train the GP model using the Adam optimizer to minimize the negative log marginal likelihood (NLML).\n\n    By default, prints progress periodically and optionally plots the NLML loss curve over training iterations.\n    This function is typically called after initialization unless `run_training=True` was set earlier.\n\n    Parameters\n    ----------\n    num_iter : int, optional\n        Number of optimization steps to perform. Default is 500.\n\n    learn_rate : float, optional\n        Learning rate for the Adam optimizer. Default is 0.1.\n\n    plot : bool, optional\n        If True, display a plot of the NLML loss as training progresses.\n\n    verbose : bool, optional\n        If True, print progress updates at regular intervals during training.\n    \"\"\"\n\n    self.model.train()\n    self.likelihood.train()\n\n    optimizer = torch.optim.Adam(self.model.parameters(), lr=learn_rate)\n    mll = gpytorch.mlls.ExactMarginalLogLikelihood(self.likelihood, self.model)\n\n    print_every = max(1, num_iter // 20)\n\n    if plot:\n        plt.figure(figsize=(8, 5))\n\n    for i in range(num_iter):\n        optimizer.zero_grad()\n        output = self.model(self.train_times)\n        loss = -mll(output, self.train_rates)\n        loss.backward()\n\n        if verbose and (i == num_iter - 1 or i % print_every == 0):\n            if self.white_noise:\n                if self.train_errors.size(dim=0) &gt; 0:\n                    noise_param = self.model.likelihood.second_noise.item()\n                else:\n                    noise_param = self.model.likelihood.noise.item()\n\n            if self.kernel_form == 'SpectralMixture':\n                mixture_scales = self.model.covar_module.base_kernel.mixture_scales\n                mixture_scales = mixture_scales.detach().numpy().flatten()\n                mixture_weights = self.model.covar_module.base_kernel.mixture_weights\n                mixture_weights = mixture_weights.detach().numpy().flatten()\n\n                if self.white_noise:\n                    print('Iter %d/%d - loss: %.3f   mixture_lengthscales: %s   mixture_weights: %s   noise: %.1e' % (\n                        i + 1, num_iter, loss.item(),\n                        mixture_scales.round(3),\n                        mixture_weights.round(3),\n                        noise_param\n                    ))\n                else:\n                    print('Iter %d/%d - loss: %.3f   mixture_lengthscales: %s   mixture_weights: %s' % (\n                        i + 1, num_iter, loss.item(),\n                        mixture_scales.round(3),\n                        mixture_weights.round(3)\n                    ))\n\n            elif self.kernel_form == 'Periodic':\n                if self.white_noise:\n                    print('Iter %d/%d - loss: %.3f   period length: %.3f   lengthscale: %.3f   noise: %.1e' % (\n                        i + 1, num_iter, loss.item(),\n                        self.model.covar_module.base_kernel.period_length.item(),\n                        self.model.covar_module.base_kernel.lengthscale.item(),\n                        noise_param\n                    ))\n                else:\n                    print('Iter %d/%d - loss: %.3f   lengthscale: %.1e' % (\n                        i + 1, num_iter, loss.item(),\n                        self.model.covar_module.base_kernel.lengthscale.item()\n                    ))\n\n            else:\n                if self.white_noise:\n                    print('Iter %d/%d - loss: %.3f   lengthscale: %.3f   noise: %.1e' % (\n                        i + 1, num_iter, loss.item(),\n                        self.model.covar_module.base_kernel.lengthscale.item(),\n                        noise_param\n                    ))\n                else:\n                    print('Iter %d/%d - loss: %.3f   lengthscale: %.1e' % (\n                        i + 1, num_iter, loss.item(),\n                        self.model.covar_module.base_kernel.lengthscale.item()\n                    ))\n\n        optimizer.step()\n\n        if plot:\n            plt.scatter(i, loss.item(), color='black', s=2)\n\n    if verbose:\n        final_hypers = self.get_hyperparameters()\n        print(\n            \"Training complete. \\n\"\n            f\"   - Final loss: {loss.item():0.5}\\n\"\n            f\"   - Final hyperparameters:\")\n        for key, value in final_hypers.items():\n            print(f\"      {key:42}: {np.round(value, 4)}\")\n\n    if plot:\n        plt.xlabel('Iteration')\n        plt.ylabel('Negative Marginal Log Likelihood')\n        plt.title('Training Progress')\n        plt.show()\n</code></pre>"},{"location":"reference/lag_energy_spectrum/","title":"lag_energy_spectrum","text":""},{"location":"reference/lag_energy_spectrum/#stela_toolkit.lag_energy_spectrum.LagEnergySpectrum","title":"<code>LagEnergySpectrum</code>","text":"<p>Compute the time lag as a function of energy between two sets of light curves or GP models.</p> <p>This class accepts lists of LightCurve objects or trained GaussianProcess models, one per energy bin. If the inputs are GP models, the most recently generated samples will be used automatically. If no samples are found, 1000 realizations will be generated on a 1000-point grid.</p> <p>Each light curve pair (one per energy bin) is used to compute a single lag by integrating the cross-spectrum over a specified frequency range. This yields one lag per energy bin, forming the lag-energy spectrum.</p> <p>A positive lag means that the time series in <code>lcs_or_models1</code> is lagging behind the corresponding series in <code>lcs_or_models2</code>.</p> <p>Coherence values are also computed for each energy bin to assess correlation strength, and noise bias correction can be applied to the coherence before estimating uncertainties.</p> <p>Parameters:</p> Name Type Description Default <code>lcs_or_models1</code> <code>list of LightCurve or GaussianProcess</code> <p>First set of inputs, one per energy bin.</p> required <code>lcs_or_models2</code> <code>list of LightCurve or GaussianProcess</code> <p>Second set of inputs, matched to <code>lcs_or_models1</code>.</p> required <code>fmin</code> <code>float</code> <p>Minimum frequency to include when integrating.</p> required <code>fmax</code> <code>float</code> <p>Maximum frequency to include when integrating.</p> required <code>bin_edges</code> <code>array - like</code> <p>Edges of the energy bins corresponding to the light curves.</p> <code>[]</code> <code>subtract_coh_bias</code> <code>bool</code> <p>Whether to subtract the coherence noise bias before estimating lag uncertainties.</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>energies</code> <code>array - like</code> <p>Mean energy of each bin.</p> <code>energy_widths</code> <code>array - like</code> <p>Half-width of each energy bin.</p> <code>lags</code> <code>array - like</code> <p>Integrated time lag per energy bin.</p> <code>lag_errors</code> <code>array - like</code> <p>Uncertainties (1 sigma) in each lag value.</p> <code>cohs</code> <code>array - like</code> <p>Coherence values per energy bin.</p> <code>coh_errors</code> <code>array - like</code> <p>Uncertainties (1 sigma) in the coherence values.</p> Source code in <code>stela_toolkit/lag_energy_spectrum.py</code> <pre><code>class LagEnergySpectrum:\n    \"\"\"\n    Compute the time lag as a function of energy between two sets of light curves or GP models.\n\n    This class accepts lists of LightCurve objects or trained GaussianProcess models, one per energy bin.\n    If the inputs are GP models, the most recently generated samples will be used automatically.\n    If no samples are found, 1000 realizations will be generated on a 1000-point grid.\n\n    Each light curve pair (one per energy bin) is used to compute a single lag by integrating the\n    cross-spectrum over a specified frequency range. This yields one lag per energy bin, forming\n    the lag-energy spectrum.\n\n    A **positive lag** means that the time series in `lcs_or_models1` is **lagging behind**\n    the corresponding series in `lcs_or_models2`.\n\n    Coherence values are also computed for each energy bin to assess correlation strength,\n    and noise bias correction can be applied to the coherence before estimating uncertainties.\n\n    Parameters\n    ----------\n    lcs_or_models1 : list of LightCurve or GaussianProcess\n        First set of inputs, one per energy bin.\n\n    lcs_or_models2 : list of LightCurve or GaussianProcess\n        Second set of inputs, matched to `lcs_or_models1`.\n\n    fmin : float\n        Minimum frequency to include when integrating.\n\n    fmax : float\n        Maximum frequency to include when integrating.\n\n    bin_edges : array-like\n        Edges of the energy bins corresponding to the light curves.\n\n    subtract_coh_bias : bool, optional\n        Whether to subtract the coherence noise bias before estimating lag uncertainties.\n\n    Attributes\n    ----------\n    energies : array-like\n        Mean energy of each bin.\n\n    energy_widths : array-like\n        Half-width of each energy bin.\n\n    lags : array-like\n        Integrated time lag per energy bin.\n\n    lag_errors : array-like\n        Uncertainties (1 sigma) in each lag value.\n\n    cohs : array-like\n        Coherence values per energy bin.\n\n    coh_errors : array-like\n        Uncertainties (1 sigma) in the coherence values.\n    \"\"\"\n\n    def __init__(self,\n                 lcs_or_models1,\n                 lcs_or_models2,\n                 fmin,\n                 fmax,\n                 bin_edges=[],\n                 subtract_coh_bias=True):\n\n        # leave main input check to LagFrequencySpectrum, check same input dimensions for now.\n        if len(lcs_or_models1) != len(lcs_or_models2):\n            raise ValueError(\"The lightcurves_or_models arrays must contain the sane number of lightcurve/model objects.\")\n\n        self.data_models1 = lcs_or_models1\n        self.data_models2 = lcs_or_models2\n\n        self.energies = [np.mean(bin_edges[i], bin_edges[i+1]) for i in range(len(bin_edges[:-1]))]\n        self.energy_widths = np.diff(bin_edges) / 2\n\n        self.fmin, self.fmax = fmin, fmax\n        lag_spectrum = self.compute_lag_spectrum(subtract_coh_bias=subtract_coh_bias)\n        self.lags, self.lag_errors, self.cohs, self. coh_errors = lag_spectrum\n\n    def compute_lag_spectrum(self, subtract_coh_bias):\n        \"\"\"\n        Compute the lag and coherence for each energy bin.\n\n        Parameters\n        ----------\n        subtract_coh_bias : bool\n            Whether to subtract Poisson noise bias from the coherence.\n\n        Returns\n        -------\n        lags : list\n            List of integrated lags for each bin.\n\n        lag_errors : list\n            List of lag uncertainties.\n\n        cohs : list\n            List of mean coherence values.\n\n        coh_errors : list\n            List of coherence uncertainties.\n        \"\"\"\n\n        lags, lag_errors, cohs, coh_errors = [], [], [], []\n        for i in range(len(self.data_models1)):\n            lfs = LagFrequencySpectrum(self.data_models1[i],\n                                       self.data_models2[i],\n                                       fmin=self.fmin,\n                                       fmax=self.fmax,\n                                       num_bins=1,\n                                       subtract_coh_bias=subtract_coh_bias,\n                                       )\n            lags.append(lfs.lags)\n            lag_errors.append(lfs.lag_errors)\n            cohs.append(lfs.cohs)\n            coh_errors.append(lfs.coh_errors)\n\n        return lags, lag_errors, cohs, coh_errors\n\n    def plot(self, energies=None, energy_widths=None, lags=None, lag_errors=None, cohs=None, coh_errors=None, **kwargs):\n        \"\"\"\n        Plot the lag-energy spectrum and associated coherence values.\n\n        Parameters\n        ----------\n        **kwargs : dict\n            Keyword arguments for customizing the plot (e.g., xlabel, xscale, yscale).\n        \"\"\"\n        energies = self.energies if energies is None else energies\n        energy_widths = self.energy_widths if energy_widths is None else energy_widths\n        lags = self.lags if lags is None else lags\n        lag_errors = self.lag_errors if lag_errors is None else lag_errors\n        cohs = self.cohs if cohs is None else cohs\n        coh_errors = self.coh_errors if coh_errors is None else coh_errors\n\n        figsize = kwargs.get('figsize', (8, 6))\n        xlabel = kwargs.get('xlabel', 'Energy')\n        ylabel = kwargs.get('ylabel', 'Time Lag')\n        xscale = kwargs.get('xscale', 'log')\n        yscale = kwargs.get('yscale', 'linear')\n\n        fig, (ax1, ax2) = plt.subplots(2, 1, gridspec_kw={'height_ratios': [2, 1]}, figsize=figsize, sharex=True)\n        plt.subplots_adjust(hspace=0.05)\n\n        # Lag-energy spectrum\n        ax1.errorbar(energies, lags, xerr=energy_widths, yerr=lag_errors, fmt='o', color='black', ms=3, lw=1.5)\n        ax1.set_xscale(xscale)\n        ax1.set_yscale(yscale)\n        ax1.set_ylabel(ylabel, fontsize=12)\n        ax1.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n        ax1.tick_params(which='both', direction='in', length=6, width=1, top=True, right=True, labelsize=12)\n\n        # Coherence spectrum\n        if cohs is not None and coh_errors is not None:\n            ax2.errorbar(energies, cohs, xerr=energy_widths, yerr=coh_errors, fmt='o', color='black', ms=3, lw=1.5)\n            ax2.set_xscale(xscale)\n            ax2.set_ylabel('Coherence', fontsize=12)\n            ax2.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n            ax2.tick_params(which='both', direction='in', length=6, width=1, top=True, right=True, labelsize=12)\n\n        fig.text(0.5, 0.04, xlabel, ha='center', va='center', fontsize=12)\n        plt.tight_layout()\n        plt.show()\n\n    def count_frequencies_in_bins(self, fmin=None, fmax=None, num_bins=None, bin_type=None, bin_edges=[]):\n        \"\"\"\n        Counts the number of frequencies in each frequency bin.\n        Wrapper method to use FrequencyBinning.count_frequencies_in_bins with class attributes.\n        \"\"\"\n\n        return FrequencyBinning.count_frequencies_in_bins(\n            self, fmin=fmin, fmax=fmax, num_bins=num_bins, bin_type=bin_type, bin_edges=bin_edges\n        )\n</code></pre>"},{"location":"reference/lag_energy_spectrum/#stela_toolkit.lag_energy_spectrum.LagEnergySpectrum.compute_lag_spectrum","title":"<code>compute_lag_spectrum(subtract_coh_bias)</code>","text":"<p>Compute the lag and coherence for each energy bin.</p> <p>Parameters:</p> Name Type Description Default <code>subtract_coh_bias</code> <code>bool</code> <p>Whether to subtract Poisson noise bias from the coherence.</p> required <p>Returns:</p> Name Type Description <code>lags</code> <code>list</code> <p>List of integrated lags for each bin.</p> <code>lag_errors</code> <code>list</code> <p>List of lag uncertainties.</p> <code>cohs</code> <code>list</code> <p>List of mean coherence values.</p> <code>coh_errors</code> <code>list</code> <p>List of coherence uncertainties.</p> Source code in <code>stela_toolkit/lag_energy_spectrum.py</code> <pre><code>def compute_lag_spectrum(self, subtract_coh_bias):\n    \"\"\"\n    Compute the lag and coherence for each energy bin.\n\n    Parameters\n    ----------\n    subtract_coh_bias : bool\n        Whether to subtract Poisson noise bias from the coherence.\n\n    Returns\n    -------\n    lags : list\n        List of integrated lags for each bin.\n\n    lag_errors : list\n        List of lag uncertainties.\n\n    cohs : list\n        List of mean coherence values.\n\n    coh_errors : list\n        List of coherence uncertainties.\n    \"\"\"\n\n    lags, lag_errors, cohs, coh_errors = [], [], [], []\n    for i in range(len(self.data_models1)):\n        lfs = LagFrequencySpectrum(self.data_models1[i],\n                                   self.data_models2[i],\n                                   fmin=self.fmin,\n                                   fmax=self.fmax,\n                                   num_bins=1,\n                                   subtract_coh_bias=subtract_coh_bias,\n                                   )\n        lags.append(lfs.lags)\n        lag_errors.append(lfs.lag_errors)\n        cohs.append(lfs.cohs)\n        coh_errors.append(lfs.coh_errors)\n\n    return lags, lag_errors, cohs, coh_errors\n</code></pre>"},{"location":"reference/lag_energy_spectrum/#stela_toolkit.lag_energy_spectrum.LagEnergySpectrum.count_frequencies_in_bins","title":"<code>count_frequencies_in_bins(fmin=None, fmax=None, num_bins=None, bin_type=None, bin_edges=[])</code>","text":"<p>Counts the number of frequencies in each frequency bin. Wrapper method to use FrequencyBinning.count_frequencies_in_bins with class attributes.</p> Source code in <code>stela_toolkit/lag_energy_spectrum.py</code> <pre><code>def count_frequencies_in_bins(self, fmin=None, fmax=None, num_bins=None, bin_type=None, bin_edges=[]):\n    \"\"\"\n    Counts the number of frequencies in each frequency bin.\n    Wrapper method to use FrequencyBinning.count_frequencies_in_bins with class attributes.\n    \"\"\"\n\n    return FrequencyBinning.count_frequencies_in_bins(\n        self, fmin=fmin, fmax=fmax, num_bins=num_bins, bin_type=bin_type, bin_edges=bin_edges\n    )\n</code></pre>"},{"location":"reference/lag_energy_spectrum/#stela_toolkit.lag_energy_spectrum.LagEnergySpectrum.plot","title":"<code>plot(energies=None, energy_widths=None, lags=None, lag_errors=None, cohs=None, coh_errors=None, **kwargs)</code>","text":"<p>Plot the lag-energy spectrum and associated coherence values.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>dict</code> <p>Keyword arguments for customizing the plot (e.g., xlabel, xscale, yscale).</p> <code>{}</code> Source code in <code>stela_toolkit/lag_energy_spectrum.py</code> <pre><code>def plot(self, energies=None, energy_widths=None, lags=None, lag_errors=None, cohs=None, coh_errors=None, **kwargs):\n    \"\"\"\n    Plot the lag-energy spectrum and associated coherence values.\n\n    Parameters\n    ----------\n    **kwargs : dict\n        Keyword arguments for customizing the plot (e.g., xlabel, xscale, yscale).\n    \"\"\"\n    energies = self.energies if energies is None else energies\n    energy_widths = self.energy_widths if energy_widths is None else energy_widths\n    lags = self.lags if lags is None else lags\n    lag_errors = self.lag_errors if lag_errors is None else lag_errors\n    cohs = self.cohs if cohs is None else cohs\n    coh_errors = self.coh_errors if coh_errors is None else coh_errors\n\n    figsize = kwargs.get('figsize', (8, 6))\n    xlabel = kwargs.get('xlabel', 'Energy')\n    ylabel = kwargs.get('ylabel', 'Time Lag')\n    xscale = kwargs.get('xscale', 'log')\n    yscale = kwargs.get('yscale', 'linear')\n\n    fig, (ax1, ax2) = plt.subplots(2, 1, gridspec_kw={'height_ratios': [2, 1]}, figsize=figsize, sharex=True)\n    plt.subplots_adjust(hspace=0.05)\n\n    # Lag-energy spectrum\n    ax1.errorbar(energies, lags, xerr=energy_widths, yerr=lag_errors, fmt='o', color='black', ms=3, lw=1.5)\n    ax1.set_xscale(xscale)\n    ax1.set_yscale(yscale)\n    ax1.set_ylabel(ylabel, fontsize=12)\n    ax1.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n    ax1.tick_params(which='both', direction='in', length=6, width=1, top=True, right=True, labelsize=12)\n\n    # Coherence spectrum\n    if cohs is not None and coh_errors is not None:\n        ax2.errorbar(energies, cohs, xerr=energy_widths, yerr=coh_errors, fmt='o', color='black', ms=3, lw=1.5)\n        ax2.set_xscale(xscale)\n        ax2.set_ylabel('Coherence', fontsize=12)\n        ax2.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n        ax2.tick_params(which='both', direction='in', length=6, width=1, top=True, right=True, labelsize=12)\n\n    fig.text(0.5, 0.04, xlabel, ha='center', va='center', fontsize=12)\n    plt.tight_layout()\n    plt.show()\n</code></pre>"},{"location":"reference/lag_frequency_spectrum/","title":"lag_frequency_spectrum","text":""},{"location":"reference/lag_frequency_spectrum/#stela_toolkit.lag_frequency_spectrum.LagFrequencySpectrum","title":"<code>LagFrequencySpectrum</code>","text":"<p>Compute the time lag as a function of frequency between two time series.</p> <p>This class accepts either LightCurve objects (with regular sampling) or trained GaussianProcess models from STELA. If GP models are provided, the most recently generated samples are used. If no samples have been created yet, the toolkit will automatically generate 1000 samples on a 1000-point grid.</p> <p>The sign convention is such that a positive lag indicates that the input provided as <code>lc_or_model1</code> is lagging behind the input provided as <code>lc_or_model2</code>.</p> <p>There are two modes for computing uncertainties:</p> <ul> <li> <p>If the inputs are individual light curves, lag uncertainties are propagated from   the coherence spectrum using a theoretical error model.</p> </li> <li> <p>If the inputs are GP models, the lag spectrum is computed for each sample and   uncertainties are reported as the standard deviation across all samples.</p> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>lc_or_model1</code> <code>LightCurve or GaussianProcess</code> <p>First input time series or trained model.</p> required <code>lc_or_model2</code> <code>LightCurve or GaussianProcess</code> <p>Second input time series or trained model.</p> required <code>fmin</code> <code>float or auto</code> <p>Minimum frequency to include. If 'auto', uses the lowest nonzero FFT frequency.</p> <code>'auto'</code> <code>fmax</code> <code>float or auto</code> <p>Maximum frequency to include. If 'auto', uses the Nyquist frequency.</p> <code>'auto'</code> <code>num_bins</code> <code>int</code> <p>Number of frequency bins.</p> <code>None</code> <code>bin_type</code> <code>str</code> <p>Type of binning: 'log' or 'linear'.</p> <code>'log'</code> <code>bin_edges</code> <code>array - like</code> <p>Custom bin edges (overrides <code>num_bins</code> and <code>bin_type</code>).</p> <code>[]</code> <code>subtract_coh_bias</code> <code>bool</code> <p>Whether to subtract Poisson noise bias from the coherence.</p> <code>True</code> <code>plot_lfs</code> <code>bool</code> <p>Whether to generate the lag-frequency plot on initialization.</p> required <p>Attributes:</p> Name Type Description <code>freqs</code> <code>array - like</code> <p>Frequency bin centers.</p> <code>freq_widths</code> <code>array - like</code> <p>Bin widths for each frequency bin.</p> <code>lags</code> <code>array - like</code> <p>Computed time lags.</p> <code>lag_errors</code> <code>array - like</code> <p>Uncertainties in the lag estimates.</p> <code>cohs</code> <code>array - like</code> <p>Coherence values for each frequency bin.</p> <code>coh_errors</code> <code>array - like</code> <p>Uncertainties in the coherence values.</p> Source code in <code>stela_toolkit/lag_frequency_spectrum.py</code> <pre><code>class LagFrequencySpectrum:\n    \"\"\"\n    Compute the time lag as a function of frequency between two time series.\n\n    This class accepts either LightCurve objects (with regular sampling) or trained\n    GaussianProcess models from STELA. If GP models are provided, the most\n    recently generated samples are used. If no samples have been created yet,\n    the toolkit will automatically generate 1000 samples on a 1000-point grid.\n\n    The sign convention is such that a **positive lag** indicates that the input provided as\n    `lc_or_model1` is **lagging behind** the input provided as `lc_or_model2`.\n\n    There are two modes for computing uncertainties:\n\n    - If the inputs are individual light curves, lag uncertainties are propagated from\n      the coherence spectrum using a theoretical error model.\n\n    - If the inputs are GP models, the lag spectrum is computed for each sample and\n      uncertainties are reported as the standard deviation across all samples.\n\n    Parameters\n    ----------\n    lc_or_model1 : LightCurve or GaussianProcess\n        First input time series or trained model.\n\n    lc_or_model2 : LightCurve or GaussianProcess\n        Second input time series or trained model.\n\n    fmin : float or 'auto', optional\n        Minimum frequency to include. If 'auto', uses the lowest nonzero FFT frequency.\n\n    fmax : float or 'auto', optional\n        Maximum frequency to include. If 'auto', uses the Nyquist frequency.\n\n    num_bins : int, optional\n        Number of frequency bins.\n\n    bin_type : str, optional\n        Type of binning: 'log' or 'linear'.\n\n    bin_edges : array-like, optional\n        Custom bin edges (overrides `num_bins` and `bin_type`).\n\n    subtract_coh_bias : bool, optional\n        Whether to subtract Poisson noise bias from the coherence.\n\n    plot_lfs : bool, optional\n        Whether to generate the lag-frequency plot on initialization.\n\n    Attributes\n    ----------\n    freqs : array-like\n        Frequency bin centers.\n\n    freq_widths : array-like\n        Bin widths for each frequency bin.\n\n    lags : array-like\n        Computed time lags.\n\n    lag_errors : array-like\n        Uncertainties in the lag estimates.\n\n    cohs : array-like\n        Coherence values for each frequency bin.\n\n    coh_errors : array-like\n        Uncertainties in the coherence values.\n    \"\"\"\n\n    def __init__(self,\n                 lc_or_model1,\n                 lc_or_model2,\n                 fmin='auto',\n                 fmax='auto',\n                 num_bins=None,\n                 bin_type=\"log\",\n                 bin_edges=[],\n                 subtract_coh_bias=True):\n\n        input_data = _CheckInputs._check_lightcurve_or_model(lc_or_model1)\n        if input_data['type'] == 'model':\n            self.times1, self.rates1 = input_data['data']\n        else:\n            self.times1, self.rates1, _ = input_data['data']\n\n        input_data = _CheckInputs._check_lightcurve_or_model(lc_or_model2)\n        if input_data['type'] == 'model':\n            self.times2, self.rates2 = input_data['data']\n        else:\n            self.times2, self.rates2, _ = input_data['data']\n\n        _CheckInputs._check_input_bins(num_bins, bin_type, bin_edges)\n\n        if not np.allclose(self.times1, self.times2):\n            raise ValueError(\"The time arrays of the two light curves must be identical.\")\n\n        # Use absolute min and max frequencies if set to 'auto'\n        self.dt = np.diff(self.times1)[0]\n        self.fmin = np.fft.rfftfreq(len(self.rates1), d=self.dt)[1] if fmin == 'auto' else fmin\n        self.fmax = np.fft.rfftfreq(len(self.rates1), d=self.dt)[-1] if fmax == 'auto' else fmax  # nyquist frequency\n\n        self.num_bins = num_bins\n        self.bin_type = bin_type\n        self.bin_edges = bin_edges\n\n        if len(self.rates1.shape) == 2 and len(self.rates2.shape) == 2:\n            lag_spectrum = self.compute_stacked_lag_spectrum()\n        else:\n            lag_spectrum = self.compute_lag_spectrum(subtract_coh_bias=subtract_coh_bias)\n\n        self.freqs, self.freq_widths, self.lags, self.lag_errors, self.cohs, self.coh_errors = lag_spectrum\n\n    def compute_lag_spectrum(self, \n                             times1=None, rates1=None,\n                             times2=None, rates2=None,\n                             subtract_coh_bias=True):\n        \"\"\"\n        Compute the lag spectrum for a single pair of light curves or model realizations.\n\n        The phase of the cross-spectrum is converted to time lags, and uncertainties are\n        computed either from coherence (for raw light curves) or from GP sampling (if using\n        stacked realizations).\n\n        Parameters\n        ----------\n        times1 : array-like, optional\n            Time values for the first time series.\n\n        rates1 : array-like, optional\n            Rate or flux values for the first time series.\n\n        times2 : array-like, optional\n            Time values for the second time series.\n\n        rates2 : array-like, optional\n            Rate or flux values for the second time series.\n\n        subtract_coh_bias : bool, optional\n            Whether to subtract noise bias from the coherence estimate.\n\n        Returns\n        -------\n        freqs : array-like\n            Center of each frequency bin.\n\n        freq_widths : array-like\n            Width of each frequency bin.\n\n        lags : array-like\n            Time lag values at each frequency.\n\n        lag_errors : array-like\n            Uncertainties on the lag values.\n\n        cohs : array-like\n            Coherence values at each frequency.\n\n        coh_errors : array-like\n            Uncertainties on the coherence values.\n        \"\"\"\n\n        times1 = times1 if times1 is not None else self.times1\n        times2 = times2 if times2 is not None else self.times2\n        rates1 = rates1 if rates1 is not None else self.rates1\n        rates2 = rates2 if rates2 is not None else self.rates2 \n\n        lc1 = LightCurve(times=times1, rates=rates1)\n        lc2 = LightCurve(times=times2, rates=rates2)\n\n        # Compute the cross spectrum\n        cross_spectrum = CrossSpectrum(lc1, lc2,\n                                       fmin=self.fmin, fmax=self.fmax,\n                                       num_bins=self.num_bins, bin_type=self.bin_type,\n                                       bin_edges=self.bin_edges,\n                                       norm=False\n                                    )\n\n        lags = np.angle(cross_spectrum.cs) / (2 * np.pi * cross_spectrum.freqs)\n\n        coherence = Coherence(lc1, lc2,\n                              fmin=self.fmin, fmax=self.fmax,\n                              num_bins=self.num_bins, bin_type=self.bin_type, bin_edges=self.bin_edges,\n                              subtract_noise_bias=subtract_coh_bias\n                            )    \n        cohs = coherence.cohs\n        coh_errors = coherence.coh_errors\n\n        num_freq = self.count_frequencies_in_bins()\n\n        phase_errors = _ClearWarnings.run(\n            lambda: np.sqrt((1 - coherence.cohs) / (2 * coherence.cohs * num_freq)),\n            explanation=\"Error from sqrt when computing (unbinned) phase errors here is common \"\n                        \"and typically due to &gt;1 coherence at the minimum frequency.\"\n        )\n\n        lag_errors = phase_errors / (2 * np.pi * cross_spectrum.freqs)\n\n        return cross_spectrum.freqs, cross_spectrum.freq_widths, lags, lag_errors, cohs, coh_errors\n\n    def compute_stacked_lag_spectrum(self):\n        \"\"\"\n        Compute lag-frequency spectrum for stacked GP samples.\n\n        This method assumes the input light curves are model-generated and include\n        multiple realizations. Returns mean and standard deviation of lag and coherence.\n\n        Returns\n        -------\n        freqs : array-like\n            Frequency bin centers.\n\n        freq_widths : array-like\n            Frequency bin widths.\n\n        lags : array-like\n            Mean time lags across samples.\n\n        lag_errors : array-like\n            Standard deviation of lags.\n\n        cohs : array-like\n            Mean coherence values.\n\n        coh_errors : array-like\n            Standard deviation of coherence values.\n        \"\"\"\n\n        # Compute lag spectrum for each pair of realizations\n        lag_spectra = []\n        coh_spectra = []\n        for i in range(self.rates1.shape[0]):\n            lag_spectrum = self.compute_lag_spectrum(times1=self.times1, rates1=self.rates1[i],\n                                                     times2=self.times2, rates2=self.rates2[i],\n                                                     subtract_coh_bias=False\n                                                    )\n            lag_spectra.append(lag_spectrum[2])\n            coh_spectra.append(lag_spectrum[4])\n\n        # Average lag spectra\n        lag_spectra_mean = np.mean(lag_spectra, axis=0)\n        lag_spectra_std = np.std(lag_spectra, axis=0)\n\n        # Average coherence spectra\n        coh_spectra_mean = np.mean(coh_spectra, axis=0)\n        coh_spectra_std = np.std(coh_spectra, axis=0)\n\n        freqs, freq_widths = lag_spectrum[0], lag_spectrum[1]\n\n        return freqs, freq_widths, lag_spectra_mean, lag_spectra_std, coh_spectra_mean, coh_spectra_std\n\n    def plot(self, **kwargs):\n        \"\"\"\n        Plot the lag-frequency and coherence spectrum.\n        Plot appearance can be customized using keyword arguments:\n\n        - figsize : tuple, optional\n            Size of the figure (default: (8, 6)).\n        - xlabel : str, optional\n            Label for the x-axis (default: \"Frequency\").\n        - ylabel : str, optional\n            Label for the y-axis of the lag panel (default: \"Time Lag\").\n        - xscale : str, optional\n            Scale for the x-axis (\"log\" or \"linear\", default: \"log\").\n        - yscale : str, optional\n            Scale for the y-axis of the lag panel (\"linear\" or \"log\", default: \"linear\").\n        \"\"\"\n        figsize = kwargs.get('figsize', (8, 6))\n        xlabel = kwargs.get('xlabel', 'Frequency')\n        ylabel = kwargs.get('ylabel', 'Time Lag')\n        xscale = kwargs.get('xscale', 'log')\n        yscale = kwargs.get('yscale', 'linear')\n\n        fig, (ax1, ax2) = plt.subplots(2, 1, gridspec_kw={'height_ratios': [2, 1]}, figsize=figsize, sharex=True)\n        plt.subplots_adjust(hspace=0.05)\n\n        # Lag-frequency spectrum\n        ax1.errorbar(self.freqs, self.lags, xerr=self.freq_widths, yerr=self.lag_errors,\n                    fmt='o', color='black', ms=3, lw=1.5)\n        ax1.set_xscale(xscale)\n        ax1.set_yscale(yscale)\n        ax1.set_ylabel(ylabel, fontsize=12)\n        ax1.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n        ax1.tick_params(which='both', direction='in', length=6, width=1, top=True, right=True, labelsize=12)\n\n        # Coherence spectrum\n        if self.cohs is not None and self.coh_errors is not None:\n            ax2.errorbar(self.freqs, self.cohs, xerr=self.freq_widths, yerr=self.coh_errors,\n                        fmt='o', color='black', ms=3, lw=1.5)\n            ax2.set_xscale(xscale)\n            ax2.set_ylabel('Coherence', fontsize=12)\n            ax2.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n            ax2.tick_params(which='both', direction='in', length=6, width=1, top=True, right=True, labelsize=12)\n\n        fig.text(0.5, 0.04, xlabel, ha='center', va='center', fontsize=12)\n        plt.show()\n\n    def count_frequencies_in_bins(self, fmin=None, fmax=None, num_bins=None, bin_type=None, bin_edges=[]):\n        \"\"\"\n        Counts the number of frequencies in each frequency bin.\n        Wrapper method to use FrequencyBinning.count_frequencies_in_bins with class attributes.\n        \"\"\"\n\n        return FrequencyBinning.count_frequencies_in_bins(\n            self, fmin=fmin, fmax=fmax, num_bins=num_bins, bin_type=bin_type, bin_edges=bin_edges\n        )\n</code></pre>"},{"location":"reference/lag_frequency_spectrum/#stela_toolkit.lag_frequency_spectrum.LagFrequencySpectrum.compute_lag_spectrum","title":"<code>compute_lag_spectrum(times1=None, rates1=None, times2=None, rates2=None, subtract_coh_bias=True)</code>","text":"<p>Compute the lag spectrum for a single pair of light curves or model realizations.</p> <p>The phase of the cross-spectrum is converted to time lags, and uncertainties are computed either from coherence (for raw light curves) or from GP sampling (if using stacked realizations).</p> <p>Parameters:</p> Name Type Description Default <code>times1</code> <code>array - like</code> <p>Time values for the first time series.</p> <code>None</code> <code>rates1</code> <code>array - like</code> <p>Rate or flux values for the first time series.</p> <code>None</code> <code>times2</code> <code>array - like</code> <p>Time values for the second time series.</p> <code>None</code> <code>rates2</code> <code>array - like</code> <p>Rate or flux values for the second time series.</p> <code>None</code> <code>subtract_coh_bias</code> <code>bool</code> <p>Whether to subtract noise bias from the coherence estimate.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>freqs</code> <code>array - like</code> <p>Center of each frequency bin.</p> <code>freq_widths</code> <code>array - like</code> <p>Width of each frequency bin.</p> <code>lags</code> <code>array - like</code> <p>Time lag values at each frequency.</p> <code>lag_errors</code> <code>array - like</code> <p>Uncertainties on the lag values.</p> <code>cohs</code> <code>array - like</code> <p>Coherence values at each frequency.</p> <code>coh_errors</code> <code>array - like</code> <p>Uncertainties on the coherence values.</p> Source code in <code>stela_toolkit/lag_frequency_spectrum.py</code> <pre><code>def compute_lag_spectrum(self, \n                         times1=None, rates1=None,\n                         times2=None, rates2=None,\n                         subtract_coh_bias=True):\n    \"\"\"\n    Compute the lag spectrum for a single pair of light curves or model realizations.\n\n    The phase of the cross-spectrum is converted to time lags, and uncertainties are\n    computed either from coherence (for raw light curves) or from GP sampling (if using\n    stacked realizations).\n\n    Parameters\n    ----------\n    times1 : array-like, optional\n        Time values for the first time series.\n\n    rates1 : array-like, optional\n        Rate or flux values for the first time series.\n\n    times2 : array-like, optional\n        Time values for the second time series.\n\n    rates2 : array-like, optional\n        Rate or flux values for the second time series.\n\n    subtract_coh_bias : bool, optional\n        Whether to subtract noise bias from the coherence estimate.\n\n    Returns\n    -------\n    freqs : array-like\n        Center of each frequency bin.\n\n    freq_widths : array-like\n        Width of each frequency bin.\n\n    lags : array-like\n        Time lag values at each frequency.\n\n    lag_errors : array-like\n        Uncertainties on the lag values.\n\n    cohs : array-like\n        Coherence values at each frequency.\n\n    coh_errors : array-like\n        Uncertainties on the coherence values.\n    \"\"\"\n\n    times1 = times1 if times1 is not None else self.times1\n    times2 = times2 if times2 is not None else self.times2\n    rates1 = rates1 if rates1 is not None else self.rates1\n    rates2 = rates2 if rates2 is not None else self.rates2 \n\n    lc1 = LightCurve(times=times1, rates=rates1)\n    lc2 = LightCurve(times=times2, rates=rates2)\n\n    # Compute the cross spectrum\n    cross_spectrum = CrossSpectrum(lc1, lc2,\n                                   fmin=self.fmin, fmax=self.fmax,\n                                   num_bins=self.num_bins, bin_type=self.bin_type,\n                                   bin_edges=self.bin_edges,\n                                   norm=False\n                                )\n\n    lags = np.angle(cross_spectrum.cs) / (2 * np.pi * cross_spectrum.freqs)\n\n    coherence = Coherence(lc1, lc2,\n                          fmin=self.fmin, fmax=self.fmax,\n                          num_bins=self.num_bins, bin_type=self.bin_type, bin_edges=self.bin_edges,\n                          subtract_noise_bias=subtract_coh_bias\n                        )    \n    cohs = coherence.cohs\n    coh_errors = coherence.coh_errors\n\n    num_freq = self.count_frequencies_in_bins()\n\n    phase_errors = _ClearWarnings.run(\n        lambda: np.sqrt((1 - coherence.cohs) / (2 * coherence.cohs * num_freq)),\n        explanation=\"Error from sqrt when computing (unbinned) phase errors here is common \"\n                    \"and typically due to &gt;1 coherence at the minimum frequency.\"\n    )\n\n    lag_errors = phase_errors / (2 * np.pi * cross_spectrum.freqs)\n\n    return cross_spectrum.freqs, cross_spectrum.freq_widths, lags, lag_errors, cohs, coh_errors\n</code></pre>"},{"location":"reference/lag_frequency_spectrum/#stela_toolkit.lag_frequency_spectrum.LagFrequencySpectrum.compute_stacked_lag_spectrum","title":"<code>compute_stacked_lag_spectrum()</code>","text":"<p>Compute lag-frequency spectrum for stacked GP samples.</p> <p>This method assumes the input light curves are model-generated and include multiple realizations. Returns mean and standard deviation of lag and coherence.</p> <p>Returns:</p> Name Type Description <code>freqs</code> <code>array - like</code> <p>Frequency bin centers.</p> <code>freq_widths</code> <code>array - like</code> <p>Frequency bin widths.</p> <code>lags</code> <code>array - like</code> <p>Mean time lags across samples.</p> <code>lag_errors</code> <code>array - like</code> <p>Standard deviation of lags.</p> <code>cohs</code> <code>array - like</code> <p>Mean coherence values.</p> <code>coh_errors</code> <code>array - like</code> <p>Standard deviation of coherence values.</p> Source code in <code>stela_toolkit/lag_frequency_spectrum.py</code> <pre><code>def compute_stacked_lag_spectrum(self):\n    \"\"\"\n    Compute lag-frequency spectrum for stacked GP samples.\n\n    This method assumes the input light curves are model-generated and include\n    multiple realizations. Returns mean and standard deviation of lag and coherence.\n\n    Returns\n    -------\n    freqs : array-like\n        Frequency bin centers.\n\n    freq_widths : array-like\n        Frequency bin widths.\n\n    lags : array-like\n        Mean time lags across samples.\n\n    lag_errors : array-like\n        Standard deviation of lags.\n\n    cohs : array-like\n        Mean coherence values.\n\n    coh_errors : array-like\n        Standard deviation of coherence values.\n    \"\"\"\n\n    # Compute lag spectrum for each pair of realizations\n    lag_spectra = []\n    coh_spectra = []\n    for i in range(self.rates1.shape[0]):\n        lag_spectrum = self.compute_lag_spectrum(times1=self.times1, rates1=self.rates1[i],\n                                                 times2=self.times2, rates2=self.rates2[i],\n                                                 subtract_coh_bias=False\n                                                )\n        lag_spectra.append(lag_spectrum[2])\n        coh_spectra.append(lag_spectrum[4])\n\n    # Average lag spectra\n    lag_spectra_mean = np.mean(lag_spectra, axis=0)\n    lag_spectra_std = np.std(lag_spectra, axis=0)\n\n    # Average coherence spectra\n    coh_spectra_mean = np.mean(coh_spectra, axis=0)\n    coh_spectra_std = np.std(coh_spectra, axis=0)\n\n    freqs, freq_widths = lag_spectrum[0], lag_spectrum[1]\n\n    return freqs, freq_widths, lag_spectra_mean, lag_spectra_std, coh_spectra_mean, coh_spectra_std\n</code></pre>"},{"location":"reference/lag_frequency_spectrum/#stela_toolkit.lag_frequency_spectrum.LagFrequencySpectrum.count_frequencies_in_bins","title":"<code>count_frequencies_in_bins(fmin=None, fmax=None, num_bins=None, bin_type=None, bin_edges=[])</code>","text":"<p>Counts the number of frequencies in each frequency bin. Wrapper method to use FrequencyBinning.count_frequencies_in_bins with class attributes.</p> Source code in <code>stela_toolkit/lag_frequency_spectrum.py</code> <pre><code>def count_frequencies_in_bins(self, fmin=None, fmax=None, num_bins=None, bin_type=None, bin_edges=[]):\n    \"\"\"\n    Counts the number of frequencies in each frequency bin.\n    Wrapper method to use FrequencyBinning.count_frequencies_in_bins with class attributes.\n    \"\"\"\n\n    return FrequencyBinning.count_frequencies_in_bins(\n        self, fmin=fmin, fmax=fmax, num_bins=num_bins, bin_type=bin_type, bin_edges=bin_edges\n    )\n</code></pre>"},{"location":"reference/lag_frequency_spectrum/#stela_toolkit.lag_frequency_spectrum.LagFrequencySpectrum.plot","title":"<code>plot(**kwargs)</code>","text":"<p>Plot the lag-frequency and coherence spectrum. Plot appearance can be customized using keyword arguments:</p> <ul> <li>figsize : tuple, optional     Size of the figure (default: (8, 6)).</li> <li>xlabel : str, optional     Label for the x-axis (default: \"Frequency\").</li> <li>ylabel : str, optional     Label for the y-axis of the lag panel (default: \"Time Lag\").</li> <li>xscale : str, optional     Scale for the x-axis (\"log\" or \"linear\", default: \"log\").</li> <li>yscale : str, optional     Scale for the y-axis of the lag panel (\"linear\" or \"log\", default: \"linear\").</li> </ul> Source code in <code>stela_toolkit/lag_frequency_spectrum.py</code> <pre><code>def plot(self, **kwargs):\n    \"\"\"\n    Plot the lag-frequency and coherence spectrum.\n    Plot appearance can be customized using keyword arguments:\n\n    - figsize : tuple, optional\n        Size of the figure (default: (8, 6)).\n    - xlabel : str, optional\n        Label for the x-axis (default: \"Frequency\").\n    - ylabel : str, optional\n        Label for the y-axis of the lag panel (default: \"Time Lag\").\n    - xscale : str, optional\n        Scale for the x-axis (\"log\" or \"linear\", default: \"log\").\n    - yscale : str, optional\n        Scale for the y-axis of the lag panel (\"linear\" or \"log\", default: \"linear\").\n    \"\"\"\n    figsize = kwargs.get('figsize', (8, 6))\n    xlabel = kwargs.get('xlabel', 'Frequency')\n    ylabel = kwargs.get('ylabel', 'Time Lag')\n    xscale = kwargs.get('xscale', 'log')\n    yscale = kwargs.get('yscale', 'linear')\n\n    fig, (ax1, ax2) = plt.subplots(2, 1, gridspec_kw={'height_ratios': [2, 1]}, figsize=figsize, sharex=True)\n    plt.subplots_adjust(hspace=0.05)\n\n    # Lag-frequency spectrum\n    ax1.errorbar(self.freqs, self.lags, xerr=self.freq_widths, yerr=self.lag_errors,\n                fmt='o', color='black', ms=3, lw=1.5)\n    ax1.set_xscale(xscale)\n    ax1.set_yscale(yscale)\n    ax1.set_ylabel(ylabel, fontsize=12)\n    ax1.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n    ax1.tick_params(which='both', direction='in', length=6, width=1, top=True, right=True, labelsize=12)\n\n    # Coherence spectrum\n    if self.cohs is not None and self.coh_errors is not None:\n        ax2.errorbar(self.freqs, self.cohs, xerr=self.freq_widths, yerr=self.coh_errors,\n                    fmt='o', color='black', ms=3, lw=1.5)\n        ax2.set_xscale(xscale)\n        ax2.set_ylabel('Coherence', fontsize=12)\n        ax2.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n        ax2.tick_params(which='both', direction='in', length=6, width=1, top=True, right=True, labelsize=12)\n\n    fig.text(0.5, 0.04, xlabel, ha='center', va='center', fontsize=12)\n    plt.show()\n</code></pre>"},{"location":"reference/plot/","title":"plot","text":""},{"location":"reference/plot/#stela_toolkit.plot.Plotter","title":"<code>Plotter</code>","text":"<p>Flexible wrapper around matplotlib for plotting binned or unbinned spectral results. Handles default formatting, error bars, labels, and saving.</p> Source code in <code>stela_toolkit/plot.py</code> <pre><code>class Plotter:\n    \"\"\"\n    Flexible wrapper around matplotlib for plotting binned or unbinned spectral results.\n    Handles default formatting, error bars, labels, and saving.\n    \"\"\"\n\n    @staticmethod\n    def plot(x=None, y=None, xerr=None, yerr=None, **kwargs):\n        \"\"\"\n        Generalized plotting method for spectrum-like data.\n\n        Parameters:\n        ----------\n        x : array-like\n            x-axis values (e.g., frequencies).\n\n        y : array-like\n            y-axis values (e.g., power or cross-power values).\n\n        xerr : array-like, optional\n            Uncertainties in the x-axis values (e.g., frequency widths).\n\n        yerr : array-like, optional\n            Uncertainties in the y-axis values (e.g., power uncertainties).\n\n        **kwargs: Additional keyword arguments for customization.\n        \"\"\"\n\n        if xerr is not None:\n            xerr = xerr if len(list(xerr)) &gt; 0 else None\n        if yerr is not None:\n            yerr = yerr if len(list(yerr)) &gt; 0 else None\n\n        if x is None or y is None:\n            raise ValueError(\"Both 'x' and 'y' must be provided.\")\n\n        title = kwargs.get('title', None)\n\n        # Default plotting settings\n        if yerr is not None or xerr is not None:\n            default_plot_kwargs = {'color': 'black', 'fmt': 'o', 'ms': 3, 'lw': 1.5, 'label': None}\n        else:\n            default_plot_kwargs = {'color': 'black', 's': 3, 'label': None}\n\n        figsize = kwargs.get('figsize', (8, 4.5))\n        fig_kwargs = {'figsize': figsize, **kwargs.pop('fig_kwargs', {})}\n        plot_kwargs = {**default_plot_kwargs, **kwargs.pop('plot_kwargs', {})}\n        major_tick_kwargs = {'which': 'major', **kwargs.pop('major_tick_kwargs', {})}\n        minor_tick_kwargs = {'which': 'minor', **kwargs.pop('minor_tick_kwargs', {})}\n        savefig_kwargs = kwargs.pop('savefig_kwargs', {})\n        save = kwargs.pop('save', None)\n\n        plt.figure(**fig_kwargs)\n\n        if yerr is not None:\n            if xerr is not None:\n                plt.errorbar(x, y, xerr=xerr,yerr=yerr, **plot_kwargs)\n            else:\n                plt.errorbar(x, y, yerr=yerr, **plot_kwargs)\n        else:\n            if xerr is not None:\n                plt.errorbar(x, y, xerr=xerr, **plot_kwargs)\n            else:\n                plt.scatter(x, y, **plot_kwargs)\n\n        # Set labels if provided\n        xlabel = kwargs.get('xlabel', None)\n        ylabel = kwargs.get('ylabel', None)\n\n        if xlabel:\n            plt.xlabel(xlabel, fontsize=12)\n        if ylabel:\n            plt.ylabel(ylabel, fontsize=12)\n\n        plt.xscale(kwargs.get('xscale', 'linear'))\n        plt.yscale(kwargs.get('yscale', 'linear'))\n\n        # Show legend if label is provided\n        if plot_kwargs.get('label'):\n            plt.legend()\n\n        if title:\n            plt.title(title)\n\n        # Tick kwargs\n        major_tick_kwargs.setdefault('which', 'both')\n        major_tick_kwargs.setdefault('direction', 'in')\n        major_tick_kwargs.setdefault('length', 6)\n        major_tick_kwargs.setdefault('width', 1)\n        major_tick_kwargs.setdefault('labelsize', 12)\n        major_tick_kwargs.setdefault('top', True)\n        major_tick_kwargs.setdefault('right', True)\n\n        plt.tick_params(**major_tick_kwargs)\n        if len(minor_tick_kwargs) &gt; 1:\n            plt.minorticks_on()\n            plt.tick_params(**minor_tick_kwargs)\n\n        plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n\n        if save:\n            plt.savefig(save, **savefig_kwargs)\n\n        plt.show()\n</code></pre>"},{"location":"reference/plot/#stela_toolkit.plot.Plotter.plot","title":"<code>plot(x=None, y=None, xerr=None, yerr=None, **kwargs)</code>  <code>staticmethod</code>","text":"<p>Generalized plotting method for spectrum-like data.</p> Parameters: <p>x : array-like     x-axis values (e.g., frequencies).</p> <p>y : array-like     y-axis values (e.g., power or cross-power values).</p> <p>xerr : array-like, optional     Uncertainties in the x-axis values (e.g., frequency widths).</p> <p>yerr : array-like, optional     Uncertainties in the y-axis values (e.g., power uncertainties).</p> <p>**kwargs: Additional keyword arguments for customization.</p> Source code in <code>stela_toolkit/plot.py</code> <pre><code>@staticmethod\ndef plot(x=None, y=None, xerr=None, yerr=None, **kwargs):\n    \"\"\"\n    Generalized plotting method for spectrum-like data.\n\n    Parameters:\n    ----------\n    x : array-like\n        x-axis values (e.g., frequencies).\n\n    y : array-like\n        y-axis values (e.g., power or cross-power values).\n\n    xerr : array-like, optional\n        Uncertainties in the x-axis values (e.g., frequency widths).\n\n    yerr : array-like, optional\n        Uncertainties in the y-axis values (e.g., power uncertainties).\n\n    **kwargs: Additional keyword arguments for customization.\n    \"\"\"\n\n    if xerr is not None:\n        xerr = xerr if len(list(xerr)) &gt; 0 else None\n    if yerr is not None:\n        yerr = yerr if len(list(yerr)) &gt; 0 else None\n\n    if x is None or y is None:\n        raise ValueError(\"Both 'x' and 'y' must be provided.\")\n\n    title = kwargs.get('title', None)\n\n    # Default plotting settings\n    if yerr is not None or xerr is not None:\n        default_plot_kwargs = {'color': 'black', 'fmt': 'o', 'ms': 3, 'lw': 1.5, 'label': None}\n    else:\n        default_plot_kwargs = {'color': 'black', 's': 3, 'label': None}\n\n    figsize = kwargs.get('figsize', (8, 4.5))\n    fig_kwargs = {'figsize': figsize, **kwargs.pop('fig_kwargs', {})}\n    plot_kwargs = {**default_plot_kwargs, **kwargs.pop('plot_kwargs', {})}\n    major_tick_kwargs = {'which': 'major', **kwargs.pop('major_tick_kwargs', {})}\n    minor_tick_kwargs = {'which': 'minor', **kwargs.pop('minor_tick_kwargs', {})}\n    savefig_kwargs = kwargs.pop('savefig_kwargs', {})\n    save = kwargs.pop('save', None)\n\n    plt.figure(**fig_kwargs)\n\n    if yerr is not None:\n        if xerr is not None:\n            plt.errorbar(x, y, xerr=xerr,yerr=yerr, **plot_kwargs)\n        else:\n            plt.errorbar(x, y, yerr=yerr, **plot_kwargs)\n    else:\n        if xerr is not None:\n            plt.errorbar(x, y, xerr=xerr, **plot_kwargs)\n        else:\n            plt.scatter(x, y, **plot_kwargs)\n\n    # Set labels if provided\n    xlabel = kwargs.get('xlabel', None)\n    ylabel = kwargs.get('ylabel', None)\n\n    if xlabel:\n        plt.xlabel(xlabel, fontsize=12)\n    if ylabel:\n        plt.ylabel(ylabel, fontsize=12)\n\n    plt.xscale(kwargs.get('xscale', 'linear'))\n    plt.yscale(kwargs.get('yscale', 'linear'))\n\n    # Show legend if label is provided\n    if plot_kwargs.get('label'):\n        plt.legend()\n\n    if title:\n        plt.title(title)\n\n    # Tick kwargs\n    major_tick_kwargs.setdefault('which', 'both')\n    major_tick_kwargs.setdefault('direction', 'in')\n    major_tick_kwargs.setdefault('length', 6)\n    major_tick_kwargs.setdefault('width', 1)\n    major_tick_kwargs.setdefault('labelsize', 12)\n    major_tick_kwargs.setdefault('top', True)\n    major_tick_kwargs.setdefault('right', True)\n\n    plt.tick_params(**major_tick_kwargs)\n    if len(minor_tick_kwargs) &gt; 1:\n        plt.minorticks_on()\n        plt.tick_params(**minor_tick_kwargs)\n\n    plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n\n    if save:\n        plt.savefig(save, **savefig_kwargs)\n\n    plt.show()\n</code></pre>"},{"location":"reference/power_spectrum/","title":"power_spectrum","text":""},{"location":"reference/power_spectrum/#stela_toolkit.power_spectrum.PowerSpectrum","title":"<code>PowerSpectrum</code>","text":"<p>Compute the power spectrum of a light curve using the FFT.</p> <p>This class accepts either a STELA LightCurve object or a trained GaussianProcess model. If a GaussianProcess is passed, the most recently generated samples are used.  If no samples exist, the toolkit will automatically generate 1000 posterior realizations on a 1000-point grid.</p> <p>For single light curves, the FFT is applied directly to the time series. For GP models, the power spectrum is computed for each sampled realization, and the mean and standard deviation across all samples are returned.</p> <p>Power spectra are computed in variance units by default (i.e., normalized to units of squared flux), allowing for direct interpretation in the context of variability amplitude and fractional RMS.</p> <p>Frequency binning is supported via linear, logarithmic, or user-defined bins.</p> <p>Parameters:</p> Name Type Description Default <code>lc_or_model</code> <code>LightCurve or GaussianProcess</code> <p>Input light curve or trained GP model.</p> required <code>fmin</code> <code>float or auto</code> <p>Minimum frequency to include. If 'auto', uses the lowest nonzero FFT frequency.</p> <code>'auto'</code> <code>fmax</code> <code>float or auto</code> <p>Maximum frequency to include. If 'auto', uses the Nyquist frequency.</p> <code>'auto'</code> <code>num_bins</code> <code>int</code> <p>Number of frequency bins.</p> <code>None</code> <code>bin_type</code> <code>str</code> <p>Binning type: 'log' or 'linear'.</p> <code>'log'</code> <code>bin_edges</code> <code>array - like</code> <p>Custom bin edges (overrides <code>num_bins</code> and <code>bin_type</code>).</p> <code>[]</code> <code>norm</code> <code>bool</code> <p>Whether to normalize the power spectrum to variance units (i.e., PSD units).</p> <code>True</code> <p>Attributes:</p> Name Type Description <code>freqs</code> <code>array - like</code> <p>Center frequencies of each bin.</p> <code>freq_widths</code> <code>array - like</code> <p>Bin widths for each frequency bin.</p> <code>powers</code> <code>array - like</code> <p>Power spectrum values (or mean if using GP samples).</p> <code>power_errors</code> <code>array - like</code> <p>Uncertainties in the power spectrum (std across GP samples if applicable).</p> Source code in <code>stela_toolkit/power_spectrum.py</code> <pre><code>class PowerSpectrum:\n    \"\"\"\n    Compute the power spectrum of a light curve using the FFT.\n\n    This class accepts either a STELA LightCurve object or a trained GaussianProcess model.\n    If a GaussianProcess is passed, the most recently generated samples are used. \n    If no samples exist, the toolkit will automatically generate 1000 posterior realizations\n    on a 1000-point grid.\n\n    For single light curves, the FFT is applied directly to the time series.\n    For GP models, the power spectrum is computed for each sampled realization,\n    and the mean and standard deviation across all samples are returned.\n\n    Power spectra are computed in variance units by default (i.e., normalized to units\n    of squared flux), allowing for direct interpretation in the context of variability\n    amplitude and fractional RMS.\n\n    Frequency binning is supported via linear, logarithmic, or user-defined bins.\n\n    Parameters\n    ----------\n    lc_or_model : LightCurve or GaussianProcess\n        Input light curve or trained GP model.\n\n    fmin : float or 'auto', optional\n        Minimum frequency to include. If 'auto', uses the lowest nonzero FFT frequency.\n\n    fmax : float or 'auto', optional\n        Maximum frequency to include. If 'auto', uses the Nyquist frequency.\n\n    num_bins : int, optional\n        Number of frequency bins.\n\n    bin_type : str, optional\n        Binning type: 'log' or 'linear'.\n\n    bin_edges : array-like, optional\n        Custom bin edges (overrides `num_bins` and `bin_type`).\n\n    norm : bool, optional\n        Whether to normalize the power spectrum to variance units (i.e., PSD units).\n\n    Attributes\n    ----------\n    freqs : array-like\n        Center frequencies of each bin.\n\n    freq_widths : array-like\n        Bin widths for each frequency bin.\n\n    powers : array-like\n        Power spectrum values (or mean if using GP samples).\n\n    power_errors : array-like\n        Uncertainties in the power spectrum (std across GP samples if applicable).\n    \"\"\"\n\n    def __init__(self,\n                 lc_or_model,\n                 fmin='auto',\n                 fmax='auto',\n                 num_bins=None,\n                 bin_type=\"log\",\n                 bin_edges=[],\n                 norm=True):\n\n        # To do: ValueError for norm=True acting on mean=0 (standardized data)\n        input_data = _CheckInputs._check_lightcurve_or_model(lc_or_model)\n        if input_data['type'] == 'model':\n            self.times, self.rates = input_data['data']\n        else:\n            self.times, self.rates, _ = input_data['data']\n        _CheckInputs._check_input_bins(num_bins, bin_type, bin_edges)\n\n        # Use absolute min and max frequencies if set to 'auto'\n        self.dt = np.diff(self.times)[0]\n        self.fmin = np.fft.rfftfreq(len(self.rates), d=self.dt)[1] if fmin == 'auto' else fmin\n        self.fmax = np.fft.rfftfreq(len(self.rates), d=self.dt)[-1] if fmax == 'auto' else fmax  # nyquist frequency\n\n        self.num_bins = num_bins\n        self.bin_type = bin_type\n        self.bin_edges = bin_edges\n\n        # if multiple light curve are provided, compute the stacked power spectrum\n        if len(self.rates.shape) == 2:\n            power_spectrum = self.compute_stacked_power_spectrum(norm=norm)\n        else:\n            power_spectrum = self.compute_power_spectrum(norm=norm)\n\n        self.freqs, self.freq_widths, self.powers, self.power_errors = power_spectrum\n\n    def compute_power_spectrum(self, times=None, rates=None, norm=True):\n        \"\"\"\n        Compute the power spectrum for a single light curve.\n\n        Applies the FFT to the light curve and optionally normalizes the result\n        to variance (PSD) units. If binning is enabled, returns binned power.\n\n        Parameters\n        ----------\n        times : array-like, optional\n            Time array to use (defaults to internal value).\n\n        rates : array-like, optional\n            Rate array to use (defaults to internal value).\n\n        norm : bool, optional\n            Whether to normalize to variance units.\n\n        Returns\n        -------\n        freqs : array-like\n            Frequencies of the power spectrum.\n\n        freq_widths : array-like or None\n            Bin widths (if binned).\n\n        powers : array-like\n            Power spectrum values.\n\n        power_errors : array-like or None\n            Power spectrum uncertainties (if binned).\n        \"\"\"\n\n        times = self.times if times is None else times\n        rates = self.rates if rates is None else rates\n        length = len(rates)\n\n        freqs, fft = LightCurve(times=times, rates=rates).fft()\n        powers = np.abs(fft) ** 2\n\n        # Filter frequencies within [fmin, fmax]\n        valid_mask = (freqs &gt;= self.fmin) &amp; (freqs &lt;= self.fmax)\n        freqs = freqs[valid_mask]\n        powers = powers[valid_mask]\n\n        if norm:\n            powers /= length * np.mean(rates) ** 2 / (2 * self.dt)\n\n        # Apply binning\n        if self.num_bins or self.bin_edges:\n\n            if self.bin_edges:\n                bin_edges = FrequencyBinning.define_bins(self.fmin, self.fmax, num_bins=self.num_bins, \n                                                         bin_type=self.bin_type, bin_edges=self.bin_edges\n                                                        )\n\n            elif self.num_bins:\n                bin_edges = FrequencyBinning.define_bins(self.fmin, self.fmax, num_bins=self.num_bins, bin_type=self.bin_type)\n\n            else:\n                raise ValueError(\"Either num_bins or bin_edges must be provided.\\n\"\n                                 \"In other words, you must specify the number of bins or the bin edges.\")\n\n            binned_power = FrequencyBinning.bin_data(freqs, powers, bin_edges)\n            freqs, freq_widths, powers, power_errors = binned_power\n        else:\n            freq_widths, power_errors = None, None\n\n        return freqs, freq_widths, powers, power_errors\n\n    def compute_stacked_power_spectrum(self, norm=True):\n        \"\"\"\n        Compute power spectrum for each GP sample and return the mean and std.\n        This method is used automatically when a GP model with samples is passed.\n\n        Parameters\n        ----------\n        norm : bool, optional\n            Whether to normalize to variance units.\n\n        Returns\n        -------\n        freqs : array-like\n            Frequencies of the power spectrum.\n\n        freq_widths : array-like\n            Widths of frequency bins.\n\n        power_mean : array-like\n            Mean power spectrum values.\n\n        power_std : array-like\n            Standard deviation of power values across realizations.\n        \"\"\"\n\n        powers = []\n        for i in range(self.rates.shape[0]):\n            power_spectrum = self.compute_power_spectrum(self.times, self.rates[i], norm=norm)\n            freqs, freq_widths, power, _ = power_spectrum\n            powers.append(power)\n\n        # Stack the collected powers and errors\n        powers = np.vstack(powers)\n        power_mean = np.mean(powers, axis=0)\n        power_std = np.std(powers, axis=0)\n\n        return freqs, freq_widths, power_mean, power_std\n\n    def plot(self, freqs=None, freq_widths=None, powers=None, power_errors=None, **kwargs):\n        \"\"\"\n        Plot the power spectrum.\n\n        Parameters\n        ----------\n        **kwargs : dict\n            Custom plotting options (xlabel, yscale, etc.).\n        \"\"\"\n\n        freqs = self.freqs if freqs is None else freqs\n        freq_widths = self.freq_widths if freq_widths is None else freq_widths\n        powers = self.powers if powers is None else powers\n        power_errors = self.power_errors if power_errors is None else power_errors\n\n        kwargs.setdefault('xlabel', 'Frequency')\n        kwargs.setdefault('ylabel', 'Power')\n        kwargs.setdefault('xscale', 'log')\n        kwargs.setdefault('yscale', 'log')\n        Plotter.plot(x=freqs, y=powers, xerr=freq_widths, yerr=power_errors, **kwargs)\n\n    def count_frequencies_in_bins(self, fmin=None, fmax=None, num_bins=None, bin_type=None, bin_edges=[]):\n        \"\"\"\n        Counts the number of frequencies in each frequency bin.\n        Wrapper method to use FrequencyBinning.count_frequencies_in_bins with class attributes.\n        \"\"\"\n\n        return FrequencyBinning.count_frequencies_in_bins(\n            self, fmin=fmin, fmax=fmax, num_bins=num_bins, bin_type=bin_type, bin_edges=bin_edges\n        )\n</code></pre>"},{"location":"reference/power_spectrum/#stela_toolkit.power_spectrum.PowerSpectrum.compute_power_spectrum","title":"<code>compute_power_spectrum(times=None, rates=None, norm=True)</code>","text":"<p>Compute the power spectrum for a single light curve.</p> <p>Applies the FFT to the light curve and optionally normalizes the result to variance (PSD) units. If binning is enabled, returns binned power.</p> <p>Parameters:</p> Name Type Description Default <code>times</code> <code>array - like</code> <p>Time array to use (defaults to internal value).</p> <code>None</code> <code>rates</code> <code>array - like</code> <p>Rate array to use (defaults to internal value).</p> <code>None</code> <code>norm</code> <code>bool</code> <p>Whether to normalize to variance units.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>freqs</code> <code>array - like</code> <p>Frequencies of the power spectrum.</p> <code>freq_widths</code> <code>array - like or None</code> <p>Bin widths (if binned).</p> <code>powers</code> <code>array - like</code> <p>Power spectrum values.</p> <code>power_errors</code> <code>array - like or None</code> <p>Power spectrum uncertainties (if binned).</p> Source code in <code>stela_toolkit/power_spectrum.py</code> <pre><code>def compute_power_spectrum(self, times=None, rates=None, norm=True):\n    \"\"\"\n    Compute the power spectrum for a single light curve.\n\n    Applies the FFT to the light curve and optionally normalizes the result\n    to variance (PSD) units. If binning is enabled, returns binned power.\n\n    Parameters\n    ----------\n    times : array-like, optional\n        Time array to use (defaults to internal value).\n\n    rates : array-like, optional\n        Rate array to use (defaults to internal value).\n\n    norm : bool, optional\n        Whether to normalize to variance units.\n\n    Returns\n    -------\n    freqs : array-like\n        Frequencies of the power spectrum.\n\n    freq_widths : array-like or None\n        Bin widths (if binned).\n\n    powers : array-like\n        Power spectrum values.\n\n    power_errors : array-like or None\n        Power spectrum uncertainties (if binned).\n    \"\"\"\n\n    times = self.times if times is None else times\n    rates = self.rates if rates is None else rates\n    length = len(rates)\n\n    freqs, fft = LightCurve(times=times, rates=rates).fft()\n    powers = np.abs(fft) ** 2\n\n    # Filter frequencies within [fmin, fmax]\n    valid_mask = (freqs &gt;= self.fmin) &amp; (freqs &lt;= self.fmax)\n    freqs = freqs[valid_mask]\n    powers = powers[valid_mask]\n\n    if norm:\n        powers /= length * np.mean(rates) ** 2 / (2 * self.dt)\n\n    # Apply binning\n    if self.num_bins or self.bin_edges:\n\n        if self.bin_edges:\n            bin_edges = FrequencyBinning.define_bins(self.fmin, self.fmax, num_bins=self.num_bins, \n                                                     bin_type=self.bin_type, bin_edges=self.bin_edges\n                                                    )\n\n        elif self.num_bins:\n            bin_edges = FrequencyBinning.define_bins(self.fmin, self.fmax, num_bins=self.num_bins, bin_type=self.bin_type)\n\n        else:\n            raise ValueError(\"Either num_bins or bin_edges must be provided.\\n\"\n                             \"In other words, you must specify the number of bins or the bin edges.\")\n\n        binned_power = FrequencyBinning.bin_data(freqs, powers, bin_edges)\n        freqs, freq_widths, powers, power_errors = binned_power\n    else:\n        freq_widths, power_errors = None, None\n\n    return freqs, freq_widths, powers, power_errors\n</code></pre>"},{"location":"reference/power_spectrum/#stela_toolkit.power_spectrum.PowerSpectrum.compute_stacked_power_spectrum","title":"<code>compute_stacked_power_spectrum(norm=True)</code>","text":"<p>Compute power spectrum for each GP sample and return the mean and std. This method is used automatically when a GP model with samples is passed.</p> <p>Parameters:</p> Name Type Description Default <code>norm</code> <code>bool</code> <p>Whether to normalize to variance units.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>freqs</code> <code>array - like</code> <p>Frequencies of the power spectrum.</p> <code>freq_widths</code> <code>array - like</code> <p>Widths of frequency bins.</p> <code>power_mean</code> <code>array - like</code> <p>Mean power spectrum values.</p> <code>power_std</code> <code>array - like</code> <p>Standard deviation of power values across realizations.</p> Source code in <code>stela_toolkit/power_spectrum.py</code> <pre><code>def compute_stacked_power_spectrum(self, norm=True):\n    \"\"\"\n    Compute power spectrum for each GP sample and return the mean and std.\n    This method is used automatically when a GP model with samples is passed.\n\n    Parameters\n    ----------\n    norm : bool, optional\n        Whether to normalize to variance units.\n\n    Returns\n    -------\n    freqs : array-like\n        Frequencies of the power spectrum.\n\n    freq_widths : array-like\n        Widths of frequency bins.\n\n    power_mean : array-like\n        Mean power spectrum values.\n\n    power_std : array-like\n        Standard deviation of power values across realizations.\n    \"\"\"\n\n    powers = []\n    for i in range(self.rates.shape[0]):\n        power_spectrum = self.compute_power_spectrum(self.times, self.rates[i], norm=norm)\n        freqs, freq_widths, power, _ = power_spectrum\n        powers.append(power)\n\n    # Stack the collected powers and errors\n    powers = np.vstack(powers)\n    power_mean = np.mean(powers, axis=0)\n    power_std = np.std(powers, axis=0)\n\n    return freqs, freq_widths, power_mean, power_std\n</code></pre>"},{"location":"reference/power_spectrum/#stela_toolkit.power_spectrum.PowerSpectrum.count_frequencies_in_bins","title":"<code>count_frequencies_in_bins(fmin=None, fmax=None, num_bins=None, bin_type=None, bin_edges=[])</code>","text":"<p>Counts the number of frequencies in each frequency bin. Wrapper method to use FrequencyBinning.count_frequencies_in_bins with class attributes.</p> Source code in <code>stela_toolkit/power_spectrum.py</code> <pre><code>def count_frequencies_in_bins(self, fmin=None, fmax=None, num_bins=None, bin_type=None, bin_edges=[]):\n    \"\"\"\n    Counts the number of frequencies in each frequency bin.\n    Wrapper method to use FrequencyBinning.count_frequencies_in_bins with class attributes.\n    \"\"\"\n\n    return FrequencyBinning.count_frequencies_in_bins(\n        self, fmin=fmin, fmax=fmax, num_bins=num_bins, bin_type=bin_type, bin_edges=bin_edges\n    )\n</code></pre>"},{"location":"reference/power_spectrum/#stela_toolkit.power_spectrum.PowerSpectrum.plot","title":"<code>plot(freqs=None, freq_widths=None, powers=None, power_errors=None, **kwargs)</code>","text":"<p>Plot the power spectrum.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>dict</code> <p>Custom plotting options (xlabel, yscale, etc.).</p> <code>{}</code> Source code in <code>stela_toolkit/power_spectrum.py</code> <pre><code>def plot(self, freqs=None, freq_widths=None, powers=None, power_errors=None, **kwargs):\n    \"\"\"\n    Plot the power spectrum.\n\n    Parameters\n    ----------\n    **kwargs : dict\n        Custom plotting options (xlabel, yscale, etc.).\n    \"\"\"\n\n    freqs = self.freqs if freqs is None else freqs\n    freq_widths = self.freq_widths if freq_widths is None else freq_widths\n    powers = self.powers if powers is None else powers\n    power_errors = self.power_errors if power_errors is None else power_errors\n\n    kwargs.setdefault('xlabel', 'Frequency')\n    kwargs.setdefault('ylabel', 'Power')\n    kwargs.setdefault('xscale', 'log')\n    kwargs.setdefault('yscale', 'log')\n    Plotter.plot(x=freqs, y=powers, xerr=freq_widths, yerr=power_errors, **kwargs)\n</code></pre>"},{"location":"reference/preprocessing/","title":"preprocessing","text":""},{"location":"reference/preprocessing/#stela_toolkit.preprocessing.Preprocessing","title":"<code>Preprocessing</code>","text":"<p>Utility functions for cleaning and transforming light curves.</p> <p>The static methods in this class operate on LightCurve objects directly, modifying them in place unless otherwise specified.</p> <p>These methods are used throughout the STELA Toolkit to prepare light curves for Gaussian process modeling and spectral analysis. This includes:</p> <ul> <li>Standardizing light curve data (zero mean, unit variance)</li> <li>Applying and reversing a Box-Cox transformation to normalize flux distributions</li> <li>Checking for Gaussianity using the Shapiro-Wilk test and Q-Q plots</li> <li>Trimming light curves by time range</li> <li>Removing outliers using global or local IQR</li> <li>Polynomial detrending</li> <li>Handling NaNs or missing data</li> </ul> <p>Most methods automatically store relevant metadata (e.g., original mean, std, Box-Cox lambda) on the LightCurve object for later reversal.</p> <p>All methods are static and do not require instantiating this class.</p> Source code in <code>stela_toolkit/preprocessing.py</code> <pre><code>class Preprocessing:\n    \"\"\"\n    Utility functions for cleaning and transforming light curves.\n\n    The static methods in this class operate on LightCurve objects directly,\n    modifying them in place unless otherwise specified.\n\n    These methods are used throughout the STELA Toolkit to prepare light curves\n    for Gaussian process modeling and spectral analysis. This includes:\n\n    - Standardizing light curve data (zero mean, unit variance)\n    - Applying and reversing a Box-Cox transformation to normalize flux distributions\n    - Checking for Gaussianity using the Shapiro-Wilk test and Q-Q plots\n    - Trimming light curves by time range\n    - Removing outliers using global or local IQR\n    - Polynomial detrending\n    - Handling NaNs or missing data\n\n    Most methods automatically store relevant metadata (e.g., original mean, std, Box-Cox lambda)\n    on the LightCurve object for later reversal.\n\n    All methods are static and do not require instantiating this class.\n    \"\"\"\n\n    @staticmethod\n    def standardize(lightcurve):\n        \"\"\"\n        Standardize the light curve by subtracting its mean and dividing by its std.\n\n        Saves the original mean and std as attributes for future unstandardization.\n        \"\"\"\n        lc = lightcurve\n\n        # check for standardization\n        if np.isclose(lc.mean, 0, atol=1e-10) and np.isclose(lc.std, 1, atol=1e-10) or getattr(lc, \"is_standard\", False):\n            if not hasattr(lc, \"unstandard_mean\") and not hasattr(lc, \"unstandard_std\"):\n                lc.unstandard_mean = 0\n                lc.unstandard_std = 1\n            print(\"The data is already standardized.\")\n\n        # apply standardization\n        else:\n            lc.unstandard_mean = lc.mean\n            lc.unstandard_std = lc.std\n            lc.rates = (lc.rates - lc.unstandard_mean) / lc.unstandard_std\n            if lc.errors.size &gt; 0:\n                lc.errors = lc.errors / lc.unstandard_std\n\n        lc.is_standard = True # flag for detecting transformation without computation\n\n    @staticmethod\n    def unstandardize(lightcurve):\n        \"\"\"\n        Restore the light curve to its original units using stored mean and std.\n\n        This reverses a previous call to `standardize`.\n        \"\"\"\n        lc = lightcurve\n        # check that data has been standardized\n        if getattr(lc, \"is_standard\", False):\n            lc.rates = (lc.rates * lc.unstandard_std) + lc.unstandard_mean\n        else:\n            if np.isclose(lc.mean, 0, atol=1e-10) and np.isclose(lc.std, 1, atol=1e-10):\n                raise AttributeError(\n                    \"The data has not been standardized by STELA.\\n\"\n                    \"Please call the 'standardize' method first.\"\n                )\n            else:\n                raise AttributeError(\n                    \"The data is not standardized, and needs to be standardized first by STELA.\\n\"\n                    \"Please call the 'standardize' method first (e.g., Preprocessing.standardize(lightcurve)).\"\n                )\n\n        if lc.errors.size &gt; 0:\n            lc.errors = lc.errors * lc.unstandard_std\n\n        lc.is_standard = False  # reset the standardization flag\n\n    @staticmethod\n    def generate_qq_plot(lightcurve=None, rates=[]):\n        \"\"\"\n        Generate a Q-Q plot to visually assess normality.\n\n        Parameters\n        ----------\n        lightcurve : LightCurve, optional\n            Light curve to extract rates from.\n\n        rates : array-like, optional\n            Direct rate values if not using a LightCurve.\n        \"\"\"\n        if lightcurve:\n            rates = lightcurve.rates.copy()\n        elif np.array(rates).size != 0:\n            pass\n        else: \n            raise ValueError(\"Either 'lightcurve' or 'rates' must be provided.\")\n\n        rates_std = (rates - np.mean(rates)) / np.std(rates)\n        (osm, osr), _ = probplot(rates_std, dist=\"norm\")\n\n        plt.figure(figsize=(8, 4.5))\n        plt.plot(osm, osr, 'o', color='black', markersize=4)\n        plt.plot(osm, osm, 'g--', lw=1, label='Ideal Normal')\n\n        plt.title(\"Q-Q Plot\", fontsize=12)\n        plt.xlabel(\"Theoretical Quantiles\", fontsize=12)\n        plt.ylabel(\"Sample Quantiles\", fontsize=12)\n        plt.legend(loc='upper left')\n        plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n        plt.tick_params(which='both', direction='in', length=6, width=1, top=True, right=True, labelsize=12)\n        plt.show()\n\n    @staticmethod\n    def check_normal(lightcurve=None, rates=[], plot=True, _boxcox=False, verbose=True):\n        \"\"\"\n        Test for normality using an appropriate statistical test based on sample size.\n\n        For small samples (n &lt; 50), this uses the Shapiro-Wilk test. For larger samples,\n        it uses the Lilliefors version of the Kolmogorov-Smirnov test. Results are printed\n        with an interpretation of the strength of evidence against normality.\n\n        If `plot=True`, a Q-Q plot of the distribution is shown. This function supports either\n        a full LightCurve object or a raw array of flux values.\n\n        Parameters\n        ----------\n        lightcurve : LightCurve, optional\n            The light curve object containing the rates to test.\n\n        rates : array-like, optional\n            Direct rate values if not using a LightCurve.\n\n        plot : bool, optional\n            Whether to display a Q-Q plot.\n\n        _boxcox : bool, optional\n            Whether this check is being called internally after Box-Cox (affects messaging only).\n\n        verbose : bool, optional\n            Whether to generate print statements.\n\n        Returns\n        -------\n        is_normal : bool\n            True if the data appears normally distributed (p &gt; 0.05).\n\n        pvalue : float\n            The p-value from the chosen normality test.\n        \"\"\"\n\n        if lightcurve:\n            rates = lightcurve.rates.copy()\n        elif np.array(rates).size != 0:\n            rates = np.array(rates)\n        else:\n            raise ValueError(\"Either 'lightcurve' or 'rates' must be provided.\")\n\n        n = len(rates)\n        if n &lt; 50:\n            if verbose:\n                print(\"Using Shapiro-Wilk test (recommended for n &lt; 50)\")\n            test_name = \"Shapiro-Wilk\"\n            pvalue = shapiro(rates).pvalue\n        else:\n            if verbose:\n                print(\"Using Lilliefors test (for n &gt;= 50)\")\n            test_name = \"Lilliefors (modified KS)\"\n            _, pvalue = lilliefors(rates, dist='norm')\n\n        if verbose:\n            print(f\"{test_name} test p-value: {pvalue:.3g}\")\n            if pvalue &lt;= 0.001:\n                strength = \"very strong\"\n            elif pvalue &lt;= 0.01:\n                strength = \"strong\"\n            elif pvalue &lt;= 0.05:\n                strength = \"weak\"\n            else:\n                strength = \"little to no\"\n\n            print(f\"  -&gt; {strength.capitalize()} evidence against normality (p = {pvalue:.3g})\")\n\n            if pvalue &lt;= 0.05 and not _boxcox:\n                print(\"     - Consider running `check_boxcox_normal()` to see if a Box-Cox transformation can help.\")\n                print(\"     - Often checking normality via a Q-Q plot (run `generate_qq_plot(lightcurve)`) is sufficient.\")\n            print(\"===================\")\n\n        if plot:\n            Preprocessing.generate_qq_plot(rates=rates)\n\n        return pvalue &gt; 0.05, pvalue\n\n\n    @staticmethod\n    def boxcox_transform(lightcurve, save=True):\n        \"\"\"\n        Apply a Box-Cox transformation to normalize the flux distribution.\n\n        Also adjusts errors using the delta method. Stores the transformation\n        parameter lambda and sets a flag for reversal.\n\n        Parameters\n        ----------\n        lightcurve : LightCurve\n            The input light curve.\n\n        save : bool\n            Whether to modify the light curve in place.\n        \"\"\"\n\n        lc = lightcurve\n        rates_boxcox, lambda_opt = boxcox(lc.rates)\n\n        # transform errors using delta method (derivative-based propagation)\n        if lc.errors.size != 0:\n            if lambda_opt == 0:  # log transformation (lambda = 0)\n                errors_boxcox = lc.errors / lc.rates\n            else:\n                errors_boxcox = (lc.rates ** (lambda_opt - 1)) * lc.errors\n        else:\n            errors_boxcox = None\n\n        if save:\n            lc.rates = rates_boxcox\n            lc.errors = errors_boxcox\n            lc.lambda_boxcox = lambda_opt  # save lambda for inverse transformation\n            lc.is_boxcox_transformed = True  # flag to indicate transformation\n        else:\n            return rates_boxcox, errors_boxcox\n\n    @staticmethod\n    def reverse_boxcox_transform(lightcurve):\n        \"\"\"\n        Reverse a previously applied Box-Cox transformation.\n\n        Parameters\n        ----------\n        lightcurve : LightCurve\n            The transformed light curve.\n        \"\"\"\n\n        lc = lightcurve\n\n        if not getattr(lc, \"is_boxcox_transformed\", False):\n            raise ValueError(\"Light curve data has not been transformed with Box-Cox.\")\n\n        lambda_opt = lc.lambda_boxcox\n        if lambda_opt == 0:  # inverse log transformation\n            rates_original = np.exp(lc.rates)\n        else:\n            rates_original = (lc.rates * lambda_opt + 1) ** (1 / lambda_opt)\n\n        if lc.errors.size != 0:\n            if lambda_opt == 0:  # inverse log transformation (lambda = 0)\n                errors_original = lc.errors * rates_original\n            else:\n                errors_original = lc.errors / (rates_original ** (lambda_opt - 1))\n        else:\n            errors_original = None\n\n        lc.rates = rates_original\n        lc.errors = errors_original\n        lc.is_boxcox_transformed = False\n        del lc.lambda_boxcox\n\n    @staticmethod\n    def check_boxcox_normal(lightcurve, plot=True):\n        \"\"\"\n        Apply a Box-Cox transformation and re-test for normality using the appropriate statistical test.\n\n        This method compares the normality of the original flux distribution to its Box-Cox transformed version,\n        using either the Shapiro-Wilk or Lilliefors test depending on sample size. If `plot=True`, a Q-Q plot\n        is generated showing both the original and transformed data.\n\n        Parameters\n        ----------\n        lightcurve : LightCurve\n            The input light curve containing flux values.\n\n        plot : bool, optional\n            Whether to show a Q-Q plot comparing original and Box-Cox transformed distributions.\n\n        Returns\n        -------\n        is_normal : bool\n            True if the Box-Cox transformed data appears normally distributed (p &gt; 0.05).\n\n        pvalue : float\n            The p-value from the normality test applied to the transformed data.\n        \"\"\"\n\n        rates_original = lightcurve.rates.copy()\n        rates_boxcox, _ = Preprocessing.boxcox_transform(lightcurve, save=False)\n\n        print(\"Before Box-Cox:\")\n        print(\"----------------\")\n        Preprocessing.check_normal(lightcurve=lightcurve, plot=False)\n\n        print(\"After Box-Cox:\")\n        print(\"----------------\")\n        is_normal, pvalue = Preprocessing.check_normal(rates=rates_boxcox, plot=False, _boxcox=True)\n\n        if plot:\n            rates_original_std = (rates_original - np.mean(rates_original)) / np.std(rates_original)\n            rates_boxcox_std = (rates_boxcox - np.mean(rates_boxcox)) / np.std(rates_boxcox)\n\n            (osm1, osr1), _ = probplot(rates_original_std, dist=\"norm\")\n            (osm2, osr2), _ = probplot(rates_boxcox_std, dist=\"norm\")\n\n            plt.figure(figsize=(8, 4.5))\n            plt.plot(osm1, osr1, 'o', label='Original', color='black', alpha=0.6, markersize=4)\n            plt.plot(osm2, osr2, 'o', label='Transformed', color='dodgerblue', alpha=0.6, markersize=4)\n            plt.plot(osm1, osm1, 'g--', label='Ideal Normal', alpha=0.5, lw=1.5)\n\n            plt.xlabel(\"Theoretical Quantiles\", fontsize=12)\n            plt.ylabel(\"Sample Quantiles\", fontsize=12)\n            plt.title(\"Q-Q Plot Before and After Box-Cox\", fontsize=12)\n            plt.legend(loc='upper left')\n            plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n            plt.tick_params(which='both', direction='in', length=6, width=1, top=True, right=True, labelsize=12)\n            plt.show()\n\n        return is_normal, pvalue\n\n\n    @staticmethod\n    def trim_time_segment(lightcurve, start_time=None, end_time=None, plot=False, save=True):\n        \"\"\"\n        Trim the light curve to a given time range.\n\n        Parameters\n        ----------\n        start_time : float, optional\n            Lower time bound.\n\n        end_time : float, optional\n            Upper time bound.\n\n        plot : bool\n            Whether to plot before/after trimming.\n\n        save : bool\n            Whether to modify the light curve in place.\n        \"\"\"\n\n        lc = lightcurve\n\n        if start_time is None:\n            start_time = lc.times[0]\n        if end_time is None:\n            end_time = lc.times[-1]\n        if start_time and end_time is None:\n            raise ValueError(\"Please specify a start and/or end time.\")\n\n        # Apply mask to trim data\n        mask = (lc.times &gt;= start_time) &amp; (lc.times &lt;= end_time)\n        if plot:\n            plt.figure(figsize=(8, 4.5))\n            if lc.errors is not None and len(lc.errors) &gt; 0:\n                plt.errorbar(lc.times[mask], lc.rates[mask], yerr=lc.errors[mask], \n                             fmt='o', color='black', ms=3, label='Kept')\n                plt.errorbar(lc.times[~mask], lc.rates[~mask], yerr=lc.errors[~mask], \n                             fmt='o', color='orange', ms=3, label='Trimmed')\n            else:\n                plt.scatter(lc.times[mask], lc.rates[mask], s=6, color=\"black\", label=\"Kept\")\n                plt.scatter(lc.times[~mask], lc.rates[~mask], s=6, color=\"red\", label=\"Trimmed\")\n            plt.xlabel(\"Time\", fontsize=12)\n            plt.ylabel(\"Rates\", fontsize=12)\n            plt.title(\"Trimming\")\n            plt.legend()\n            plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n            plt.tick_params(which='both', direction='in', length=6, width=1, top=True, right=True, labelsize=12)\n            plt.show()\n\n        if save:\n            lc.times = lc.times[mask]\n            lc.rates = lc.rates[mask]\n            if lc.errors.size &gt; 0:\n                lc.errors = lc.errors[mask]\n\n    @staticmethod\n    def remove_nans(lightcurve, verbose=True):\n        \"\"\"\n        Remove time, rate, or error entries that are NaN.\n\n        Parameters\n        ----------\n        lightcurve : LightCurve\n            Light curve to clean.\n\n        verbose : bool\n            Whether to print how many NaNs were removed.\n        \"\"\"\n\n        lc = lightcurve\n        if lc.errors.size &gt; 0:\n            nonnan_mask = ~np.isnan(lc.rates) &amp; ~np.isnan(lc.times) &amp; ~np.isnan(lc.errors)\n        else:\n            nonnan_mask = ~np.isnan(lc.rates) &amp; ~np.isnan(lc.times)\n\n        if verbose:\n            print(f\"Removed {np.sum(~nonnan_mask)} NaN points.\\n\"\n                  f\"({np.sum(np.isnan(lc.rates))} NaN rates, \"\n                  f\"{np.sum(np.isnan(lc.errors))} NaN errors)\")\n            print(\"===================\")\n        lc.times = lc.times[nonnan_mask]\n        lc.rates = lc.rates[nonnan_mask]\n        if lc.errors.size &gt; 0:\n            lc.errors = lc.errors[nonnan_mask]\n\n    @staticmethod\n    def remove_outliers(lightcurve, threshold=1.5, rolling_window=None, plot=True, save=True, verbose=True):\n        \"\"\"\n        Remove outliers using the IQR method, globally or locally.\n\n        Parameters\n        ----------\n        lightcurve : LightCurve\n            The input light curve.\n\n        threshold : float\n            IQR multiplier.\n\n        rolling_window : int, optional\n            Size of local window (if local filtering is desired).\n\n        plot : bool\n            Whether to visualize removed points.\n\n        save : bool\n            Whether to modify the light curve in place.\n\n        verbose : bool\n            Whether to print how many points were removed.\n        \"\"\"\n\n        def plot_outliers(outlier_mask):\n            \"\"\"Plots the data flagged as outliers.\"\"\"\n            plt.figure(figsize=(8, 4.5))\n            if errors is not None:\n                plt.errorbar(times[~outlier_mask], rates[~outlier_mask], yerr=errors[~outlier_mask], \n                             fmt='o', color='black', ms=3, label='Kept')\n                plt.errorbar(times[outlier_mask], rates[outlier_mask], yerr=errors[outlier_mask], \n                             fmt='o', color='orange', ms=3, label='Outliers')\n            else:\n                plt.scatter(times[~outlier_mask], rates[~outlier_mask], \n                            s=6, color='black', label='Kept')\n                plt.scatter(times[outlier_mask], rates[outlier_mask], \n                            s=6, color='orange', label='Outliers')\n            plt.xlabel(\"Time\", fontsize=12)\n            plt.ylabel(\"Rates\", fontsize=12)\n            plt.title(\"Outlier Detection\")\n            plt.legend()\n            plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n            plt.tick_params(which='both', direction='in', length=6, width=1, top=True, right=True, labelsize=12)\n            plt.show()\n\n        def detect_outliers(rates, threshold, rolling_window):\n            if rolling_window:\n                outlier_mask = np.zeros_like(rates, dtype=bool)\n                half_window = rolling_window // 2\n                for i in range(len(rates)):\n                    start = max(0, i - half_window)\n                    end = min(len(rates), i + half_window + 1)\n                    local_data = rates[start:end]\n                    q1, q3 = np.percentile(local_data, [25, 75])\n                    iqr = q3 - q1\n                    lower_bound = q1 - threshold * iqr\n                    upper_bound = q3 + threshold * iqr\n                    if rates[i] &lt; lower_bound or rates[i] &gt; upper_bound:\n                        outlier_mask[i] = True\n            else:\n                q1, q3 = np.percentile(rates, [25, 75])\n                iqr = q3 - q1\n                lower_bound = q1 - threshold * iqr\n                upper_bound = q3 + threshold * iqr\n                outlier_mask = (rates &lt; lower_bound) | (rates &gt; upper_bound)\n            return outlier_mask\n\n        lc = deepcopy(lightcurve)\n        times = lc.times\n        rates = lc.rates\n        errors = lc.errors\n\n        outlier_mask = detect_outliers(rates, threshold=threshold, rolling_window=rolling_window)\n\n        if verbose:\n            print(f\"Removed {np.sum(outlier_mask)} outliers \"\n                  f\"({np.sum(outlier_mask) / len(rates) * 100:.2f}% of data).\")\n            print(\"===================\")\n\n        if plot:\n            plot_outliers(outlier_mask)\n\n        # Save results back to the original lightcurve if save=True\n        if save:\n            lc.times = times[~outlier_mask]\n            lc.rates = rates[~outlier_mask]\n            if errors.size &gt; 0:\n                lc.errors = errors[~outlier_mask]\n\n    @staticmethod\n    def polynomial_detrend(lightcurve, degree=1, plot=False, save=True):\n        \"\"\"\n        Remove a polynomial trend from the light curve.\n\n        Fits and subtracts a polynomial. Optionally modifies in place.\n\n        Parameters\n        ----------\n        lightcurve : LightCurve\n            The input light curve.\n\n        degree : int\n            Degree of the polynomial (default is 1).\n\n        plot : bool\n            Whether to show the trend removal visually.\n\n        save : bool\n            Whether to apply the change to the light curve.\n\n        Returns\n        -------\n        detrended_rates : ndarray, optional\n            Only returned if `save=False`.\n        \"\"\"\n\n        lc = deepcopy(lightcurve)\n\n        # Fit polynomial to the data\n        if lc.errors.size &gt; 0:\n            coefficients = np.polyfit(lc.times, lc.rates, degree, w=1/lc.errors)\n        else:\n            coefficients = np.polyfit(lc.times, lc.rates, degree)\n        polynomial = np.poly1d(coefficients)\n        trend = polynomial(lc.times)\n\n        detrended_rates = lc.rates - trend\n        if plot:\n            plt.figure(figsize=(8, 4.5))\n            if lc.errors is not None and len(lc.errors) &gt; 0:\n                plt.errorbar(lc.times, lc.rates, yerr=lc.errors, \n                             fmt='o', color='black', label=\"Original\", ms=3, lw=1.5, alpha=0.6)\n                plt.errorbar(lc.times, detrended_rates, yerr=lc.errors, \n                             fmt='o', color='dodgerblue', label=\"Detrended\", ms=3, lw=1.5)\n            else:\n                plt.plot(lc.times, lc.rates, label=\"Original\", color=\"black\", alpha=0.6, ms=3, lw=1.5)\n                plt.plot(lc.times, detrended_rates, label=\"Detrended\", color=\"dodgerblue\", ms=3, lw=1.5)\n            plt.plot(lc.times, trend, color='orange', linestyle='--', label='Fitted Trend')\n            plt.xlabel(\"Time\", fontsize=12)\n            plt.ylabel(\"Rates\", fontsize=12)\n            plt.title(\"Polynomial Detrending\")\n            plt.legend()\n            plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n            plt.tick_params(which='both', direction='in', length=6, width=1, top=True, right=True, labelsize=12)\n            plt.show()\n\n        if save:\n            lc.rates = detrended_rates\n</code></pre>"},{"location":"reference/preprocessing/#stela_toolkit.preprocessing.Preprocessing.boxcox_transform","title":"<code>boxcox_transform(lightcurve, save=True)</code>  <code>staticmethod</code>","text":"<p>Apply a Box-Cox transformation to normalize the flux distribution.</p> <p>Also adjusts errors using the delta method. Stores the transformation parameter lambda and sets a flag for reversal.</p> <p>Parameters:</p> Name Type Description Default <code>lightcurve</code> <code>LightCurve</code> <p>The input light curve.</p> required <code>save</code> <code>bool</code> <p>Whether to modify the light curve in place.</p> <code>True</code> Source code in <code>stela_toolkit/preprocessing.py</code> <pre><code>@staticmethod\ndef boxcox_transform(lightcurve, save=True):\n    \"\"\"\n    Apply a Box-Cox transformation to normalize the flux distribution.\n\n    Also adjusts errors using the delta method. Stores the transformation\n    parameter lambda and sets a flag for reversal.\n\n    Parameters\n    ----------\n    lightcurve : LightCurve\n        The input light curve.\n\n    save : bool\n        Whether to modify the light curve in place.\n    \"\"\"\n\n    lc = lightcurve\n    rates_boxcox, lambda_opt = boxcox(lc.rates)\n\n    # transform errors using delta method (derivative-based propagation)\n    if lc.errors.size != 0:\n        if lambda_opt == 0:  # log transformation (lambda = 0)\n            errors_boxcox = lc.errors / lc.rates\n        else:\n            errors_boxcox = (lc.rates ** (lambda_opt - 1)) * lc.errors\n    else:\n        errors_boxcox = None\n\n    if save:\n        lc.rates = rates_boxcox\n        lc.errors = errors_boxcox\n        lc.lambda_boxcox = lambda_opt  # save lambda for inverse transformation\n        lc.is_boxcox_transformed = True  # flag to indicate transformation\n    else:\n        return rates_boxcox, errors_boxcox\n</code></pre>"},{"location":"reference/preprocessing/#stela_toolkit.preprocessing.Preprocessing.check_boxcox_normal","title":"<code>check_boxcox_normal(lightcurve, plot=True)</code>  <code>staticmethod</code>","text":"<p>Apply a Box-Cox transformation and re-test for normality using the appropriate statistical test.</p> <p>This method compares the normality of the original flux distribution to its Box-Cox transformed version, using either the Shapiro-Wilk or Lilliefors test depending on sample size. If <code>plot=True</code>, a Q-Q plot is generated showing both the original and transformed data.</p> <p>Parameters:</p> Name Type Description Default <code>lightcurve</code> <code>LightCurve</code> <p>The input light curve containing flux values.</p> required <code>plot</code> <code>bool</code> <p>Whether to show a Q-Q plot comparing original and Box-Cox transformed distributions.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>is_normal</code> <code>bool</code> <p>True if the Box-Cox transformed data appears normally distributed (p &gt; 0.05).</p> <code>pvalue</code> <code>float</code> <p>The p-value from the normality test applied to the transformed data.</p> Source code in <code>stela_toolkit/preprocessing.py</code> <pre><code>@staticmethod\ndef check_boxcox_normal(lightcurve, plot=True):\n    \"\"\"\n    Apply a Box-Cox transformation and re-test for normality using the appropriate statistical test.\n\n    This method compares the normality of the original flux distribution to its Box-Cox transformed version,\n    using either the Shapiro-Wilk or Lilliefors test depending on sample size. If `plot=True`, a Q-Q plot\n    is generated showing both the original and transformed data.\n\n    Parameters\n    ----------\n    lightcurve : LightCurve\n        The input light curve containing flux values.\n\n    plot : bool, optional\n        Whether to show a Q-Q plot comparing original and Box-Cox transformed distributions.\n\n    Returns\n    -------\n    is_normal : bool\n        True if the Box-Cox transformed data appears normally distributed (p &gt; 0.05).\n\n    pvalue : float\n        The p-value from the normality test applied to the transformed data.\n    \"\"\"\n\n    rates_original = lightcurve.rates.copy()\n    rates_boxcox, _ = Preprocessing.boxcox_transform(lightcurve, save=False)\n\n    print(\"Before Box-Cox:\")\n    print(\"----------------\")\n    Preprocessing.check_normal(lightcurve=lightcurve, plot=False)\n\n    print(\"After Box-Cox:\")\n    print(\"----------------\")\n    is_normal, pvalue = Preprocessing.check_normal(rates=rates_boxcox, plot=False, _boxcox=True)\n\n    if plot:\n        rates_original_std = (rates_original - np.mean(rates_original)) / np.std(rates_original)\n        rates_boxcox_std = (rates_boxcox - np.mean(rates_boxcox)) / np.std(rates_boxcox)\n\n        (osm1, osr1), _ = probplot(rates_original_std, dist=\"norm\")\n        (osm2, osr2), _ = probplot(rates_boxcox_std, dist=\"norm\")\n\n        plt.figure(figsize=(8, 4.5))\n        plt.plot(osm1, osr1, 'o', label='Original', color='black', alpha=0.6, markersize=4)\n        plt.plot(osm2, osr2, 'o', label='Transformed', color='dodgerblue', alpha=0.6, markersize=4)\n        plt.plot(osm1, osm1, 'g--', label='Ideal Normal', alpha=0.5, lw=1.5)\n\n        plt.xlabel(\"Theoretical Quantiles\", fontsize=12)\n        plt.ylabel(\"Sample Quantiles\", fontsize=12)\n        plt.title(\"Q-Q Plot Before and After Box-Cox\", fontsize=12)\n        plt.legend(loc='upper left')\n        plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n        plt.tick_params(which='both', direction='in', length=6, width=1, top=True, right=True, labelsize=12)\n        plt.show()\n\n    return is_normal, pvalue\n</code></pre>"},{"location":"reference/preprocessing/#stela_toolkit.preprocessing.Preprocessing.check_normal","title":"<code>check_normal(lightcurve=None, rates=[], plot=True, _boxcox=False, verbose=True)</code>  <code>staticmethod</code>","text":"<p>Test for normality using an appropriate statistical test based on sample size.</p> <p>For small samples (n &lt; 50), this uses the Shapiro-Wilk test. For larger samples, it uses the Lilliefors version of the Kolmogorov-Smirnov test. Results are printed with an interpretation of the strength of evidence against normality.</p> <p>If <code>plot=True</code>, a Q-Q plot of the distribution is shown. This function supports either a full LightCurve object or a raw array of flux values.</p> <p>Parameters:</p> Name Type Description Default <code>lightcurve</code> <code>LightCurve</code> <p>The light curve object containing the rates to test.</p> <code>None</code> <code>rates</code> <code>array - like</code> <p>Direct rate values if not using a LightCurve.</p> <code>[]</code> <code>plot</code> <code>bool</code> <p>Whether to display a Q-Q plot.</p> <code>True</code> <code>_boxcox</code> <code>bool</code> <p>Whether this check is being called internally after Box-Cox (affects messaging only).</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>Whether to generate print statements.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>is_normal</code> <code>bool</code> <p>True if the data appears normally distributed (p &gt; 0.05).</p> <code>pvalue</code> <code>float</code> <p>The p-value from the chosen normality test.</p> Source code in <code>stela_toolkit/preprocessing.py</code> <pre><code>@staticmethod\ndef check_normal(lightcurve=None, rates=[], plot=True, _boxcox=False, verbose=True):\n    \"\"\"\n    Test for normality using an appropriate statistical test based on sample size.\n\n    For small samples (n &lt; 50), this uses the Shapiro-Wilk test. For larger samples,\n    it uses the Lilliefors version of the Kolmogorov-Smirnov test. Results are printed\n    with an interpretation of the strength of evidence against normality.\n\n    If `plot=True`, a Q-Q plot of the distribution is shown. This function supports either\n    a full LightCurve object or a raw array of flux values.\n\n    Parameters\n    ----------\n    lightcurve : LightCurve, optional\n        The light curve object containing the rates to test.\n\n    rates : array-like, optional\n        Direct rate values if not using a LightCurve.\n\n    plot : bool, optional\n        Whether to display a Q-Q plot.\n\n    _boxcox : bool, optional\n        Whether this check is being called internally after Box-Cox (affects messaging only).\n\n    verbose : bool, optional\n        Whether to generate print statements.\n\n    Returns\n    -------\n    is_normal : bool\n        True if the data appears normally distributed (p &gt; 0.05).\n\n    pvalue : float\n        The p-value from the chosen normality test.\n    \"\"\"\n\n    if lightcurve:\n        rates = lightcurve.rates.copy()\n    elif np.array(rates).size != 0:\n        rates = np.array(rates)\n    else:\n        raise ValueError(\"Either 'lightcurve' or 'rates' must be provided.\")\n\n    n = len(rates)\n    if n &lt; 50:\n        if verbose:\n            print(\"Using Shapiro-Wilk test (recommended for n &lt; 50)\")\n        test_name = \"Shapiro-Wilk\"\n        pvalue = shapiro(rates).pvalue\n    else:\n        if verbose:\n            print(\"Using Lilliefors test (for n &gt;= 50)\")\n        test_name = \"Lilliefors (modified KS)\"\n        _, pvalue = lilliefors(rates, dist='norm')\n\n    if verbose:\n        print(f\"{test_name} test p-value: {pvalue:.3g}\")\n        if pvalue &lt;= 0.001:\n            strength = \"very strong\"\n        elif pvalue &lt;= 0.01:\n            strength = \"strong\"\n        elif pvalue &lt;= 0.05:\n            strength = \"weak\"\n        else:\n            strength = \"little to no\"\n\n        print(f\"  -&gt; {strength.capitalize()} evidence against normality (p = {pvalue:.3g})\")\n\n        if pvalue &lt;= 0.05 and not _boxcox:\n            print(\"     - Consider running `check_boxcox_normal()` to see if a Box-Cox transformation can help.\")\n            print(\"     - Often checking normality via a Q-Q plot (run `generate_qq_plot(lightcurve)`) is sufficient.\")\n        print(\"===================\")\n\n    if plot:\n        Preprocessing.generate_qq_plot(rates=rates)\n\n    return pvalue &gt; 0.05, pvalue\n</code></pre>"},{"location":"reference/preprocessing/#stela_toolkit.preprocessing.Preprocessing.generate_qq_plot","title":"<code>generate_qq_plot(lightcurve=None, rates=[])</code>  <code>staticmethod</code>","text":"<p>Generate a Q-Q plot to visually assess normality.</p> <p>Parameters:</p> Name Type Description Default <code>lightcurve</code> <code>LightCurve</code> <p>Light curve to extract rates from.</p> <code>None</code> <code>rates</code> <code>array - like</code> <p>Direct rate values if not using a LightCurve.</p> <code>[]</code> Source code in <code>stela_toolkit/preprocessing.py</code> <pre><code>@staticmethod\ndef generate_qq_plot(lightcurve=None, rates=[]):\n    \"\"\"\n    Generate a Q-Q plot to visually assess normality.\n\n    Parameters\n    ----------\n    lightcurve : LightCurve, optional\n        Light curve to extract rates from.\n\n    rates : array-like, optional\n        Direct rate values if not using a LightCurve.\n    \"\"\"\n    if lightcurve:\n        rates = lightcurve.rates.copy()\n    elif np.array(rates).size != 0:\n        pass\n    else: \n        raise ValueError(\"Either 'lightcurve' or 'rates' must be provided.\")\n\n    rates_std = (rates - np.mean(rates)) / np.std(rates)\n    (osm, osr), _ = probplot(rates_std, dist=\"norm\")\n\n    plt.figure(figsize=(8, 4.5))\n    plt.plot(osm, osr, 'o', color='black', markersize=4)\n    plt.plot(osm, osm, 'g--', lw=1, label='Ideal Normal')\n\n    plt.title(\"Q-Q Plot\", fontsize=12)\n    plt.xlabel(\"Theoretical Quantiles\", fontsize=12)\n    plt.ylabel(\"Sample Quantiles\", fontsize=12)\n    plt.legend(loc='upper left')\n    plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n    plt.tick_params(which='both', direction='in', length=6, width=1, top=True, right=True, labelsize=12)\n    plt.show()\n</code></pre>"},{"location":"reference/preprocessing/#stela_toolkit.preprocessing.Preprocessing.polynomial_detrend","title":"<code>polynomial_detrend(lightcurve, degree=1, plot=False, save=True)</code>  <code>staticmethod</code>","text":"<p>Remove a polynomial trend from the light curve.</p> <p>Fits and subtracts a polynomial. Optionally modifies in place.</p> <p>Parameters:</p> Name Type Description Default <code>lightcurve</code> <code>LightCurve</code> <p>The input light curve.</p> required <code>degree</code> <code>int</code> <p>Degree of the polynomial (default is 1).</p> <code>1</code> <code>plot</code> <code>bool</code> <p>Whether to show the trend removal visually.</p> <code>False</code> <code>save</code> <code>bool</code> <p>Whether to apply the change to the light curve.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>detrended_rates</code> <code>(ndarray, optional)</code> <p>Only returned if <code>save=False</code>.</p> Source code in <code>stela_toolkit/preprocessing.py</code> <pre><code>@staticmethod\ndef polynomial_detrend(lightcurve, degree=1, plot=False, save=True):\n    \"\"\"\n    Remove a polynomial trend from the light curve.\n\n    Fits and subtracts a polynomial. Optionally modifies in place.\n\n    Parameters\n    ----------\n    lightcurve : LightCurve\n        The input light curve.\n\n    degree : int\n        Degree of the polynomial (default is 1).\n\n    plot : bool\n        Whether to show the trend removal visually.\n\n    save : bool\n        Whether to apply the change to the light curve.\n\n    Returns\n    -------\n    detrended_rates : ndarray, optional\n        Only returned if `save=False`.\n    \"\"\"\n\n    lc = deepcopy(lightcurve)\n\n    # Fit polynomial to the data\n    if lc.errors.size &gt; 0:\n        coefficients = np.polyfit(lc.times, lc.rates, degree, w=1/lc.errors)\n    else:\n        coefficients = np.polyfit(lc.times, lc.rates, degree)\n    polynomial = np.poly1d(coefficients)\n    trend = polynomial(lc.times)\n\n    detrended_rates = lc.rates - trend\n    if plot:\n        plt.figure(figsize=(8, 4.5))\n        if lc.errors is not None and len(lc.errors) &gt; 0:\n            plt.errorbar(lc.times, lc.rates, yerr=lc.errors, \n                         fmt='o', color='black', label=\"Original\", ms=3, lw=1.5, alpha=0.6)\n            plt.errorbar(lc.times, detrended_rates, yerr=lc.errors, \n                         fmt='o', color='dodgerblue', label=\"Detrended\", ms=3, lw=1.5)\n        else:\n            plt.plot(lc.times, lc.rates, label=\"Original\", color=\"black\", alpha=0.6, ms=3, lw=1.5)\n            plt.plot(lc.times, detrended_rates, label=\"Detrended\", color=\"dodgerblue\", ms=3, lw=1.5)\n        plt.plot(lc.times, trend, color='orange', linestyle='--', label='Fitted Trend')\n        plt.xlabel(\"Time\", fontsize=12)\n        plt.ylabel(\"Rates\", fontsize=12)\n        plt.title(\"Polynomial Detrending\")\n        plt.legend()\n        plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n        plt.tick_params(which='both', direction='in', length=6, width=1, top=True, right=True, labelsize=12)\n        plt.show()\n\n    if save:\n        lc.rates = detrended_rates\n</code></pre>"},{"location":"reference/preprocessing/#stela_toolkit.preprocessing.Preprocessing.remove_nans","title":"<code>remove_nans(lightcurve, verbose=True)</code>  <code>staticmethod</code>","text":"<p>Remove time, rate, or error entries that are NaN.</p> <p>Parameters:</p> Name Type Description Default <code>lightcurve</code> <code>LightCurve</code> <p>Light curve to clean.</p> required <code>verbose</code> <code>bool</code> <p>Whether to print how many NaNs were removed.</p> <code>True</code> Source code in <code>stela_toolkit/preprocessing.py</code> <pre><code>@staticmethod\ndef remove_nans(lightcurve, verbose=True):\n    \"\"\"\n    Remove time, rate, or error entries that are NaN.\n\n    Parameters\n    ----------\n    lightcurve : LightCurve\n        Light curve to clean.\n\n    verbose : bool\n        Whether to print how many NaNs were removed.\n    \"\"\"\n\n    lc = lightcurve\n    if lc.errors.size &gt; 0:\n        nonnan_mask = ~np.isnan(lc.rates) &amp; ~np.isnan(lc.times) &amp; ~np.isnan(lc.errors)\n    else:\n        nonnan_mask = ~np.isnan(lc.rates) &amp; ~np.isnan(lc.times)\n\n    if verbose:\n        print(f\"Removed {np.sum(~nonnan_mask)} NaN points.\\n\"\n              f\"({np.sum(np.isnan(lc.rates))} NaN rates, \"\n              f\"{np.sum(np.isnan(lc.errors))} NaN errors)\")\n        print(\"===================\")\n    lc.times = lc.times[nonnan_mask]\n    lc.rates = lc.rates[nonnan_mask]\n    if lc.errors.size &gt; 0:\n        lc.errors = lc.errors[nonnan_mask]\n</code></pre>"},{"location":"reference/preprocessing/#stela_toolkit.preprocessing.Preprocessing.remove_outliers","title":"<code>remove_outliers(lightcurve, threshold=1.5, rolling_window=None, plot=True, save=True, verbose=True)</code>  <code>staticmethod</code>","text":"<p>Remove outliers using the IQR method, globally or locally.</p> <p>Parameters:</p> Name Type Description Default <code>lightcurve</code> <code>LightCurve</code> <p>The input light curve.</p> required <code>threshold</code> <code>float</code> <p>IQR multiplier.</p> <code>1.5</code> <code>rolling_window</code> <code>int</code> <p>Size of local window (if local filtering is desired).</p> <code>None</code> <code>plot</code> <code>bool</code> <p>Whether to visualize removed points.</p> <code>True</code> <code>save</code> <code>bool</code> <p>Whether to modify the light curve in place.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>Whether to print how many points were removed.</p> <code>True</code> Source code in <code>stela_toolkit/preprocessing.py</code> <pre><code>@staticmethod\ndef remove_outliers(lightcurve, threshold=1.5, rolling_window=None, plot=True, save=True, verbose=True):\n    \"\"\"\n    Remove outliers using the IQR method, globally or locally.\n\n    Parameters\n    ----------\n    lightcurve : LightCurve\n        The input light curve.\n\n    threshold : float\n        IQR multiplier.\n\n    rolling_window : int, optional\n        Size of local window (if local filtering is desired).\n\n    plot : bool\n        Whether to visualize removed points.\n\n    save : bool\n        Whether to modify the light curve in place.\n\n    verbose : bool\n        Whether to print how many points were removed.\n    \"\"\"\n\n    def plot_outliers(outlier_mask):\n        \"\"\"Plots the data flagged as outliers.\"\"\"\n        plt.figure(figsize=(8, 4.5))\n        if errors is not None:\n            plt.errorbar(times[~outlier_mask], rates[~outlier_mask], yerr=errors[~outlier_mask], \n                         fmt='o', color='black', ms=3, label='Kept')\n            plt.errorbar(times[outlier_mask], rates[outlier_mask], yerr=errors[outlier_mask], \n                         fmt='o', color='orange', ms=3, label='Outliers')\n        else:\n            plt.scatter(times[~outlier_mask], rates[~outlier_mask], \n                        s=6, color='black', label='Kept')\n            plt.scatter(times[outlier_mask], rates[outlier_mask], \n                        s=6, color='orange', label='Outliers')\n        plt.xlabel(\"Time\", fontsize=12)\n        plt.ylabel(\"Rates\", fontsize=12)\n        plt.title(\"Outlier Detection\")\n        plt.legend()\n        plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n        plt.tick_params(which='both', direction='in', length=6, width=1, top=True, right=True, labelsize=12)\n        plt.show()\n\n    def detect_outliers(rates, threshold, rolling_window):\n        if rolling_window:\n            outlier_mask = np.zeros_like(rates, dtype=bool)\n            half_window = rolling_window // 2\n            for i in range(len(rates)):\n                start = max(0, i - half_window)\n                end = min(len(rates), i + half_window + 1)\n                local_data = rates[start:end]\n                q1, q3 = np.percentile(local_data, [25, 75])\n                iqr = q3 - q1\n                lower_bound = q1 - threshold * iqr\n                upper_bound = q3 + threshold * iqr\n                if rates[i] &lt; lower_bound or rates[i] &gt; upper_bound:\n                    outlier_mask[i] = True\n        else:\n            q1, q3 = np.percentile(rates, [25, 75])\n            iqr = q3 - q1\n            lower_bound = q1 - threshold * iqr\n            upper_bound = q3 + threshold * iqr\n            outlier_mask = (rates &lt; lower_bound) | (rates &gt; upper_bound)\n        return outlier_mask\n\n    lc = deepcopy(lightcurve)\n    times = lc.times\n    rates = lc.rates\n    errors = lc.errors\n\n    outlier_mask = detect_outliers(rates, threshold=threshold, rolling_window=rolling_window)\n\n    if verbose:\n        print(f\"Removed {np.sum(outlier_mask)} outliers \"\n              f\"({np.sum(outlier_mask) / len(rates) * 100:.2f}% of data).\")\n        print(\"===================\")\n\n    if plot:\n        plot_outliers(outlier_mask)\n\n    # Save results back to the original lightcurve if save=True\n    if save:\n        lc.times = times[~outlier_mask]\n        lc.rates = rates[~outlier_mask]\n        if errors.size &gt; 0:\n            lc.errors = errors[~outlier_mask]\n</code></pre>"},{"location":"reference/preprocessing/#stela_toolkit.preprocessing.Preprocessing.reverse_boxcox_transform","title":"<code>reverse_boxcox_transform(lightcurve)</code>  <code>staticmethod</code>","text":"<p>Reverse a previously applied Box-Cox transformation.</p> <p>Parameters:</p> Name Type Description Default <code>lightcurve</code> <code>LightCurve</code> <p>The transformed light curve.</p> required Source code in <code>stela_toolkit/preprocessing.py</code> <pre><code>@staticmethod\ndef reverse_boxcox_transform(lightcurve):\n    \"\"\"\n    Reverse a previously applied Box-Cox transformation.\n\n    Parameters\n    ----------\n    lightcurve : LightCurve\n        The transformed light curve.\n    \"\"\"\n\n    lc = lightcurve\n\n    if not getattr(lc, \"is_boxcox_transformed\", False):\n        raise ValueError(\"Light curve data has not been transformed with Box-Cox.\")\n\n    lambda_opt = lc.lambda_boxcox\n    if lambda_opt == 0:  # inverse log transformation\n        rates_original = np.exp(lc.rates)\n    else:\n        rates_original = (lc.rates * lambda_opt + 1) ** (1 / lambda_opt)\n\n    if lc.errors.size != 0:\n        if lambda_opt == 0:  # inverse log transformation (lambda = 0)\n            errors_original = lc.errors * rates_original\n        else:\n            errors_original = lc.errors / (rates_original ** (lambda_opt - 1))\n    else:\n        errors_original = None\n\n    lc.rates = rates_original\n    lc.errors = errors_original\n    lc.is_boxcox_transformed = False\n    del lc.lambda_boxcox\n</code></pre>"},{"location":"reference/preprocessing/#stela_toolkit.preprocessing.Preprocessing.standardize","title":"<code>standardize(lightcurve)</code>  <code>staticmethod</code>","text":"<p>Standardize the light curve by subtracting its mean and dividing by its std.</p> <p>Saves the original mean and std as attributes for future unstandardization.</p> Source code in <code>stela_toolkit/preprocessing.py</code> <pre><code>@staticmethod\ndef standardize(lightcurve):\n    \"\"\"\n    Standardize the light curve by subtracting its mean and dividing by its std.\n\n    Saves the original mean and std as attributes for future unstandardization.\n    \"\"\"\n    lc = lightcurve\n\n    # check for standardization\n    if np.isclose(lc.mean, 0, atol=1e-10) and np.isclose(lc.std, 1, atol=1e-10) or getattr(lc, \"is_standard\", False):\n        if not hasattr(lc, \"unstandard_mean\") and not hasattr(lc, \"unstandard_std\"):\n            lc.unstandard_mean = 0\n            lc.unstandard_std = 1\n        print(\"The data is already standardized.\")\n\n    # apply standardization\n    else:\n        lc.unstandard_mean = lc.mean\n        lc.unstandard_std = lc.std\n        lc.rates = (lc.rates - lc.unstandard_mean) / lc.unstandard_std\n        if lc.errors.size &gt; 0:\n            lc.errors = lc.errors / lc.unstandard_std\n\n    lc.is_standard = True # flag for detecting transformation without computation\n</code></pre>"},{"location":"reference/preprocessing/#stela_toolkit.preprocessing.Preprocessing.trim_time_segment","title":"<code>trim_time_segment(lightcurve, start_time=None, end_time=None, plot=False, save=True)</code>  <code>staticmethod</code>","text":"<p>Trim the light curve to a given time range.</p> <p>Parameters:</p> Name Type Description Default <code>start_time</code> <code>float</code> <p>Lower time bound.</p> <code>None</code> <code>end_time</code> <code>float</code> <p>Upper time bound.</p> <code>None</code> <code>plot</code> <code>bool</code> <p>Whether to plot before/after trimming.</p> <code>False</code> <code>save</code> <code>bool</code> <p>Whether to modify the light curve in place.</p> <code>True</code> Source code in <code>stela_toolkit/preprocessing.py</code> <pre><code>@staticmethod\ndef trim_time_segment(lightcurve, start_time=None, end_time=None, plot=False, save=True):\n    \"\"\"\n    Trim the light curve to a given time range.\n\n    Parameters\n    ----------\n    start_time : float, optional\n        Lower time bound.\n\n    end_time : float, optional\n        Upper time bound.\n\n    plot : bool\n        Whether to plot before/after trimming.\n\n    save : bool\n        Whether to modify the light curve in place.\n    \"\"\"\n\n    lc = lightcurve\n\n    if start_time is None:\n        start_time = lc.times[0]\n    if end_time is None:\n        end_time = lc.times[-1]\n    if start_time and end_time is None:\n        raise ValueError(\"Please specify a start and/or end time.\")\n\n    # Apply mask to trim data\n    mask = (lc.times &gt;= start_time) &amp; (lc.times &lt;= end_time)\n    if plot:\n        plt.figure(figsize=(8, 4.5))\n        if lc.errors is not None and len(lc.errors) &gt; 0:\n            plt.errorbar(lc.times[mask], lc.rates[mask], yerr=lc.errors[mask], \n                         fmt='o', color='black', ms=3, label='Kept')\n            plt.errorbar(lc.times[~mask], lc.rates[~mask], yerr=lc.errors[~mask], \n                         fmt='o', color='orange', ms=3, label='Trimmed')\n        else:\n            plt.scatter(lc.times[mask], lc.rates[mask], s=6, color=\"black\", label=\"Kept\")\n            plt.scatter(lc.times[~mask], lc.rates[~mask], s=6, color=\"red\", label=\"Trimmed\")\n        plt.xlabel(\"Time\", fontsize=12)\n        plt.ylabel(\"Rates\", fontsize=12)\n        plt.title(\"Trimming\")\n        plt.legend()\n        plt.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n        plt.tick_params(which='both', direction='in', length=6, width=1, top=True, right=True, labelsize=12)\n        plt.show()\n\n    if save:\n        lc.times = lc.times[mask]\n        lc.rates = lc.rates[mask]\n        if lc.errors.size &gt; 0:\n            lc.errors = lc.errors[mask]\n</code></pre>"},{"location":"reference/preprocessing/#stela_toolkit.preprocessing.Preprocessing.unstandardize","title":"<code>unstandardize(lightcurve)</code>  <code>staticmethod</code>","text":"<p>Restore the light curve to its original units using stored mean and std.</p> <p>This reverses a previous call to <code>standardize</code>.</p> Source code in <code>stela_toolkit/preprocessing.py</code> <pre><code>@staticmethod\ndef unstandardize(lightcurve):\n    \"\"\"\n    Restore the light curve to its original units using stored mean and std.\n\n    This reverses a previous call to `standardize`.\n    \"\"\"\n    lc = lightcurve\n    # check that data has been standardized\n    if getattr(lc, \"is_standard\", False):\n        lc.rates = (lc.rates * lc.unstandard_std) + lc.unstandard_mean\n    else:\n        if np.isclose(lc.mean, 0, atol=1e-10) and np.isclose(lc.std, 1, atol=1e-10):\n            raise AttributeError(\n                \"The data has not been standardized by STELA.\\n\"\n                \"Please call the 'standardize' method first.\"\n            )\n        else:\n            raise AttributeError(\n                \"The data is not standardized, and needs to be standardized first by STELA.\\n\"\n                \"Please call the 'standardize' method first (e.g., Preprocessing.standardize(lightcurve)).\"\n            )\n\n    if lc.errors.size &gt; 0:\n        lc.errors = lc.errors * lc.unstandard_std\n\n    lc.is_standard = False  # reset the standardization flag\n</code></pre>"}]}