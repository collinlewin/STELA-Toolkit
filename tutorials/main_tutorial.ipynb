{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Welcome to the STELA Toolkit!**\n",
    "\n",
    "The **STELA Toolkit** is designed for studying variability in light curves, mainly through powerful **frequency-domain data products**, including power and cross spectra, time lags, and coherences. \n",
    "\n",
    "By using **Gaussian process (GP) modeling** to interpolate uneven light curves onto a regular time grid, STELA allows you to carry out Fourier-based analyses as if your data were perfectly sampled. That said, if your data is already regularly sampled, you can use STELA just as easily—GPs are completely optional.\n",
    "\n",
    "In addition to frequency-domain tools, STELA also supports time-domain lag analysis using the cross-correlation function (CCF). For irregularly sampled data, you can choose between modeling the light curves with GPs or using the widely adopted interpolated cross-correlation function (ICCF) method, which linearly interpolates one time series onto the other’s grid.\n",
    "\n",
    "STELA was designed to be convenient and intuitive, regardless of your background in Python or statistics. If you ever run into trouble—whether it’s a bug, something confusing, or a suggestion for how STELA could serve you better—I’d love to hear from you. Please open an issue on the [GitHub Issues Page](https://github.com/collinlewin/stela-toolkit/issues), or feel free to email me directly: Collin Lewin (clewin@mit.edu).\n",
    "\n",
    "##### *In this tutorial, you’ll learn how to...*\n",
    "1. **Import, clean, and plot** time series data\n",
    "2. Model variability using **Gaussian processes (GPs)**\n",
    "3. **Predict new values** of the time series at previously unobserved times\n",
    "4. Generate powerful **frequency-domain and lag-based data products**\n",
    "6. **Simulate light curves** with custom variability and response/lag properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import STELA\n",
    "from stela_toolkit import *\n",
    "\n",
    "import numpy as np\n",
    "print(np.__version__)\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Importing Your Data**\n",
    "---\n",
    "\n",
    "All core functionality in the STELA Toolkit begins with time series data, which we represent using the `LightCurve` class. You can create a `LightCurve` object in one of two ways:\n",
    "\n",
    "1. **From a file**  \n",
    "   STELA supports text-based formats like `.dat`, `.txt`, and `.csv`, as well as FITS files. By default, it assumes the first three columns in the file contain:\n",
    "   - Time\n",
    "   - Measured values (e.g., flux or count rate)\n",
    "   - Measurement uncertainties\n",
    "\n",
    "   If your file uses different columns, you can specify which ones to use with the `file_columns` argument.\n",
    "\n",
    "2. **From NumPy arrays**  \n",
    "   You can also construct a light curve directly by passing arrays for time, values, and uncertainties.\n",
    "\n",
    "> *Tip:* Once you’ve created a `LightCurve`, you can pass it to any STELA analysis tool: modeling, frequency-domain transforms, or lag computations.\n",
    "\n",
    "Below, we’ll load and plot data from the AGN NGC 5548, observed by the Swift Observatory as part of the AGN STORM campaign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Load light curve directly from a text file\n",
    "lightcurve = LightCurve(file_path='../data/NGC5548_U_swift.dat')\n",
    "\n",
    "# Option 2: Or from arrays\n",
    "data = np.genfromtxt(f'../data/NGC5548_X_swift.dat')\n",
    "lightcurve = LightCurve(times=data[:, 0], rates=data[:, 1], errors=data[:, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Plotting Data and Results**\n",
    "---\n",
    "\n",
    "STELA makes it easy to preview and customize plots of your light curve—and not just light curves. **Every main class in the toolkit** (from power spectra to lag measurements) includes a `.plot()` method with a consistent structure, so once you learn it, you can apply it everywhere.\n",
    "\n",
    "Almost every visual element of the plot can be customized using keyword arguments:\n",
    "\n",
    "- Basic plot settings like `xlabel`, `ylabel`, `title`, `figsize`, `xscale`, and `yscale`\n",
    "- Advanced customization via `plot_kwargs`, `fig_kwargs`, `tick_kwargs`, etc.\n",
    "- Save options using `save` and `save_kwargs`\n",
    "\n",
    "This makes it easy to generate quick diagnostic plots or high-quality figures for publications.\n",
    "\n",
    "Let’s start with a simple plot, followed by a fully customized one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic plot\n",
    "lightcurve.plot()\n",
    "\n",
    "# Customized plot\n",
    "lightcurve.plot(\n",
    "    figsize=(8, 4),\n",
    "    xlabel='Modified Heliocentric Julian Date (Days)',\n",
    "    ylabel='Flux',\n",
    "    xlim=(lightcurve.times[0], lightcurve.times[-1]),\n",
    "    title='NGC 5548 U-band Light Curve',\n",
    "    plot_kwargs={'color': 'purple', 'fmt': 'o', 'lw': 1, 'ms': 3},\n",
    "    major_tick_kwargs={'direction': 'in', 'top': True, 'right': True, 'length': 6, 'width': 1},\n",
    "    minor_tick_kwargs={'direction': 'in', 'top': True, 'right': True, 'length': 3, 'width': 0.5}\n",
    "    # save='my_plot.png', savefig_kwargs={'dpi': 300}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Preprocessing and Cleaning**\n",
    "---\n",
    "\n",
    "Before modeling or computing spectral products, it’s often helpful to inspect and clean your light curve. STELA provides a number of tools to help you do this safely and flexibly.\n",
    "\n",
    "All preprocessing tools live in the `Preprocessing` class. You don’t need to create an instance—just call the methods directly.\n",
    "\n",
    "Most of these functions let you:\n",
    "- **Visualize changes** with `plot=True`\n",
    "- **Avoid committing changes** by setting `save=False` (recommended for first passes)\n",
    "- Clean data **without modifying the original file or LightCurve object**\n",
    "\n",
    "Common tasks include:\n",
    "\n",
    "- `remove_nans()` — drop entries with missing data\n",
    "- `remove_outliers()` — exclude extreme points using the interquartile range (IQR)\n",
    "- `trim_time_segment()` — keep only part of the light curve (e.g., for campaign segmentation)\n",
    "- `polynomial_detrend()` — subtract long-term trends before analysis\n",
    "- `standardize()` and `unstandardize()` — mean-center and rescale your data\n",
    "\n",
    "Here’s how to explore these features in practice:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any NaNs in the time, flux, or error arrays\n",
    "Preprocessing.remove_nans(lightcurve)\n",
    "\n",
    "# Explore outlier removal using a rolling IQR window, or a global IQR (set rolling_window = None)\n",
    "Preprocessing.remove_outliers(lightcurve, threshold=1.5, rolling_window=10, plot=True, verbose=True, save=False)\n",
    "Preprocessing.remove_outliers(lightcurve, threshold=1.5, rolling_window=50, plot=True, verbose=True, save=False)\n",
    "\n",
    "# Trim the light curve to a specific observing window\n",
    "Preprocessing.trim_time_segment(lightcurve, end_time=56815, plot=True, save=False)\n",
    "\n",
    "# Detrend the light curve using a polynomial fit (here, degree 3)\n",
    "Preprocessing.polynomial_detrend(lightcurve, degree=3, plot=True, save=False)\n",
    "\n",
    "# Standardize the light curve (zero mean, unit variance)\n",
    "Preprocessing.standardize(lightcurve)\n",
    "Preprocessing.unstandardize(lightcurve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Checking for Normality**\n",
    "---\n",
    "\n",
    "Before fitting a Gaussian Process model, it's important to check whether your light curve's flux distribution is **approximately normal**. Gaussian processes assume the data is drawn from a Gaussian distribution, so significant departures from normality can hurt performance or lead to unstable fits.\n",
    "\n",
    "STELA gives you a few tools in the `Preprocessing` class to assess and correct this via transformations:\n",
    "\n",
    "- **`generate_qq_plot()`**: Plots a Q-Q (quantile-quantile) comparison of your data against a normal distribution. If the points fall roughly on a straight 1:1 line, the data is reasonably normal.\n",
    "- **`check_normal()`**: Performs a formal statistical test for normality at significance level of 0.05. STELA automatically chooses the most appropriate test depending on your sample size, using Shapiro-Wilk for small sample (n<50) and Lilliefors for larger ones (n>50). A Q-Q plot can also be produced here using `plot=True`.\n",
    "- **`check_boxcox_normal()`**: Applies a Box-Cox transformation and re-runs the normality test to see the degree to which the transformation improves normality. A Q-Q plot with both the original and transformed data overlaid can also be produced here using `plot=True`.\n",
    "\n",
    "*Note:* The Box-Cox transform is a power-law transformation that reshapes the data to better resemble a normal distribution, and STELA automatically optimizes the transformation parameter (λ) for your dataset.\n",
    "\n",
    "> **Don't want to do all this beforehand?** Use the `enforce_normality=True` in the `GaussianProcess` class (see below). \n",
    "\n",
    "\n",
    "Let’s try it out and see how our light curve looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the Q-Q plot already appears reasonable, although the tails here seem nonnormal, or our normal is skewed\n",
    "Preprocessing.generate_qq_plot(lightcurve)\n",
    "\n",
    "# test indicates that the data is not normal\n",
    "Preprocessing.check_normal(lightcurve, plot=False)\n",
    "\n",
    "# our boxcox transformation helps!\n",
    "Preprocessing.check_boxcox_normal(lightcurve, plot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Gaussian Process Models in STELA**\n",
    "---\n",
    "\n",
    "To model light curve variability in STELA, we use the `GaussianProcess` class. In summary, a **Gaussian Process (GP)** is a flexible, non-parametric model that treats your data as samples drawn from a multivariate normal distribution, with covariance determined by a *kernel function*. This allows us to infer what the light curve may have looked like between or beyond the observed times, while also accounting for uncertainty in a principled way.\n",
    "\n",
    "> If you’d rather not check for normality yourself (as in the previous section), simply set `enforce_normality=True`. STELA will automatically assess whether your light curve’s flux distribution is sufficiently Gaussian, and if not, apply a Box-Cox transformation. It will then recheck the transformed data to confirm whether normality has improved.\n",
    "\n",
    "> Want to learn more on GPs?  \n",
    "> Read the [Introduction to Gaussian Processes]() part of the documentation (under construction, this won't work yet) to understand the theory behind what we’re doing here.\n",
    "\n",
    "\n",
    "In STELA, you pass your `LightCurve` object to the `GaussianProcess` class to create the model.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Kernel Functions**\n",
    "\n",
    "The covariance function, or *kernel function*, determines how the model relates flux measurements across time. STELA supports a range of kernel options, from smooth to structured and periodic. These are specified using the `kernel_form` argument when creating a `GaussianProcess` model.\n",
    "\n",
    "##### **Basic Kernels**\n",
    "You can choose from several standard kernel types:\n",
    "\n",
    "1. **Radial Basis Function (RBF)**: very smooth and widely used.\n",
    "2. **Rational Quadratic (RQ)**: similar to RBF, but with variable smoothness and an additional hyperparameter.\n",
    "3. **Matern kernels**: less smooth; behavior controlled by the fixed parameter ν:\n",
    "   - `Matern12` (ν = 1/2)\n",
    "   - `Matern32` (ν = 3/2)\n",
    "   - `Matern52` (ν = 5/2)\n",
    "4. **Periodic**: captures repeated patterns with fixed or learned periodicity.\n",
    "5. **Spectral Mixture**: fits a weighted mixture of Gaussians in the frequency domain to model rich stationary processes.  \n",
    "   - Can learn periodic, quasi-periodic, and multi-scale behavior from data.  \n",
    "   - Syntax: `\"SpectralMixture, N\"` sets the number of mixtures to `N`, e.g., `\"SpectralMixture, 4\"`.\n",
    "\n",
    "> If `kernel_form='auto'`, STELA will try a list of standard kernels (like `RBF`, `RQ`, `Matern`, `SpectralMixture`) and select the best one using AIC.\n",
    "\n",
    "##### **Composing Kernels**\n",
    "You can now define **custom combinations of kernels** using arithmetic expressions:\n",
    "\n",
    "- `+` combines kernels additively (e.g., `RBF + Periodic` models smooth variation plus periodicity)\n",
    "- `*` multiplies kernels to form structured, *quasi-periodic* or modulated processes\n",
    "- Parentheses are supported for grouping\n",
    "\n",
    "**Examples:**\n",
    "\n",
    "- `'RBF + Periodic'`: smooth baseline with superimposed periodic component  \n",
    "- `'(Matern32 + Periodic) * RQ'`: quasi-periodic behavior with moderate roughness \n",
    "\n",
    "It's a good idea to compare AIC/BIC between models (using .aic(), .bic after training) to avoid overfitting with too many parameters.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Error and Noise Handling**\n",
    "\n",
    "STELA’s GP models automatically incorporate your observational errors if you’ve provided them in the `LightCurve`. These act as fixed noise levels for each data point in the likelihood.\n",
    "\n",
    "In addition, you can optionally fit an **extra white noise component** to account for unmodeled variability. This is enabled via ``white_noise=True``.\n",
    "\n",
    "---\n",
    "\n",
    "### **Setting Up a GP Model**\n",
    "---\n",
    "\n",
    "You can create a `GaussianProcess` model by passing in your `LightCurve` object, along with options for kernel choice, noise modeling, and normality checking. This step only sets up the model—it doesn’t train it yet.\n",
    "\n",
    "Let’s initialize a model and prepare it for training, although we could just set `run_training=True` for future reference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_model = GaussianProcess(lightcurve, \n",
    "                           kernel_form=\"Matern32\", \n",
    "                           white_noise=True, \n",
    "                           enforce_normality=True, \n",
    "                           run_training=False\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training the Model**\n",
    "---\n",
    "\n",
    "Once your Gaussian Process model is initialized, you can *train* it to fit your data. This step adjusts the kernel’s hyperparameters (e.g., length scale, amplitude, white noise) so the model best represents the variability in your light curve.\n",
    "\n",
    "Training is performed by minimizing the **Negative Log Marginal Likelihood (NLML)** — a standard loss function for GPs. It measures how likely your observed data is under the current model.\n",
    "\n",
    "More precisely, the marginal likelihood is the probability of observing your data given the kernel hyperparameters (e.g., length scale, amplitude), **after integrating over all possible functions** the GP could represent. Lower NLML means the model is assigning higher probability to your observed data.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Running Training**\n",
    "\n",
    "To train your model, call the `.train()` method on your `GaussianProcess` object. This adjusts the kernel’s hyperparameters so the model best fits your light curve.\n",
    "\n",
    "If you don’t know much about optimization, don’t worry! While the default training settings (`learn_rate=0.1`, `num_iter=500`) are usually sufficient for most light curves and kernels, it's good practice to plot the NLML:\n",
    "\n",
    "> **Recommended:** Set `plot_training=True`. This will show you how the training loss (NLML) evolves. You want to see the curve decrease and then flatten out by the end — that means the model has reached a stable fit. Otherwise see the subsection below.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Training Parameters**\n",
    "\n",
    "- `num_iter`: how many training steps to take (optional, default 500)\n",
    "- `learn_rate`: how fast the optimizer updates the parameters (optional, default 0.1)\n",
    "- `plot`: show a plot of the loss improving over time (optional, default False)\n",
    "- `verbose`: print optimization details (optional, default False)\n",
    "\n",
    "---\n",
    "\n",
    "#### **What to Try if Things Don’t Look Right**\n",
    "\n",
    "- If the loss curve is **very slowly decreasing**, try increasing `learn_rate` (e.g., to `0.3` or `0.5`)\n",
    "- If the curve is **bouncy or erratic**, lower `learn_rate` (e.g., to `0.01`)\n",
    "- If the curve is **still going down at the end**, increase `num_iter` (e.g., to `1000`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model manually after initialization\n",
    "gp_model.train(\n",
    "    num_iter=500,        # Number of training steps\n",
    "    learn_rate=0.1,      # Step size for optimizer\n",
    "    plot=True,           # Show NLML evolution\n",
    "    verbose=True         # Print progress info\n",
    ")\n",
    "\n",
    "# Alternatively: train automatically during initialization\n",
    "gp_model = GaussianProcess(lightcurve, \n",
    "                           kernel_form=\"Matern32 + Periodic\", \n",
    "                           white_noise=True, \n",
    "                           enforce_normality=True, \n",
    "                           run_training=True,\n",
    "                           plot_training=False,\n",
    "                           num_iter=500,        \n",
    "                           learn_rate=0.1, \n",
    "                           verbose=False)\n",
    "\n",
    "# To view the hyperparameters of a model at a later time, use the get_hyperparameters method\n",
    "gp_model.get_hyperparameters()\n",
    "\n",
    "# It is good practice to save the model, so that you can load it later for consistency\n",
    "# Both can be done with the package's save_model and load_model methods\n",
    "# gp_model.save_model('gp_model.pkl')\n",
    "# gp_model.load_model('gp_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Selecting a Kernel Form**\n",
    "---\n",
    "\n",
    "When you create a Gaussian Process model, a key decision is which *kernel* to use. The kernel affects how the model captures the observed variability, including how smooth, erratic, or periodic the signal is. \n",
    "\n",
    "This choice directly affects how well the GP can capture the underlying structure of your light curve and thus should be assessed on each light curve independently. \n",
    "\n",
    "---\n",
    "\n",
    "##### **Option 1: Let STELA Select Automatically**\n",
    "\n",
    "If you're unsure which kernel is best, you can pass `'auto'` or a list of kernel names to `kernel_form`. STELA will try each one, train a model for each (even if `run_training=False`), and select the best based on Akaike Information Criterion (AIC). AIC rewards higher likelihood, and punishes model complexity based on the number of kernel hyperparameters.\n",
    "\n",
    "##### **Option 2: Compare Kernels Manually**\n",
    "\n",
    "If you'd like to explore different kernels yourself, you can train them one at a time and compare their AIC/BIC scores manually. \n",
    "\n",
    "> Tip: Try simple kernels first, then build up with composite ones. Let the data justify added complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Let STELA select automatically\n",
    "# Will take a bit longer than normal to train, assess all the models\n",
    "gp_model = GaussianProcess(lightcurve,\n",
    "                           kernel_form=\"auto\", # consider all kernels available\n",
    "                         # kernel_form=[\"RBF\", \"Matern32\", \"RQ\", \"Matern52\"],\n",
    "                           white_noise=True, \n",
    "                           enforce_normality=False, \n",
    "                        )  \n",
    "\n",
    "# Option 2: Compare kernels manually\n",
    "gp1 = GaussianProcess(lightcurve, kernel_form=\"RBF\", run_training=True)\n",
    "gp2 = GaussianProcess(lightcurve, kernel_form=\"Matern32\", run_training=True)\n",
    "\n",
    "print(\"Kernel Statistics (lower is better for AIC/BIC):\")\n",
    "print(\"===============================================\")\n",
    "\n",
    "print(f\"{'Kernel':<12} {'AIC':>10} {'BIC':>10}\")\n",
    "print(\"-\" * 34)\n",
    "print(f\"{'RBF':<12} {gp1.aic():>10.3f} {gp1.bic():>10.3f}\")\n",
    "print(f\"{'Matern32':<12} {gp2.aic():>10.3f} {gp2.bic():>10.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Predicting Missing Data in the Gaps**\n",
    "---\n",
    "\n",
    "Once your Gaussian Process model is trained, there are two main ways to evaluate it:\n",
    "\n",
    "---\n",
    "\n",
    "#### **1. `.predict()` : Posterior Mean and Standard Deviation**\n",
    "\n",
    "The `.predict()` method gives you the **posterior mean and standard deviation** of the GP at each time point. These represent the *best guess* of the true underlying light curve and the uncertainty in that guess.\n",
    "\n",
    "This is useful for:\n",
    "- Visualizing the smoothed light curve\n",
    "- Understanding where the model is confident vs. uncertain\n",
    "\n",
    "But: we don't typically use `.predict()` results directly in downstream analyses like power spectrum or lag estimation, because those calculations require full realizations of plausible light curves — not just the mean prediction.\n",
    "\n",
    "---\n",
    "\n",
    "#### **2. `.sample()` : Draw Posterior Realizations**\n",
    "\n",
    "The `.sample()` method draws full realizations from the trained GP’s posterior distribution — that is, **plausible versions of what the true light curve could have looked like**, given your data and the learned model.\n",
    "\n",
    "These samples allow STELA to **intuitively propagate uncertainties** from the GP method onto the data products below:\n",
    "\n",
    "Specifically, instead of computing these data products just once, STELA:\n",
    "1. Computes the result for **each individual realization**  \n",
    "2. Aggregates the results across all samples  \n",
    "3. Reports the **mean and spread (standard deviation)** of the final measurement\n",
    "\n",
    "---\n",
    "\n",
    "#### **How Samples Are Used Internally**\n",
    "\n",
    "Many STELA classes — including:\n",
    "- `PowerSpectrum`\n",
    "- `LagFrequencySpectrum`\n",
    "- `CrossSpectrum`\n",
    "- `Coherence`\n",
    "- `CrossCorrelation` (for GP mode)\n",
    "\n",
    "will **automatically use the most recently generated samples** from the model when you use the model as an input in these classes. You do **not** need to pass the samples in manually.\n",
    "\n",
    "> If you don’t generate samples yourself, STELA will do it for you upon input of the model into any of the classes above (default: 1000 samples).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the model's prediction, uncertainty, and a sample realization\n",
    "gp_model.plot()\n",
    "\n",
    "# Define a regular time grid for prediction and sampling\n",
    "new_times = np.linspace(lightcurve.times[0], lightcurve.times[-1], 1000, dtype=np.float64)\n",
    "\n",
    "# Draw 1000 posterior samples from the GP on this grid\n",
    "# Each realization is a row in the resulting array\n",
    "samples = gp_model.sample(new_times, num_samples=1000)\n",
    "\n",
    "# Compute the posterior mean and 2-sigma confidence bounds\n",
    "predict_mean, lower, upper = gp_model.predict(new_times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Frequency-Resolved Analysis Tools**\n",
    "---\n",
    "\n",
    "STELA includes several classes for computing main frequency-resolved data products. These tools help quantify variability, correlation, and time delays between light curves across different timescales/temporal frequencies.\n",
    "\n",
    "All of the classes listed below follow a similar interface and workflow:\n",
    "\n",
    "---\n",
    "\n",
    "#### **Shared Structure and Workflow**\n",
    "\n",
    "Each class takes as input:\n",
    "- Either a pair of light curves (`LightCurve` objects), or\n",
    "- A pair of trained GP models (`GaussianProcess` objects)\n",
    "\n",
    "If you pass GP models, STELA will automatically use the most recently generated GP samples.  \n",
    "If you haven’t generated samples yet, the class will generate 1000 by default.\n",
    "\n",
    "All classes support (via user-defined inputs):\n",
    "- **Frequency binning** (log or linear)\n",
    "- **Custom bin edges**\n",
    "- **Frequency range controls (`fmin`, `fmax`)**\n",
    "  - Use `fmin=\"auto\"`, `fmax=\"auto\"` to use the minimum and maximum frequency possible given the lightcurve duration, sampling rate.\n",
    "- A `.plot()` method to quickly visualize the result, including the coherence spectrum for lags.\n",
    "- An optional `.count_frequencies_in_bins()` method (for diagnostics)\n",
    "\n",
    "---\n",
    "\n",
    "#### **What Each Class Computes**\n",
    "\n",
    "- **PowerSpectrum**  \n",
    "  Quantifies the variability amplitude in a single light curve as a function of frequency.\n",
    "\n",
    "  The PSD can also be fit via a maximum likelihood approach! Two models are currently supported:\n",
    "  - `powerlaw`\n",
    "  - `powerlaw_lorentzian`: power-law plus a Lorentzian component.  \n",
    "\n",
    "- **CrossSpectrum**  \n",
    "  Measures the complex correlation between two light curves in the frequency domain.\n",
    "\n",
    "- **LagFrequencySpectrum**  \n",
    "  Extracts the phase lag between two light curves as a function of frequency. Coherence spectrum also computed.\n",
    "\n",
    "- **LagEnergySpectrum**  \n",
    "  Computes the time lag between a broad reference band and several comparison bands (e.g., energy bins), averaged over a specified frequency range. Coherence spectrum also computed.\n",
    "\n",
    "- **Coherence**  \n",
    "  Measures how strongly two light curves are linearly related at each frequency (ranges from 0 to 1). Additional coherence from correlated noise can be subtracted using `subtract_bias=True`. This is not needed for GP realizations, which effectively removes the coherence from noise.\n",
    "\n",
    "---\n",
    "\n",
    "In the following examples, we’ll show how to use each of these classes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the power spectrum of the light curve\n",
    "# norm=True (default) for normalization consistent with PSD, otherwise periodogram\n",
    "ps = PowerSpectrum(gp_model, fmin='auto', fmax='auto', num_bins=8, bin_type='log', norm=True)\n",
    "\n",
    "# Fit a powerlaw model to the PSD, can also do 'powerlaw_lorentzian' for QPO fitting!\n",
    "ps.fit('powerlaw')\n",
    "print(ps.model_params)\n",
    "ps.plot(step=False)\n",
    "\n",
    "\n",
    "# Access the frequency and power arrays for your own use\n",
    "freqs = ps.freqs\n",
    "freq_widths = ps.freq_widths\n",
    "powers = ps.powers\n",
    "power_errors = ps.power_errors\n",
    "\n",
    "# Show how many frequencies are in each bin\n",
    "print(\"Number of frequencies per bin:\")\n",
    "print(\"===============================\")\n",
    "print(ps.count_frequencies_in_bins())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lightcurve_uvw2 = LightCurve(file_path=\"../data/NGC5548_UVW2_swift.dat\")\n",
    "gp_model_uvw2 = GaussianProcess(lightcurve_uvw2,\n",
    "                           kernel_form=\"auto\", # consider all kernels available\n",
    "                           white_noise=True, \n",
    "                           enforce_normality=False, \n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples using the same time grid as the other band (important!!)\n",
    "gp_model_uvw2.sample(new_times, num_samples=1000)\n",
    "\n",
    "# Compute the cross spectrum between two light curves or GP models\n",
    "cs = CrossSpectrum(gp_model, gp_model_uvw2, \n",
    "                   fmin='auto', fmax='auto',\n",
    "                   num_bins=8, bin_type='log')\n",
    "cs.plot()\n",
    "\n",
    "# Compute the lag-frequency spectrum between two bands\n",
    "# Positive lag indicates that the first light curve LAGS the second\n",
    "lag_freq = LagFrequencySpectrum(gp_model, gp_model_uvw2,\n",
    "                                fmin='auto', fmax='auto',\n",
    "                                num_bins=8, bin_type='log')\n",
    "lag_freq.plot()\n",
    "\n",
    "# Compute the coherence spectrum\n",
    "coh = Coherence(gp_model, gp_model_uvw2, \n",
    "                fmin='auto', fmax='auto',\n",
    "                num_bins=8, bin_type='log')\n",
    "coh.plot()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
