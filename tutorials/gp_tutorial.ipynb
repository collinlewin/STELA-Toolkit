{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Welcome to the TSI-Toolkit!**\n",
    "The **goal of this tutorial** is to familiarize the user with using tsi-toolkit to import their time series (i.e., light curve) data, to create and compare models that aim to capture the variablity in their data, and apply models to make predictions of the time series at new, previously unobserved data points, either in the future (forecasting) or between observed values (interpolation). \n",
    "\n",
    "This package was designed to be convenient and intuitive, regardless of your background in Python and machine learning. I deeply appreciate those who voice their frustrations, whether about bugs or how this package could better serve you. Please feel free to open a new issue post in the [Github Repo's Issue Page](https://github.com/collinlewin/tsi-toolkit/issues), or by emailing me, Collin Lewin (clewin@mit.edu).\n",
    "\n",
    "##### *In this tutorial, we will learn...*\n",
    "1. How to **import, clean, and plot** time series data\n",
    "2. The basics of **modeling data with Gaussian processes (GPs)**\n",
    "3. How to **train a GP model** using our data, and how to select between GP models\n",
    "4. **Predicting new values** of the time series at unseen times to produce regular sampling\n",
    "5. Generate a slew of powerful products for gaining insight on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tsi_toolkit import *\n",
    "import numpy as np\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Importing Data**\n",
    "----------------------\n",
    "\n",
    "Each main functionality of the toolkit takes the form of a class. Time Series data is input to the package via the **TimeSeries class** by providing either a...\n",
    "\n",
    "1. File name. Text-based and fits file formats are supported, including .csv with comma delimiters.\n",
    "    - Assumes first three columns in the file contains times, values, and sigmas, respectively. Modify which columns to use with the file_columns attribute (see below).  \n",
    "2. Arrays containing the time, values (i.e., flux), and sigmas (uncertainties on each measurement).\n",
    "\n",
    "***Tip:** Instantiate classes simply with \"variable_name = ClassName(arguments)\".*\n",
    "- All arguments for any class are detailed in the source code (in /tsi_toolkit). Afterwards, you are ready to use the methods of that class!\n",
    "\n",
    "For this tutorial, we will be working with observations of the active galactic nucleus (AGN) NGC5548 made by the Neil Gehrels Swift Observatory as part of the exceptional AGN STORM campaign. Let's import and plot the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import from file\n",
    "file_path = '/home/clewin/projects/tsi-toolkit/data/'\n",
    "lightcurve = TimeSeries(file_path = f'{file_path}NGC5548_U_swift.dat')\n",
    "\n",
    "# ... or import from arrays\n",
    "data = np.genfromtxt(f'{file_path}NGC5548_X_swift.dat')\n",
    "lightcurve = TimeSeries(times=data[:,0], values=data[:,1], sigmas=data[:,2])\n",
    "\n",
    "# Plot easily with the plot method. Nearly every class has a plot method!\n",
    "lightcurve.plot()\n",
    "\n",
    "# Want to customize the plot?\n",
    "# Almost every facet of the plot can be changed with the plot method's kwargs!\n",
    "# This includes the figure, axes, plot, tick, and saving kwargs.\n",
    "lightcurve.plot(figsize=(8,4),\n",
    "                xlabel='Modified Heliocentric Julian Date (Days)', \n",
    "                ylabel='Flux',\n",
    "                xlim = (lightcurve.times[0], lightcurve.times[-1]),\n",
    "                title='NGC 5548 U-band Lightcurve',\n",
    "                fig_kwargs={'linewidth':28},\n",
    "                plot_kwargs={'color':'purple', 'fmt':'o', 'lw':1, 'ms':3},\n",
    "                major_tick_kwargs={'direction':'in', 'top':True, 'right':True, 'length':6, 'width':1},\n",
    "                minor_tick_kwargs={'direction':'in', 'top':True, 'right':True, 'length':3, 'width':0.5},\n",
    "                # save=f\"{file_path}pretty_plot.png\"\n",
    "                # save_kwargs={'dpi':300}\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Data Preprocessing/Cleaning**\n",
    "-----\n",
    "\n",
    "The **Preprocessing class** is full of methods for you to explore cleaning your data, including outlier detection, polynomial detrending, trimming, standardization, and removing nans. \n",
    "- These methods operate exclusively on the TimeSeries instance we made before, and never on the original data/file!\n",
    "\n",
    "The key word above is **\"explore\"**\n",
    "* *save=False* allows the user to experiment without committing to any changes.\n",
    "* *plot=True* visualizes these changes.\n",
    "\n",
    "***Note:*** There is no need to instantiate the Preprocessing class like we did with the TimeSeries class: it is simply a utility class that takes our instance of TimeSeries as an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove nans\n",
    "Preprocessing.remove_nans(lightcurve)\n",
    "\n",
    "# Remove outliers using the interquartile range (IQR)\n",
    "# Points beyond threshold*IQR from the median are removed\n",
    "# Rolling window is the number of points to consider when calculating IQR\n",
    "# If rolling_window is None, the entire lightcurve is used to calculate IQR\n",
    "Preprocessing.remove_outliers(lightcurve, threshold=1.5, rolling_window=10, plot=True, verbose=True, save=False)\n",
    "Preprocessing.remove_outliers(lightcurve, threshold=1.5, rolling_window=50, plot=True, verbose=True, save=False)\n",
    "Preprocessing.remove_outliers(lightcurve, threshold=1.5, rolling_window=None, plot=True, verbose=True, save=True)\n",
    "\n",
    "# Trim the lightcurve to a specific time segment\n",
    "Preprocessing.trim_time_segment(lightcurve, end_time=56815, plot=True, save=False)\n",
    "\n",
    "# Detrend the lightcurve using a polynomial of a specified degree\n",
    "Preprocessing.polynomial_detrend(lightcurve, degree=3, plot=True, save=False)\n",
    "\n",
    "# Standardize the lightcurve\n",
    "Preprocessing.standardize(lightcurve)\n",
    "Preprocessing.unstandardize(lightcurve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Introduction to Gaussian Processes**\n",
    "-----\n",
    "##### What is a Gaussian Process?\n",
    "-----\n",
    "\n",
    "Imagine you want to predict or model a function that describes how something changes over time, in this case the brightness of an AGN. A **Gaussian Process (GP)** is a tool that allows for modeling this function *even if we don't know its exact form.*\n",
    "\n",
    "* Unlike linear regression, in which we define a fixed function form with parameters that we learn, a GP is a probabilistic framework in which we instead model the data as a collection of random variables (the time series values, flux in this case), where any *subset of these variables follows a joint Gaussian distribution.*\n",
    "        \n",
    "    - *Put plainly, this is a prior for the distribution of the data.*\n",
    "\n",
    "Like any Gaussian, we define the joint Gaussian using a mean and covariance; in this case, functions with parameters that we do learn from the data in order to refine the prior:\n",
    "- **Mean function $m(t)$:** reflects the expected value at any point in time ($t$).\n",
    "    - The *mean function* is defined to be 0, as the GP class will standardize your data for you (which is undone at the end to prevent confusion). In doing so, we assume the data has no inherent trend, allowing the GP to learn the structure entirely from the covariance function and observed data.\n",
    "    \n",
    "- **Covariance (i.e. Kernel) function $k(t,t')$:** measures how correlated values at different time values ($t,t'$), and hence measures the variability, smoothness, periodicity, etc. We learn properties of the variability by optimizing the **hyperparameters** of this function (e.g., length scales, amplitudes).\n",
    "\n",
    "##### Optimizing the Hyperparameters of the Kernel Function\n",
    "-----\n",
    "\n",
    "The kernel function has infinitely many functional forms, each with their own hyperparameters and their own efficacy in capturing the variability in the data. \n",
    "\n",
    "The process of finding the optimal set of hyperparameters that make the observed data most likely, given the GP assumptions, is equivalent to maximizing the log marginal likelihood, or, equivalently, minimizing the **negative log marginal likelihood (NLML)** in this case.\n",
    "    \n",
    "- *In Bayesian verbage:* this is optimizing the model evidence and is thus not technically \"fitting\" to the data in the way we do with non-Bayesian models like linear regression. We avoid maximum likelihood estimation or maximum a posteriori estimation for GPs because they result in overfitting the data.\n",
    "\n",
    "##### Predictions using GPs\n",
    "-----\n",
    "\n",
    "Bayesian inference is at the heart of making predictions at new time points. The GP prior is updated to form the posterior distribution, which can be conditioned on the data that we have observed. \n",
    "\n",
    "By *sampling from this posterior distribution*, we can draw realizations that pass through or near the observed data (accounting for noise) while extrapolating plausible behaviors to unobserved points.\n",
    "* These realizations provide predictions of the function values at new time points, with multiple realizations allowing for intuitive uncertainty in those predictions, reflecting areas where the model is less certain due to a lack of observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Process Models in TSI-Toolkit\n",
    "-----\n",
    "\n",
    "We use the **GaussianProcess class** for creating, training, and predicting from a GP model using the TimeSeries instance (the \"lightcurve\" variable) as an input. \n",
    "\n",
    "**Kernels**\n",
    "\n",
    "* The most common kernel function forms are included in this package, set with \"kernel_form\".\n",
    "     \n",
    "    1. Rational Quadratic (RQ)\n",
    "    2. Radial Basis Function (RBF)\n",
    "    3. Three forms of the Matern kernel, each with a different value for the smoothness parameter ($\\nu=1/2, 3/2, 5/2$, respectively named Matern12, Matern32, Matern52. *This parameter is not to be confused with a hyperparameter to be optimized!*)\n",
    "    4. The Spectral Mixture kernel, which is sophisticated and thus included for experimentation.\n",
    "        - The number of spectral mixtures is a parameter for the user (e.g., \"SpectralMixture, 4\" sets 4 mixtures). I recommend sticking to the tried-and-true previous kernel forms. \n",
    "    \n",
    "    - To view these functional forms, please refer to the [the GPyTorch documentation](https://docs.gpytorch.ai/en/latest/kernels.html).\n",
    "\n",
    "**White Noise**\n",
    "\n",
    "* In addition to accounting for measurement errors (only if provided), the user can fit for white noise by setting \"white_noise=True\". This adds to the variance only, as expected: $k(x_1, x_1) = \\sigma_{noise}^2$.\n",
    "\n",
    "**Training**\n",
    "\n",
    "* We optimize the kernel hyperparameters by minimizing the NLML using the ADAM optimizer. For simplicity, this optimizer adjusts the hyperparameter values at each iteration in the direction that results in a lower \"loss\" (the objective function, NLML in our case).\n",
    "\n",
    "- The **learning rate** sets the size of the parameter change each iteration (set by the \"learn_rate\" argument), with the **number of iterations** set by the \"num_iter\" argument.\n",
    "\n",
    "    - In most cases, the default values (learn_rate = 0.01, train_iter) should be fine, but it is good practice to confirm the training process results yourself and have confidence in your results!\n",
    "    - View the training process using \"verbose=True\" and/or \"plot_training=True\" to print and/or visualize how the NLML evolves.\n",
    "    - ***Tip:*** If it is taking forever to converge (monotonically), increase the learning rate. If the hyperparameters are bouncing around a suspected optimum, lower the learning rate. See the image below.\n",
    "    \n",
    "    \n",
    "\n",
    "<img src=\"./learning_rate.png\" style=\"width:80%;height:auto;\">\n",
    "\n",
    "*Adjusting learning rate based on loss, credit To Jeremy Jordan (https://www.jeremyjordan.me/nn-learning-rate/).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Gaussian Process model on the lightcurve, using a Rational Quadratic kernel\n",
    "# I have included all the relevant arguments for learning purposes, \n",
    "# although white_noise, learn_rate, num_iter, and verbose are set to their default values\n",
    "gp_model = GaussianProcess(timeseries=lightcurve, \n",
    "                           kernel_form='RQ', \n",
    "                           white_noise=True,\n",
    "                           learn_rate=1e-1,\n",
    "                           num_iter=1000,\n",
    "                           verbose=True,\n",
    "                           plot_training=True,\n",
    "                        )\n",
    "# In other words, the following line would produce the same results as above\n",
    "# gp_model = GaussianProcess(timeseries=lightcurve, kernel_form='RQ')\n",
    "\n",
    "# Note the convergence in the loss, and the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To view the hyperparameters of a model at a later time, use the get_hyperparameters method\n",
    "gp_model.get_hyperparameters()\n",
    "\n",
    "# It is good practice to save the model, so that you can load it later for consistency\n",
    "# Both can be done with the package's save_model and load_model methods\n",
    "# gp_model.save_model('gp_model.pkl')\n",
    "# gp_model.load_model('gp_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A main motivation for this package was to create a user-friendly method for comparng kernel models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_model.akaike_inf_crit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_model = GaussianProcess(timeseries=lightcurve,\n",
    "                           kernel_form='auto')\n",
    "#gp_model = GaussianProcess(timeseries=lightcurve,\n",
    "#                           kernel_form=['RQ', 'Matern12', 'Matern52'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can visualize the predictions of the model with the plot method\n",
    "gp_model.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_times = np.linspace(lightcurve.times[0], lightcurve.times[-1], 1000)\n",
    "samples = gp_model.sample(prediction_times, num_samples=1000) # can also use saving arguments, detects what type of file (binary or text) # needs testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "power_spectrum = PowerSpectrum(times=prediction_times, values=samples, norm=True)\n",
    "power_spectrum.plot()\n",
    "\n",
    "power_spectrum.bin(num_bins=13, bin_type=\"log\", save=False, plot=True)\n",
    "\n",
    "# will add this tomorrow to properly merge those first two bins.\n",
    "power_spectrum.bin(bin_edges = [])\n",
    "\n",
    "# let's look at number of frequencies in bins, which can be done for already defined bins (no parameters)\n",
    "# or we can explore how the frequencies would look in a new binning selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
